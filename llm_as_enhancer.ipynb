{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn.models import MLP\n",
    "from torch_geometric.nn.conv import GCNConv, GINConv, SAGEConv\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import LabelPropagation\n",
    "from torch_geometric.nn.models import GAT\n",
    "import dgl.nn.pytorch as dglnn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv as PYGGATConv\n",
    "# import rev.memgcn as memgcn\n",
    "# from rev.rev_layer import SharedDropout\n",
    "import copy\n",
    "from torch_sparse import SparseTensor, matmul\n",
    "from torch_geometric.utils import degree\n",
    "import numpy as np\n",
    "import math\n",
    "import tqdm\n",
    "from dgl import function as fn\n",
    "from dgl._ffi.base import DGLError\n",
    "from dgl.nn.pytorch.utils import Identity\n",
    "from dgl.ops import edge_softmax\n",
    "from dgl.utils import expand_as_pair\n",
    "from torch_geometric.loader import NeighborLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_layers, input_dim, hidden_dimension, num_classes, dropout, norm=None) -> None:\n",
    "        super().__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.norms = torch.nn.ModuleList()\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        if num_layers == 1:\n",
    "            self.convs.append(GCNConv(input_dim, num_classes, cached=False,\n",
    "                             normalize=True))\n",
    "        else:\n",
    "            self.convs.append(GCNConv(input_dim, hidden_dimension, cached=False,\n",
    "                             normalize=True))\n",
    "            if norm:\n",
    "                self.norms.append(torch.nn.BatchNorm1d(hidden_dimension))\n",
    "            else:\n",
    "                self.norms.append(torch.nn.Identity())\n",
    "\n",
    "            for _ in range(num_layers - 2):\n",
    "                self.convs.append(GCNConv(hidden_dimension, hidden_dimension, cached=False,\n",
    "                             normalize=True))\n",
    "                if norm:\n",
    "                    self.norms.append(torch.nn.BatchNorm1d(hidden_dimension))\n",
    "                else:\n",
    "                    self.norms.append(torch.nn.Identity())\n",
    "\n",
    "            self.convs.append(GCNConv(hidden_dimension, num_classes, cached=False, normalize=True))\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # x, edge_index, edge_weight= data.x, data.edge_index, data.edge_weight\n",
    "        for i in range(self.num_layers):\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            if i != self.num_layers - 1:\n",
    "                x = self.norms[i](x)\n",
    "                x = F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import BertTokenizer, BertModel, AutoTokenizer, DebertaModel, AutoModel, PreTrainedModel\n",
    "\n",
    "class deberta:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.__name__ = 'microsoft/deberta-base'\n",
    "        self.__num_node_features__ = 768 \n",
    "        self.device = 'cpu'\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-base\")\n",
    "# Load model directly\n",
    "        self.model = AutoModel.from_pretrained(\"microsoft/deberta-base\")\n",
    "        # self.model = DebertaModel.from_pretrained(\"microsoft/deberta-base\")\n",
    "        \n",
    "        # self.__output_dim__ = self.__model__.\n",
    "    # @property\n",
    "    def parameters(self):\n",
    "        return self.model.parameters()\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "\n",
    "    def eval(self):\n",
    "        self.model.eval()\n",
    "\n",
    "    @property\n",
    "    def num_node_features(self):\n",
    "        return 768\n",
    "\n",
    "    def to(self, device):\n",
    "        self.model = self.model.to(device)\n",
    "        self.device = device\n",
    "        return self\n",
    "\n",
    "    def forward(self, text):\n",
    "\n",
    "        def model_forward_input(input):\n",
    "            input = self.tokenizer(input, return_tensors='pt').to(self.device)\n",
    "            output = self.model(**input).last_hidden_state.mean(dim=1)\n",
    "            # print(output.shape)\n",
    "            # return self.model(**input).last_hidden_state.mean(dim=1)\n",
    "            # print(output.shape)\n",
    "            return torch.squeeze(output)\n",
    "\n",
    "        return torch.stack(list(map(model_forward_input, text)))\n",
    "\n",
    "    def __call__(self, data):\n",
    "        if isinstance(data, str):\n",
    "            return self.forward([data])\n",
    "        if isinstance(data, list):\n",
    "            return self.forward(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed(seed_val):\n",
    "    import random\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    # import tensorflow as tf\n",
    "    # tf.random.set_seed(seed_value)\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data\n",
    "data, num_classes, text = load_data('ogbn-arxiv', use_dgl=False, use_text=True)\n",
    "# data.text = text\n",
    "# data.x = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = deberta()\n",
    "gcn = GCN(num_layers=2, input_dim=lm.num_node_features, hidden_dimension=128, num_classes=num_classes, dropout=0.1, norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "lm = lm.to(device)\n",
    "gcn = gcn.to(device)\n",
    "\n",
    "# Create NeighborLoader for mini-batch training\n",
    "train_loader = NeighborLoader(data, input_nodes=data.train_mask, num_neighbors=[2, 2], batch_size=4, shuffle=True)\n",
    "valid_loader = NeighborLoader(data, input_nodes=data.val_mask, num_neighbors=[2, 2], batch_size=4, shuffle=False)\n",
    "test_loader = NeighborLoader(data, input_nodes=data.test_mask, num_neighbors=[2, 2], batch_size=4, shuffle=False)\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': lm.parameters(), 'lr': 1e-4, 'weight_decay': 5e-4},\n",
    "    {'params': gcn.parameters(), 'lr': 0.01, 'weight_decay': 5e-4}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for data in train_loader:\n",
    "    cnt += 1\n",
    "    print(cnt)\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    lm.train()\n",
    "    gcn.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = lm([text[i] for i in batch.n_id])\n",
    "        out = gcn(out, batch.edge_index)\n",
    "        loss = F.cross_entropy(out[batch.train_mask], batch.y[batch.train_mask].squeeze())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        del out\n",
    "        torch.cuda.empty_cache()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def test(loader):\n",
    "    lm.eval()\n",
    "    gcn.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        out = lm([text[i] for i in batch.n_id])\n",
    "        out = gcn(out, batch.edge_index)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += (pred[batch.test_mask] == batch.y[batch.test_mask].squeeze()).sum().item()\n",
    "        total += batch.test_mask.sum().item()\n",
    "        del out\n",
    "        torch.cuda.empty_cache()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, 201):\n",
    "    loss = train()\n",
    "    with torch.no_grad():\n",
    "        train_acc = test(train_loader)\n",
    "        valid_acc = test(valid_loader)\n",
    "        test_acc = test(test_loader)\n",
    "    # if epoch % 10 == 0:\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train Acc: {train_acc:.4f}, '\n",
    "            f'Valid Acc: {valid_acc:.4f}, Test Acc: {test_acc:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
