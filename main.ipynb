{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/co-training/env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "import tqdm\n",
    "from typing import Optional\n",
    "from argparse import Namespace\n",
    "import json\n",
    "import types\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import sklearn\n",
    "import numpy as np\n",
    "\n",
    "from cotraining import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_namespace(d):\n",
    "    \"\"\"\n",
    "    Recursively converts a dictionary to a SimpleNamespace.\n",
    "    \n",
    "    Args:\n",
    "        d (dict): The dictionary to convert.\n",
    "        \n",
    "    Returns:\n",
    "        SimpleNamespace: The converted namespace.\n",
    "    \"\"\"\n",
    "    if isinstance(d, dict):\n",
    "        # Convert sub-dictionaries to SimpleNamespace recursively\n",
    "        return types.SimpleNamespace(**{k: dict_to_namespace(v) for k, v in d.items()})\n",
    "    else:\n",
    "        # Return non-dictionary values as-is\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(42)\n",
    "\n",
    "\n",
    "with open('config/arxiv.json') as file:\n",
    "    config = json.loads(file.read())\n",
    "config = dict_to_namespace(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/co-training/env/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "lm = deberta().to(config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from ogb.nodeproppred import DglNodePropPredDataset\n",
    "\n",
    "graph, num_classes, text = load_data('ogbn-arxiv', use_dgl=True, use_text=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "features = torch.load('dataset/arxiv.pt')\n",
    "graph.ndata['x'] = features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = graphsage(num_nodes= graph.num_nodes(), in_feats=lm.__num_node_features__, h_feats=64, num_classes=num_classes).to(config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt = torch.optim.Adam(list(model.parameters())+list(lm.parameters())) # \n",
    "opt = torch.optim.Adam([\n",
    "    {'params': lm.parameters(), 'lr': 1e-4},\n",
    "    {'params': model.parameters(), 'lr': 0.01}])\n",
    "\n",
    "train_dataloader, valid_dataloader, test_dataloader = init_dataloader(graph, 'train', config), init_dataloader(graph, 'val', config), init_dataloader(graph, 'test', config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_accuracy = 0\n",
    "best_model_path = 'model.pt'\n",
    "\n",
    "\n",
    "forward_once(train_dataloader, model)\n",
    "forward_once(valid_dataloader, model)\n",
    "forward_once(test_dataloader, model)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 465/465 [00:29<00:00, 15.83it/s]\n",
      "100%|██████████| 759/759 [00:21<00:00, 35.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Valid Accuracy 0.015860215053763442  Best Accuracy 0.015860215053763442 Test Accuracy 0.013236989459815546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 71/465 [00:03<00:21, 18.27it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "# with tqdm.tqdm(train_dataloader) as tq:\n",
    "#     for step, (input_nodes, output_nodes, mfgs) in enumerate(tq):\n",
    "#         # inputs = [text[i] for i in input_nodes]\n",
    "#         with torch.no_grad():\n",
    "#             # x = lm(inputs)\n",
    "#             x = mfgs[0].srcdata['x']\n",
    "#             model.forward_once(mfgs, x)\n",
    "\n",
    "# with tqdm.tqdm(valid_dataloader) as tq:\n",
    "#     for step, (input_nodes, output_nodes, mfgs) in enumerate(tq):\n",
    "#         # inputs = [text[i] for i in input_nodes]\n",
    "#         with torch.no_grad():\n",
    "#             # x = lm(inputs)\n",
    "#             x = mfgs[0].srcdata['x']\n",
    "#             model.forward_once(mfgs, x)\n",
    "\n",
    "# with tqdm.tqdm(test_dataloader) as tq:\n",
    "#     for step, (input_nodes, output_nodes, mfgs) in enumerate(tq):\n",
    "#         # inputs = [text[i] for i in input_nodes]\n",
    "#         with torch.no_grad():\n",
    "#             # x = lm(inputs)\n",
    "#             x = mfgs[0].srcdata['x']\n",
    "#             model.forward_once(mfgs, x)\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "\n",
    "    with tqdm.tqdm(train_dataloader) as tq:\n",
    "        for step, (input_nodes, output_nodes, mfgs) in enumerate(tq):\n",
    "            # print(output_nodes)\n",
    "            inputs = [text[i] for i in output_nodes]\n",
    "            labels = mfgs[-1].dstdata['y']\n",
    "            \n",
    "            inputs = lm(inputs).to(config.device)\n",
    "\n",
    "            predictions = model(mfgs=mfgs, x=inputs, batch_size=config.train_loader.batch_size)\n",
    "            labels = torch.flatten(labels)\n",
    "            # print(predictions.device, labels.device)\n",
    "            loss = F.cross_entropy(predictions, labels)\n",
    "            # loss = torch.tensor(0.)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            accuracy = sklearn.metrics.accuracy_score(labels.cpu().numpy(), predictions.argmax(1).detach().cpu().numpy())\n",
    "\n",
    "            tq.set_postfix({'loss': '%.03f' % loss.item(), 'acc': '%.03f' % accuracy}, refresh=False)\n",
    "\n",
    "            del input_nodes, output_nodes, mfgs, inputs, labels, predictions, loss\n",
    "            torch.cuda.empty_cache()\n",
    "            # print(torch.cuda.mem_get_info())\n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    with torch.no_grad() and tqdm.tqdm(valid_dataloader) as tq, torch.no_grad():\n",
    "        for input_nodes, output_nodes, mfgs in tq:\n",
    "            inputs = mfgs[0].srcdata['x']\n",
    "            labels.append(mfgs[-1].dstdata['y'].cpu().numpy())\n",
    "            predictions.append(model(mfgs=mfgs, x=inputs, batch_size=config.train_loader.batch_size).argmax(1).cpu().numpy())\n",
    "        predictions = np.concatenate(predictions)\n",
    "        labels = np.concatenate(labels)\n",
    "        val_accuracy = sklearn.metrics.accuracy_score(labels, predictions)\n",
    "        if best_val_accuracy <= val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            torch.save(model, best_model_path)\n",
    "\n",
    "    best_model = torch.load(best_model_path)\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    with torch.no_grad() and tqdm.tqdm(test_dataloader) as tq, torch.no_grad():\n",
    "        for input_nodes, output_nodes, mfgs in tq:\n",
    "            # inputs = [text[i] for i in input_nodes]\n",
    "            # print(type(mfgs[0]))\n",
    "            inputs = mfgs[0].srcdata['x']\n",
    "            labels.append(mfgs[-1].dstdata['y'].cpu().numpy())\n",
    "            # inputs = lm(inputs).to(device)\n",
    "            predictions.append(model(mfgs=mfgs, x=inputs, batch_size=config.train_loader.batch_size).argmax(1).cpu().numpy())\n",
    "        predictions = np.concatenate(predictions)\n",
    "        # print(predictions)\n",
    "        labels = np.concatenate(labels)\n",
    "        test_accuracy = sklearn.metrics.accuracy_score(labels, predictions)\n",
    "\n",
    "        print('Epoch {} Valid Accuracy {}  Best Accuracy {} Test Accuracy {}'.format(epoch, val_accuracy, best_val_accuracy, test_accuracy))\n",
    "        # Note that this tutorial do not train the whole model to the end.\n",
    "        # break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
