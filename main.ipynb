{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/co-training/env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import BertTokenizer, BertModel, AutoTokenizer, DebertaModel, AutoModel, PreTrainedModel\n",
    "\n",
    "def seed(seed_val):\n",
    "    import random\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    # import tensorflow as tf\n",
    "    # tf.random.set_seed(seed_value)\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "seed(42)\n",
    "\n",
    "class deberta:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.__name__ = 'microsoft/deberta-base'\n",
    "        self.__num_node_features__ = 768 \n",
    "        self.device = 'cpu'\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-base\")\n",
    "# Load model directly\n",
    "        self.model = AutoModel.from_pretrained(\"microsoft/deberta-base\")\n",
    "        # self.model = DebertaModel.from_pretrained(\"microsoft/deberta-base\")\n",
    "        \n",
    "        # self.__output_dim__ = self.__model__.\n",
    "    # @property\n",
    "    def parameters(self):\n",
    "        return self.model.parameters()\n",
    "\n",
    "    @property\n",
    "    def num_node_features(self):\n",
    "        return 768\n",
    "\n",
    "    def to(self, device):\n",
    "        self.model = self.model.to(device)\n",
    "        self.device = device\n",
    "        return self\n",
    "\n",
    "    def forward(self, text):\n",
    "\n",
    "        def model_forward_input(input):\n",
    "            input = self.tokenizer(input, return_tensors='pt').to(self.device)\n",
    "            output = self.model(**input).last_hidden_state.mean(dim=1)\n",
    "            # print(output.shape)\n",
    "            # return self.model(**input).last_hidden_state.mean(dim=1)\n",
    "            # print(output.shape)\n",
    "            return torch.squeeze(output)\n",
    "\n",
    "        return torch.stack(list(map(model_forward_input, text)))\n",
    "\n",
    "    def __call__(self, data):\n",
    "        if isinstance(data, str):\n",
    "            return self.forward([data])\n",
    "        if isinstance(data, list):\n",
    "            return self.forward(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "class History(torch.nn.Module):\n",
    "    r\"\"\"A historical embedding storage module.\"\"\"\n",
    "    def __init__(self, num_embeddings: int, embedding_dim: int, device=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        pin_memory = device is None or str(device) == 'cpu'\n",
    "        self.emb = torch.empty(num_embeddings, embedding_dim, device=device,\n",
    "                               pin_memory=pin_memory)\n",
    "\n",
    "        self._device = 'cpu'\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.emb.fill_(0)\n",
    "\n",
    "    def _apply(self, fn):\n",
    "        # Set the `_device` of the module without transfering `self.emb`.\n",
    "        self._device = fn(torch.zeros(1)).device\n",
    "        return self\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def pull(self, n_id: Optional[Tensor] = None) -> Tensor:\n",
    "        out = self.emb\n",
    "        if n_id is not None:\n",
    "            assert n_id.device == self.emb.device\n",
    "            out = out.index_select(0, n_id)\n",
    "        return out.to(device=self._device)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def push(self, x, n_id: Optional[Tensor] = None,\n",
    "             offset: Optional[Tensor] = None, count: Optional[Tensor] = None):\n",
    "\n",
    "        if n_id is None and x.size(0) != self.num_embeddings:\n",
    "            raise ValueError\n",
    "\n",
    "        elif n_id is None and x.size(0) == self.num_embeddings:\n",
    "            self.emb.copy_(x)\n",
    "\n",
    "        elif offset is None or count is None:\n",
    "            assert n_id.device == self.emb.device\n",
    "            self.emb[n_id] = x.to(self.emb.device).detach()\n",
    "\n",
    "        else:  # Push in chunks:\n",
    "            src_o = 0\n",
    "            x = x.to(self.emb.device)\n",
    "            for dst_o, c, in zip(offset.tolist(), count.tolist()):\n",
    "                self.emb[dst_o:dst_o + c] = x[src_o:src_o + c]\n",
    "                src_o += c\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        \"\"\"\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (f'{self.__class__.__name__}({self.num_embeddings}, '\n",
    "                f'{self.embedding_dim}, emb_device={self.emb.device}, '\n",
    "                f'device={self._device})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/co-training/env/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "lm = deberta().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import numpy as np\n",
    "# from ogb.nodeproppred import DglNodePropPredDataset\n",
    "\n",
    "from data_utils import load_data\n",
    "\n",
    "# dataset = dgl.data.CoraGraphDataset()\n",
    "# # device = 'cuda'      # change to 'cuda' for GPU\n",
    "# graph = dataset[0]\n",
    "graph, num_classes, text = load_data('ogbn-arxiv', use_dgl=True, use_text=True)\n",
    "# print(graph.ndata)\n",
    "# print(type(graph))\n",
    "# graph = (dataset)\n",
    "\n",
    "\n",
    "# get text feat\n",
    "# dataset, num_classes, text = load_data('cora', use_dgl=True, use_text=True)\n",
    "# features = []\n",
    "# with torch.no_grad():\n",
    "#     for i, t in enumerate(text):\n",
    "#         if i % 1000 == 0:\n",
    "#             print(i)\n",
    "#         features.append(lm(t).squeeze().cpu())\n",
    "# features = torch.stack(features)\n",
    "# torch.save(features, 'arxiv.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import numpy as np\n",
    "from ogb.nodeproppred import DglNodePropPredDataset\n",
    "\n",
    "from data_utils import load_data\n",
    "\n",
    "# dataset = dgl.data.CoraGraphDataset()\n",
    "# # device = 'cuda'      # change to 'cuda' for GPU\n",
    "# graph = dataset[0]\n",
    "graph, num_classes, text = load_data('ogbn-arxiv', use_dgl=True, use_text=True)\n",
    "# print(graph.ndata)\n",
    "# print(type(graph))\n",
    "# graph = (dataset)\n",
    "\n",
    "\n",
    "# get text feat\n",
    "# dataset, num_classes, text = load_data('cora', use_dgl=True, use_text=True)\n",
    "# features = []\n",
    "# with torch.no_grad():\n",
    "#     for t in text:\n",
    "#         features.append(lm(t).squeeze().cpu())\n",
    "# features = torch.stack(features)\n",
    "# torch.save(features, 'arxiv.pt')\n",
    "# print(\"graph.ndata['feat'].shape\", graph.ndata['feat'].shape)\n",
    "# print(\"features.shape\", features.shape)\n",
    "features = torch.load('dataset/arxiv.pt')\n",
    "graph.ndata['x'] = features\n",
    "\n",
    "train_mask = graph.ndata['train_mask']\n",
    "# train_mask = graph.train_mask\n",
    "train_nids = torch.nonzero(train_mask, as_tuple=False).squeeze()\n",
    "val_mask = graph.ndata['val_mask']\n",
    "# val_mask = graph.val_mask\n",
    "val_nids = torch.nonzero(val_mask, as_tuple=False).squeeze()\n",
    "test_mask = graph.ndata['test_mask']\n",
    "# test_mask = graph.test_mask\n",
    "test_nids = torch.nonzero(test_mask, as_tuple=False).squeeze()\n",
    "\n",
    "sampler = dgl.dataloading.NeighborSampler([4, 4])\n",
    "\n",
    "train_dataloader = dgl.dataloading.DataLoader(\n",
    "    # The following arguments are specific to DGL's DataLoader.\n",
    "    graph,              # The graph\n",
    "    train_nids,         # The node IDs to iterate over in minibatches\n",
    "    sampler,            # The neighbor sampler\n",
    "    device=device,      # Put the sampled MFGs on CPU or GPU\n",
    "    # The following arguments are inherited from PyTorch DataLoader.\n",
    "    batch_size=32,    # Batch size\n",
    "    shuffle=True,       # Whether to shuffle the nodes for every epoch\n",
    "    drop_last=True,    # Whether to drop the last incomplete batch\n",
    "    num_workers=0       # Number of sampler processes\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "169343"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.num_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "from dgl.nn import SAGEConv\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, num_classes):\n",
    "        super(Model, self).__init__()\n",
    "        self.history = [History(graph.num_nodes(), embedding_dim=in_feats, device='cpu'), History(graph.num_nodes(), embedding_dim=h_feats, device='cpu')]\n",
    "        self.conv1 = SAGEConv(in_feats, h_feats, aggregator_type='mean')\n",
    "        self.conv2 = SAGEConv(h_feats, num_classes, aggregator_type='mean')\n",
    "        self.h_feats = h_feats\n",
    "\n",
    "\n",
    "    def push_and_pull(self, history: History, x: Tensor,\n",
    "                      batch_size: Optional[int] = None,\n",
    "                      n_id: Optional[Tensor] = None,\n",
    "                      offset: Optional[Tensor] = None,\n",
    "                      count: Optional[Tensor] = None) -> Tensor:\n",
    "        history.push(x[:batch_size], n_id[:batch_size], offset, count)\n",
    "        h = history.pull(n_id[batch_size:]).to(x.device)\n",
    "        return torch.cat([x[:batch_size], h], dim=0)\n",
    "\n",
    "    def forward(self, mfgs, x, batch_size):\n",
    "        # Lines that are changed are marked with an arrow: \"<---\"\n",
    "        # print(\"x.shape: \", x.shape)\n",
    "        x = self.push_and_pull(self.history[0], x, batch_size, mfgs[0].srcdata['_ID'].cpu())\n",
    "        # print(\"x.shape: \", x.shape)\n",
    "        h_dst = x[:mfgs[0].num_dst_nodes()]  # <---\n",
    "        # print(\"h_dst.shape\", h_dst.shape)\n",
    "        h = self.conv1(mfgs[0], (x, h_dst))  # <---\n",
    "        h = F.relu(h)\n",
    "        h = self.push_and_pull(self.history[1], h, batch_size, mfgs[1].srcdata['_ID'].cpu())\n",
    "        h_dst = h[:mfgs[1].num_dst_nodes()]  # <---\n",
    "        h = self.conv2(mfgs[1], (h, h_dst))  # <---\n",
    "        return h\n",
    "    \n",
    "    # @torch.no_grad()\n",
    "    # def forward_once(self, mfgs, x, batch_size):\n",
    "    #     with tqdm.tqdm(train_dataloader) as tq:\n",
    "    #         for step, (input_nodes, output_nodes, mfgs) in enumerate(tq):\n",
    "    #             h_dst = x[:mfgs[0].num_dst_nodes()]  # <---\n",
    "    #             # print(\"h_dst.shape\", h_dst.shape)\n",
    "    #             h = self.conv1(mfgs[0], (x, h_dst))  # <---\n",
    "    #             h = F.relu(h)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = Model(in_feats=lm.__num_node_features__, h_feats=64, num_classes=num_classes).to(device)\n",
    "# model = Model(in_feats=1433, h_feats=64, num_classes=7).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt = torch.optim.Adam(list(model.parameters())+list(lm.parameters())) # \n",
    "opt = torch.optim.Adam([\n",
    "    # {'params': lm.parameters(), 'lr': 1e-4},\n",
    "    {'params': model.parameters(), 'lr': 0.01}])\n",
    "valid_dataloader = dgl.dataloading.DataLoader(\n",
    "    graph, val_nids, sampler,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=0,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def forward_once(x, lm, gnn):\n",
    "    x0 = lm(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 13/2841 [00:16<1:01:31,  1.31s/it, loss=3.297, acc=0.188]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m inputs \u001b[38;5;241m=\u001b[39m [text[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m output_nodes]\n\u001b[1;32m     19\u001b[0m labels \u001b[38;5;241m=\u001b[39m mfgs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdstdata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 21\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mlm\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# mfgs = mfgs.to(device)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# print(inputs.shape, labels.shape)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# print(len(labels), inputs.shape, input_nodes.shape, output_nodes.shape)    \u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# break        \u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# predictions = model(mfgs, inputs, )\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# print(inputs.device, model.device)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model(mfgs\u001b[38;5;241m=\u001b[39mmfgs, x\u001b[38;5;241m=\u001b[39minputs, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 57\u001b[0m, in \u001b[0;36mdeberta.__call__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward([data])\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 51\u001b[0m, in \u001b[0;36mdeberta.forward\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# print(output.shape)\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m# return self.model(**input).last_hidden_state.mean(dim=1)\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# print(output.shape)\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39msqueeze(output)\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_forward_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[1], line 45\u001b[0m, in \u001b[0;36mdeberta.forward.<locals>.model_forward_input\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmodel_forward_input\u001b[39m(\u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(\u001b[38;5;28minput\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 45\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlast_hidden_state\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# print(output.shape)\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m# return self.model(**input).last_hidden_state.mean(dim=1)\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# print(output.shape)\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39msqueeze(output)\n",
      "File \u001b[0;32m~/co-training/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/co-training/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/co-training/env/lib/python3.8/site-packages/transformers/models/deberta/modeling_deberta.py:967\u001b[0m, in \u001b[0;36mDebertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    957\u001b[0m     token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    959\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    960\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    961\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    964\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m    965\u001b[0m )\n\u001b[0;32m--> 967\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    974\u001b[0m encoded_layers \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/co-training/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/co-training/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/co-training/env/lib/python3.8/site-packages/transformers/models/deberta/modeling_deberta.py:463\u001b[0m, in \u001b[0;36mDebertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001b[0m\n\u001b[1;32m    453\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    454\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    455\u001b[0m         next_kv,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    460\u001b[0m         output_attentions,\n\u001b[1;32m    461\u001b[0m     )\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 463\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnext_kv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[1;32m    473\u001b[0m     hidden_states, att_m \u001b[38;5;241m=\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/co-training/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/co-training/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/co-training/env/lib/python3.8/site-packages/transformers/models/deberta/modeling_deberta.py:376\u001b[0m, in \u001b[0;36mDebertaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    368\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    369\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    374\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    375\u001b[0m ):\n\u001b[0;32m--> 376\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[1;32m    385\u001b[0m         attention_output, att_matrix \u001b[38;5;241m=\u001b[39m attention_output\n",
      "File \u001b[0;32m~/co-training/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1528\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m             tracing_state\u001b[38;5;241m.\u001b[39mpop_scope()\n\u001b[1;32m   1526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m-> 1528\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrapped_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1529\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import sklearn.metrics\n",
    "\n",
    "best_accuracy = 0\n",
    "best_model_path = 'model.pt'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "\n",
    "    with tqdm.tqdm(train_dataloader) as tq:\n",
    "        for step, (input_nodes, output_nodes, mfgs) in enumerate(tq):\n",
    "            # print(111111)\n",
    "            # print(mfgs[0].srcdata)\n",
    "            inputs = mfgs[0].srcdata['x']\n",
    "            # break\n",
    "            # print(inputs.shape, input_nodes.shape)\n",
    "            inputs = [text[i] for i in output_nodes]\n",
    "            labels = mfgs[-1].dstdata['y']\n",
    "            \n",
    "            inputs = lm(inputs).to(device)\n",
    "            # mfgs = mfgs.to(device)\n",
    "            # print(inputs.shape, labels.shape)\n",
    "            # print(len(labels), inputs.shape, input_nodes.shape, output_nodes.shape)    \n",
    "            # break        \n",
    "            # predictions = model(mfgs, inputs, )\n",
    "            # print(inputs.device, model.device)\n",
    "            predictions = model(mfgs=mfgs, x=inputs, batch_size=16)\n",
    "            labels = torch.flatten(labels)\n",
    "            # print(predictions.device, labels.device)\n",
    "            loss = F.cross_entropy(predictions, labels)\n",
    "            # loss = torch.tensor(0.)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            accuracy = sklearn.metrics.accuracy_score(labels.cpu().numpy(), predictions.argmax(1).detach().cpu().numpy())\n",
    "\n",
    "            tq.set_postfix({'loss': '%.03f' % loss.item(), 'acc': '%.03f' % accuracy}, refresh=False)\n",
    "\n",
    "            del input_nodes, output_nodes, mfgs, inputs, labels, predictions, loss\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    with torch.no_grad() and tqdm.tqdm(valid_dataloader) as tq, torch.no_grad():\n",
    "        for input_nodes, output_nodes, mfgs in tq:\n",
    "            # inputs = [text[i] for i in input_nodes]\n",
    "            # print(type(mfgs[0]))\n",
    "            inputs = mfgs[0].srcdata['x']\n",
    "            labels.append(mfgs[-1].dstdata['y'].cpu().numpy())\n",
    "            # inputs = lm(inputs).to(device)\n",
    "            predictions.append(model(mfgs, inputs).argmax(1).cpu().numpy())\n",
    "        predictions = np.concatenate(predictions)\n",
    "        # print(predictions)\n",
    "        labels = np.concatenate(labels)\n",
    "        accuracy = sklearn.metrics.accuracy_score(labels, predictions)\n",
    "        # print('Epoch {} Validation Accuracy {}'.format(epoch, accuracy))\n",
    "        if best_accuracy < accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "        #     inputs = mfgs[0].srcdata['feat']\n",
    "        #     labels.append(mfgs[-1].dstdata['label'].cpu().numpy())\n",
    "        #     predictions.append(model(mfgs, inputs).argmax(1).cpu().numpy())\n",
    "        # predictions = np.concatenate(predictions)\n",
    "        # labels = np.concatenate(labels)\n",
    "        # accuracy = sklearn.metrics.accuracy_score(labels, predictions)\n",
    "        print('Epoch {} Validation Accuracy {} Best Accuracy {}'.format(epoch, accuracy, best_accuracy))\n",
    "        if best_accuracy < accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "\n",
    "        # Note that this tutorial do not train the whole model to the end.\n",
    "        # break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
