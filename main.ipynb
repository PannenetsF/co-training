{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import tqdm\n",
    "from typing import Optional\n",
    "from argparse import Namespace\n",
    "import json\n",
    "import types\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import dgl\n",
    "\n",
    "from cotraining import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_namespace(d):\n",
    "    \"\"\"\n",
    "    Recursively converts a dictionary to a SimpleNamespace.\n",
    "    \n",
    "    Args:\n",
    "        d (dict): The dictionary to convert.\n",
    "        \n",
    "    Returns:\n",
    "        SimpleNamespace: The converted namespace.\n",
    "    \"\"\"\n",
    "    if isinstance(d, dict):\n",
    "        # Convert sub-dictionaries to SimpleNamespace recursively\n",
    "        return types.SimpleNamespace(**{k: dict_to_namespace(v) for k, v in d.items()})\n",
    "    else:\n",
    "        # Return non-dictionary values as-is\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training params:\n",
    "config = {\n",
    "    \"seed\": 42,\n",
    "    \"device\": 'cuda',\n",
    "    \"epoch\": 2000,\n",
    "\n",
    "    \"lm_type\": 'deberta-base',\n",
    "    \"lm_lr\": 1e-4,\n",
    "    \"lm_max_length\": 512,\n",
    "    \"lm_weight_decay\": 1e-4,\n",
    "    \"lm_padding\": True,\n",
    "    \"lm_truncation\": True,\n",
    "    \"lm_requires_grad\": False,\n",
    "    \"pooler_hidden_size\": 128, \n",
    "    \"pooler_dropout\": 0.5,\n",
    "    \"pooler_hidden_act\": 'relu',\n",
    "\n",
    "    \"num_nodes\": 169343,\n",
    "    \"num_node_features\": 128,\n",
    "    \"gnn_h_feats\": 256,\n",
    "    \"gnn_lr\": 0.001,\n",
    "    \"gnn_weight_decay\": 0,\n",
    "    \"gnn_dropout\": 0.5,\n",
    "    \"gnn_requires_grad\": True,\n",
    "    \"gnn_num_layers\":7,\n",
    "\n",
    "    \"once_batch_size\": 64,\n",
    "    \"once_shuffle\": True,\n",
    "    \"once_drop_last\": True,\n",
    "\n",
    "    \"train_batch_size\": 1024,\n",
    "    \"train_shuffle\": True,\n",
    "    \"train_drop_last\": True,\n",
    "\n",
    "    \"valid_batch_size\": 1024,\n",
    "    \"valid_shuffle\": True,\n",
    "    \"valid_drop_last\": True,\n",
    "\n",
    "    \"test_batch_size\": 1024,\n",
    "    \"test_shuffle\": True,\n",
    "    \"test_drop_last\": True,\n",
    "}\n",
    "\n",
    "config = dict_to_namespace(config)\n",
    "config.epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(config.seed)\n",
    "\n",
    "\n",
    "# with open('config/arxiv.json') as file:\n",
    "#     config = json.loads(file.read())\n",
    "# config = dict_to_namespace(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = deberta(config).to(config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph, num_classes, text = load_data('ogbn-arxiv', use_dgl=True, use_text=True)\n",
    "graph = dgl.to_bidirected(graph, copy_ndata=True)\n",
    "graph = dgl.remove_self_loop(graph)\n",
    "graph = dgl.add_self_loop(graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = torch.load('arxiv_deberta.pt')\n",
    "# graph.ndata['x'] = torch.squeeze(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.ndata['x'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = graphsage(num_nodes=graph.num_nodes(), in_feats=lm.__num_node_features__, h_feats=64, num_classes=num_classes).to(config.device)\n",
    "model = graphsage(num_layers=config.gnn_num_layers, num_nodes=config.num_nodes, in_feats=config.num_node_features, h_feats=config.gnn_h_feats, num_classes=num_classes, dropout=config.gnn_dropout).to(config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in lm.parameters():\n",
    "    param.requires_grad = config.lm_requires_grad\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = config.gnn_requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt = torch.optim.Adam(list(model.parameters())+list(lm.parameters())) # \n",
    "opt = torch.optim.Adam([\n",
    "    {'params': lm.parameters(), 'lr': config.lm_lr, \"weight_decay\": config.lm_weight_decay},\n",
    "    {'params': model.parameters(), 'lr': config.gnn_lr, \"weight_decay\": config.gnn_weight_decay}])\n",
    "\n",
    "train_dataloader, valid_dataloader, test_dataloader = init_dataloader(graph, 'once', config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# forward_once(train_dataloader, model)\n",
    "# forward_once(valid_dataloader, model)\n",
    "# forward_once(test_dataloader, model)\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, valid_dataloader, test_dataloader = init_dataloader(graph, 'train', config), init_dataloader(graph, 'val', config), init_dataloader(graph, 'test', config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# with tqdm.tqdm(train_dataloader) as tq:\n",
    "#     for step, (input_nodes, output_nodes, mfgs) in enumerate(tq):\n",
    "#         # inputs = [text[i] for i in input_nodes]\n",
    "#         with torch.no_grad():\n",
    "#             # x = lm(inputs)\n",
    "#             x = mfgs[0].srcdata['x']\n",
    "#             model.forward_once(mfgs, x)\n",
    "\n",
    "# with tqdm.tqdm(valid_dataloader) as tq:\n",
    "#     for step, (input_nodes, output_nodes, mfgs) in enumerate(tq):\n",
    "#         # inputs = [text[i] for i in input_nodes]\n",
    "#         with torch.no_grad():\n",
    "#             # x = lm(inputs)\n",
    "#             x = mfgs[0].srcdata['x']\n",
    "#             model.forward_once(mfgs, x)\n",
    "\n",
    "# with tqdm.tqdm(test_dataloader) as tq:\n",
    "#     for step, (input_nodes, output_nodes, mfgs) in enumerate(tq):\n",
    "#         # inputs = [text[i] for i in input_nodes]\n",
    "#         with torch.no_grad():\n",
    "#             # x = lm(inputs)\n",
    "#             x = mfgs[0].srcdata['x']\n",
    "#             model.forward_once(mfgs, x)\n",
    "\n",
    "best_val_accuracy = 0.\n",
    "best_model_path = 'model.pt'\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "\n",
    "    with tqdm.tqdm(train_dataloader) as tq:\n",
    "        for step, (input_nodes, output_nodes, mfgs) in enumerate(tq):\n",
    "            # print(output_nodes)\n",
    "            # inputs = [text[i] for i in output_nodes]\n",
    "            labels = mfgs[-1].dstdata['y']\n",
    "            \n",
    "            # inputs = lm(inputs).to(config.device)\n",
    "            inputs = mfgs[0].srcdata['x']\n",
    "\n",
    "            # print(inputs.shape, input_nodes.shape, output_nodes.shape, labels.shape)\n",
    "\n",
    "            # predictions = model(mfgs=mfgs, x=inputs, batch_size=config.train_batch_size)\n",
    "            predictions = model(mfgs=mfgs, x=inputs)\n",
    "            labels = torch.flatten(labels)\n",
    "            # print(predictions.device, labels.device)\n",
    "            loss = F.cross_entropy(predictions, labels)\n",
    "            # loss = torch.tensor(0.)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            accuracy = sklearn.metrics.accuracy_score(labels.cpu().numpy(), predictions.argmax(1).detach().cpu().numpy())\n",
    "\n",
    "            tq.set_postfix({'loss': '%.03f' % loss.item(), 'acc': '%.03f' % accuracy}, refresh=False)\n",
    "\n",
    "            del input_nodes, output_nodes, mfgs, inputs, labels, predictions, loss\n",
    "            torch.cuda.empty_cache()\n",
    "            # print(torch.cuda.mem_get_info())\n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    with torch.no_grad() and tqdm.tqdm(valid_dataloader) as tq, torch.no_grad():\n",
    "        for input_nodes, output_nodes, mfgs in tq:\n",
    "            inputs = mfgs[0].srcdata['x']\n",
    "            labels.append(mfgs[-1].dstdata['y'].cpu().numpy())\n",
    "            # predictions.append(model(mfgs=mfgs, x=inputs, batch_size=config.valid_batch_size).argmax(1).cpu().numpy())\n",
    "            predictions.append(model(mfgs=mfgs, x=inputs).argmax(1).cpu().numpy())\n",
    "        predictions = np.concatenate(predictions)\n",
    "        labels = np.concatenate(labels)\n",
    "        val_accuracy = sklearn.metrics.accuracy_score(labels, predictions)\n",
    "        if best_val_accuracy <= val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            torch.save(model, best_model_path)\n",
    "\n",
    "    best_model = torch.load(best_model_path)\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    with torch.no_grad() and tqdm.tqdm(test_dataloader) as tq, torch.no_grad():\n",
    "        for input_nodes, output_nodes, mfgs in tq:\n",
    "            # inputs = [text[i] for i in input_nodes]\n",
    "            # print(type(mfgs[0]))\n",
    "            inputs = mfgs[0].srcdata['x']\n",
    "            labels.append(mfgs[-1].dstdata['y'].cpu().numpy())\n",
    "            # inputs = lm(inputs).to(device)\n",
    "            predictions.append(model(mfgs=mfgs, x=inputs).argmax(1).cpu().numpy())\n",
    "            # predictions.append(model(mfgs=mfgs, x=inputs, batch_size=config.test_batch_size).argmax(1).cpu().numpy())\n",
    "        predictions = np.concatenate(predictions)\n",
    "        # print(predictions)\n",
    "        labels = np.concatenate(labels)\n",
    "        test_accuracy = sklearn.metrics.accuracy_score(labels, predictions)\n",
    "\n",
    "        # with open('log.txt', 'a') as file:\n",
    "        #     file.write('Epoch {} Valid Accuracy {}  Best Accuracy {} Test Accuracy {}\\n'.format(epoch, val_accuracy, best_val_accuracy, test_accuracy))\n",
    "\n",
    "        print('Epoch {} Valid Accuracy {}  Best Accuracy {} Test Accuracy {}'.format(epoch, val_accuracy, best_val_accuracy, test_accuracy))\n",
    "        # Note that this tutorial do not train the whole model to the end.\n",
    "        # break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
