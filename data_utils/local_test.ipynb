{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/co-training/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    paper id                                              title  \\\n",
      "0   200971.0           ontology as a source for rule generation   \n",
      "1   549074.0  a novel methodology for thermal analysis a 3 d...   \n",
      "2   630234.0  spreadsheets on the move an evaluation of mobi...   \n",
      "3   803423.0  multi view metric learning for multi view vide...   \n",
      "4  1102481.0    big data analytics in future internet of things   \n",
      "\n",
      "                                                 abs  \n",
      "0  This paper discloses the potential of OWL (Web...  \n",
      "1  The semiconductor industry is reaching a fasci...  \n",
      "2  The power of mobile devices has increased dram...  \n",
      "3  Traditional methods on video summarization are...  \n",
      "4  Current research on Internet of Things (IoT) m...  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Data(num_nodes=169343, x=[169343, 128], node_year=[169343, 1], y=[169343, 1], adj_t=[169343, 169343, nnz=1166243], train_mask=[169343], val_mask=[169343], test_mask=[169343], edge_index=[169343, 169343, nnz=2315598]),\n",
       " [\"Title: evasion attacks against machine learning at test time\\nAbstract: In security-sensitive applications, the success of machine learning depends on a thorough vetting of their resistance to adversarial data. In one pertinent, well-motivated attack scenario, an adversary may attempt to evade a deployed system at test time by carefully manipulating attack samples. In this work, we present a simple but effective gradient-based approach that can be exploited to systematically assess the security of several, widely-used classification algorithms against evasion attacks. Following a recently proposed framework for security evaluation, we simulate attack scenarios that exhibit different risk levels for the classifier by increasing the attacker's knowledge of the system and her ability to manipulate attack samples. This gives the classifier designer a better picture of the classifier performance under evasion attacks, and allows him to perform a more informed model selection (or parameter setting). We evaluate our approach on the relevant security task of malware detection in PDF files, and show that such systems can be easily evaded. We also sketch some countermeasures suggested by our analysis.\",\n",
       "  'Title: how hard is computing parity with noisy communications\\nAbstract: We show a tight lower bound of $\\\\Omega(N \\\\log\\\\log N)$ on the number of transmissions required to compute the parity of $N$ input bits with constant error in a noisy communication network of $N$ randomly placed sensors, each having one input bit and communicating with others using local transmissions with power near the connectivity threshold. This result settles the lower bound question left open by Ying, Srikant and Dullerud (WiOpt 06), who showed how the sum of all the $N$ bits can be computed using $O(N \\\\log\\\\log N)$ transmissions. The same lower bound has been shown to hold for a host of other functions including majority by Dutta and Radhakrishnan (FOCS 2008). #R##N#Most works on lower bounds for communication networks considered mostly the full broadcast model without using the fact that the communication in real networks is local, determined by the power of the transmitters. In fact, in full broadcast networks computing parity needs $\\\\theta(N)$ transmissions. To obtain our lower bound we employ techniques developed by Goyal, Kindler and Saks (FOCS 05), who showed lower bounds in the full broadcast model by reducing the problem to a model of noisy decision trees. However, in order to capture the limited range of transmissions in real sensor networks, we adapt their definition of noisy decision trees and allow each node of the tree access to only a limited part of the input. Our lower bound is obtained by exploiting special properties of parity computations in such noisy decision trees.',\n",
       "  \"Title: on the absence of the rip in real world applications of compressed sensing and the rip in levels\\nAbstract: The purpose of this paper is twofold. The first is to point out that the Restricted Isometry Property (RIP) does not hold in many applications where compressed sensing is successfully used. This includes fields like Magnetic Resonance Imaging (MRI), Computerized Tomography, Electron Microscopy, Radio Interferometry and Fluorescence Microscopy. We demonstrate that for natural compressed sensing matrices involving a level based reconstruction basis (e.g. wavelets), the number of measurements required to recover all $s$-sparse signals for reasonable $s$ is excessive. In particular, uniform recovery of all $s$-sparse signals is quite unrealistic. This realisation shows that the RIP is insufficient for explaining the success of compressed sensing in various practical applications. The second purpose of the paper is to introduce a new framework based on a generalised RIP-like definition that fits the applications where compressed sensing is used. We show that the shortcomings that show that uniform recovery is unreasonable no longer apply if we instead ask for structured recovery that is uniform only within each of the levels. To examine this phenomenon, a new tool, termed the 'Restricted Isometry Property in Levels' is described and analysed. Furthermore, we show that with certain conditions on the Restricted Isometry Property in Levels, a form of uniform recovery within each level is possible. Finally, we conclude the paper by providing examples that demonstrate the optimality of the results obtained.\",\n",
       "  \"Title: a promise theory perspective on data networks\\nAbstract: Networking is undergoing a transformation throughout our industry. The shift from hardware driven products with ad hoc control to Software Defined Networks is now well underway. In this paper, we adopt the perspective of the Promise Theory to examine the current state of networking technologies so that we might see beyond specific technologies to principles for building flexible and scalable networks. Today's applications are increasingly distributed planet-wide in cloud-like hosting environments. Promise Theory's bottom-up modelling has been applied to server management for many years and lends itself to principles of self-healing, scalability and robustness.\",\n",
       "  \"Title: analysis of asymptotically optimal sampling based motion planning algorithms for lipschitz continuous dynamical systems\\nAbstract: Over the last 20 years significant effort has been dedicated to the development of sampling-based motion planning algorithms such as the Rapidly-exploring Random Trees (RRT) and its asymptotically optimal version (e.g. RRT*). However, asymptotic optimality for RRT* only holds for linear and fully actuated systems or for a small number of non-linear systems (e.g. Dubin's car) for which a steering function is available. The purpose of this paper is to show that asymptotically optimal motion planning for dynamical systems with differential constraints can be achieved without the use of a steering function. We develop a novel analysis on sampling-based planning algorithms that sample the control space. This analysis demonstrated that asymptotically optimal path planning for any Lipschitz continuous dynamical system can be achieved by sampling the control space directly. We also determine theoretical bounds on the convergence rates for this class of algorithms. As the number of iterations increases, the trajectory generated by these algorithms, approaches the optimal control trajectory, with probability one. Simulation results are promising.\",\n",
       "  'Title: the edge group coloring problem with applications to multicast switching\\nAbstract: This paper introduces a natural generalization of the classical edge coloring problem in graphs that provides a useful abstraction for two well-known problems in multicast switching. We show that the problem is NP-hard and evaluate the performance of several approximation algorithms, both analytically and experimentally. We find that for random $\\\\chi$-colorable graphs, the number of colors used by the best algorithms falls within a small constant factor of $\\\\chi$, where the constant factor is mainly a function of the ratio of the number of outputs to inputs. When this ratio is less than 10, the best algorithms produces solutions that use fewer than $2\\\\chi$ colors. In addition, one of the algorithms studied finds high quality approximate solutions for any graph with high probability, where the probability of a low quality solution is a function only of the random choices made by the algorithm.',\n",
       "  'Title: webvrgis based city bigdata 3d visualization and analysis\\nAbstract: This paper shows the WEBVRGIS platform overlying multiple types of data about Shenzhen over a 3d globe. The amount of information that can be visualized with this platform is overwhelming, and the GIS-based navigational scheme allows to have great flexibility to access the different available data sources. For example,visualising historical and forecasted passenger volume at stations could be very helpful when overlaid with other social data.',\n",
       "  'Title: information theoretic authentication and secrecy codes in the splitting model\\nAbstract: In the splitting model, information theoretic authentication codes allow non-deterministic encoding, that is, several messages can be used to communicate a particular plaintext. Certain applications require that the aspect of secrecy should hold simultaneously. Ogata-Kurosawa-Stinson-Saido (2004) have constructed optimal splitting authentication codes achieving perfect secrecy for the special case when the number of keys equals the number of messages. In this paper, we establish a construction method for optimal splitting authentication codes with perfect secrecy in the more general case when the number of keys may differ from the number of messages. To the best knowledge, this is the first result of this type.',\n",
       "  'Title: whealth transforming telehealth services\\nAbstract: A worldwide increase in proportions of older people in the population poses the challenge of managing their increasing healthcare needs within limited resources. To achieve this many countries are interested in adopting telehealth technology. Several shortcomings of state-of-the-art telehealth technology constrain widespread adoption of telehealth services. We present an ensemble-sensing framework - wHealth (short form of wireless health) for effective delivery of telehealth services. It extracts personal health information using sensors embedded in everyday devices and allows effective and seamless communication between patients and clinicians. Due to the non-stigmatizing design, ease of maintenance, simplistic interaction and seamless intervention, our wHealth platform has the potential to enable widespread adoption of telehealth services for managing elderly healthcare. We discuss the key barriers and potential solutions to make the wHealth platform a reality.',\n",
       "  'Title: nonparametric decentralized sequential detection via universal source coding\\nAbstract: We consider nonparametric or universal sequential hypothesis testing problem when the distribution under the null hypothesis is fully known but the alternate hypothesis corresponds to some other unknown distribution. These algorithms are primarily motivated fr om spectrum sensing in Cognitive Radios and intruder detection in wireless sensor networks. We use easily implementable universal lossless source codes to propose simple algorithms for such a setup. The algorithms are first proposed for discrete alphabet. Their performance and asymptotic properties are studied theoretically. Later these are extended to continuous alphabets. Their performance with two well known universal source codes, Lempel-Ziv code and Krichevsky-Trofimov estimator with Arithmetic Enc oder are compared. These algorithms are also compared with the tests using various other nonparametric estimators. Finally a decentralized version utilizing spatial diversity is also proposed. Its performa nce is analysed and asymptotic properties are proved.',\n",
       "  'Title: online learning in decentralized multiuser resource sharing problems\\nAbstract: In this paper, we consider the general scenario of resource sharing in a decentralized system when the resource rewards/qualities are time-varying and unknown to the users, and using the same resource by multiple users leads to reduced quality due to resource sharing. Firstly, we consider a user-independent reward model with no communication between the users, where a user gets feedback about the congestion level in the resource it uses. Secondly, we consider user-specific rewards and allow costly communication between the users. The users have a cooperative goal of achieving the highest system utility. There are multiple obstacles in achieving this goal such as the decentralized nature of the system, unknown resource qualities, communication, computation and switching costs. We propose distributed learning algorithms with logarithmic regret with respect to the optimal allocation. Our logarithmic regret result holds under both i.i.d. and Markovian reward models, as well as under communication, computation and switching costs.',\n",
       "  \"Title: truthful secretaries with budgets\\nAbstract: We study online auction settings in which agents arrive and depart dynamically in a random (secretary) order, and each agent's private type consists of the agent's arrival and departure times, value and budget. We consider multi-unit auctions with additive agents for the allocation of both divisible and indivisible items. For both settings, we devise truthful mechanisms that give a constant approximation with respect to the auctioneer's revenue, under a large market assumption. For divisible items, we devise in addition a truthful mechanism that gives a constant approximation with respect to the liquid welfare --- a natural efficiency measure for budgeted settings introduced by Dobzinski and Paes Leme [ICALP'14]. Our techniques provide high-level principles for transforming offline truthful mechanisms into online ones, with or without budget constraints. To the best of our knowledge, this is the first work that addresses the non-trivial challenge of combining online settings with budgeted agents.\",\n",
       "  'Title: improving the bound on the rip constant in generalized orthogonal matching pursuit\\nAbstract: The generalized Orthogonal Matching Pursuit (gOMP) is a recently proposed compressive sensing greedy recovery algorithm which generalizes the OMP algorithm by selecting N( ≥ 1) atoms in each iteration. In this letter, we demonstrate that the gOMP can successfully reconstruct a K-sparse signal from a compressed measurement y=Φx by a maximum of K iterations if the sensing matrix Φ satisfies the Restricted Isometry Property (RIP) of order NK, with the RIP constant δNK satisfying δNK <; √N/√K+2√N. The proposed bound is an improvement over the existing bound on δNK. We also show that by increasing the RIP order just by one (i.e., NK+1 from NK), it is possible to refine the bound further to δNK+1 <; √N/√K+√N, which is consistent (for N=1) with the near optimal bound on δK+1 in OMP.',\n",
       "  \"Title: a system for reflection in c\\nAbstract: Object-oriented programming languages such as Java and Objective C have become popular for implementing agent-based and other object-based simulations since objects in those languages can {\\\\em reflect} (i.e. make runtime queries of an object's structure). This allows, for example, a fairly trivial {\\\\em serialisation} routine (conversion of an object into a binary representation that can be stored or passed over a network) to be written. However C++ does not offer this ability, as type information is thrown away at compile time. Yet C++ is often a preferred development environment, whether for performance reasons or for its expressive features such as operator overloading. #R##N#In this paper, we present the {\\\\em Classdesc} system which brings many of the benefits of object reflection to C++.\",\n",
       "  'Title: a bi level view of inpainting based image compression\\nAbstract: Inpainting based image compression approaches, especially linear and non-linear diffusion models, are an active research topic for lossy image compression. The major challenge in these compression models is to find a small set of descriptive supporting points, which allow for an accurate reconstruction of the original image. It turns out in practice that this is a challenging problem even for the simplest Laplacian interpolation model. In this paper, we revisit the Laplacian interpolation compression model and introduce two fast algorithms, namely successive preconditioning primal dual algorithm and the recently proposed iPiano algorithm, to solve this problem efficiently. Furthermore, we extend the Laplacian interpolation based compression model to a more general form, which is based on principles from bi-level optimization. We investigate two different variants of the Laplacian model, namely biharmonic interpolation and smoothed Total Variation regularization. Our numerical results show that significant improvements can be obtained from the biharmonic interpolation model, and it can recover an image with very high quality from only 5% pixels.',\n",
       "  'Title: distributed graph automata\\nAbstract: Combining ideas from distributed algorithms and alternating automata, we introduce a new class of finite graph automata that recognize precisely the languages of finite graphs definable in monadic second-order logic. By restricting transitions to be nondeterministic or deterministic, we also obtain two strictly weaker variants of our automata for which the emptiness problem is decidable.',\n",
       "  'Title: randomness efficient rumor spreading\\nAbstract: We study the classical rumor spreading problem, which is used to spread information in an unknown network with $n$ nodes. We present the first protocol for any expander graph $G$ with $n$ nodes and minimum degree $\\\\Theta(n)$ such that, the protocol informs every node in $O(\\\\log n)$ rounds with high probability, and uses $O(\\\\log n\\\\log\\\\log n)$ random bits in total. The runtime of our protocol is tight, and the randomness requirement of $O(\\\\log n\\\\log\\\\log n)$ random bits almost matches the lower bound of $\\\\Omega(\\\\log n)$ random bits. We further study rumor spreading protocols for more general graphs, and for several graph topologies our protocols are as fast as the classical protocol and use $\\\\tilde{O}(\\\\log n)$ random bits in total, in contrast to $O(n\\\\log^2n)$ random bits used in the well-known rumor spreading push protocol. These results together give us almost full understanding of the randomness requirement for this basic epidemic process. #R##N#Our protocols rely on a novel reduction between rumor spreading processes and branching programs, and this reduction provides a general framework to derandomize these complex and distributed epidemic processes. Interestingly, one cannot simply apply PRGs for branching programs as rumor spreading process is not characterized by small-space computation. Our protocols require the composition of several pseudorandom objects, e.g. pseudorandom generators, and pairwise independent generators. Besides designing rumor spreading protocols, the techniques developed here may have applications in studying the randomness complexity of distributed algorithms.',\n",
       "  'Title: back to the past source identification in diffusion networks from partially observed cascades\\nAbstract: When a piece of malicious information becomes rampant in an information diffusion network, can we identify the source node that originally introduced the piece into the network and infer the time when it initiated this? Being able to do so is critical for curtailing the spread of malicious information, and reducing the potential losses incurred. This is a very challenging problem since typically only incomplete traces are observed and we need to unroll the incomplete traces into the past in order to pinpoint the source. In this paper, we tackle this problem by developing a two-stage framework, which first learns a continuous-time diffusion network model based on historical diffusion traces and then identifies the source of an incomplete diffusion trace by maximizing the likelihood of the trace under the learned model. Experiments on both large synthetic and real-world data show that our framework can effectively go back to the past, and pinpoint the source node and its initiation time significantly more accurately than previous state-of-the-arts.',\n",
       "  'Title: bayesian two sample tests\\nAbstract: In this paper, we present two classes of Bayesian approaches to the two-sample problem. Our first class of methods extends the Bayesian t-test to include all parametric models in the exponential family and their conjugate priors. Our second class of methods uses Dirichlet process mixtures (DPM) of such conjugate-exponential distributions as flexible nonparametric priors over the unknown distributions.',\n",
       "  'Title: electrical structure based pmu placement in electric power systems\\nAbstract: Recent work on complex networks compared the topological and electrical structures of the power grid, taking into account the underlying physical laws that govern the electrical connectivity between various components in the network. A distance metric, namely, resistance distance was introduced to provide a more comprehensive description of interconnections in power systems compared with the topological structure, which is based only on geographic connections between network components. Motivated by these studies, in this paper we revisit the phasor measurement unit (PMU) placement problem by deriving the connectivity matrix of the network using resistance distances between buses in the grid, and use it in the integer program formulations for several standard IEEE bus systems. The main result of this paper is rather discouraging: more number of PMUs are required, compared with those obtained using the topological structure, to meet the desired objective of complete network observability without zero injection measurements. However, in light of recent advances in the electrical structure of the grid, our study provides a more realistic perspective of PMU placement in power systems. By further exploring the connectivity matrix derived using the electrical structure, we devise a procedure to solve the placement problem without resorting to linear programming.',\n",
       "  'Title: on state dependent broadcast channels with cooperation\\nAbstract: In this paper, we investigate problems of communication over physically degraded, state-dependent broadcast channels (BCs) with cooperating decoders. Two different setups are considered and their capacity regions are characterized. First, we study a setting in which one decoder can use a finite capacity link to send the other decoder information regarding the messages or the channel states. In this scenario we analyze two cases: one where noncausal state information is available to the encoder and the strong decoder and the other where state information is available only to the encoder in a causal manner. Second, we examine a setting in which the cooperation between the decoders is limited to taking place before the outputs of the channel are given. In this case, one decoder, which is informed of the state sequence noncausally, can cooperate only to send the other decoder rate-limited information about the state sequence. The proofs of the capacity regions introduce a new method of coding for channels with cooperation between different users, where we exploit the link between the decoders for multiple-binning. Finally, we discuss the optimality of using rate splitting techniques when coding for cooperative BCs. In particular, we show that rate splitting is not necessarily optimal when coding for cooperative BCs by solving an example in which our method of coding outperforms rate splitting.',\n",
       "  'Title: detecting simultaneous integer relations for several real vectors\\nAbstract: An algorithm which either finds an nonzero integer vector m for given t real n-dimensional vectors x1,��� , xt such that x T m = 0 or proves that no such integer vector with norm less than a given bound exists is presented in this paper. The cost of the algorithm is at mostO(n 4 + n 3 log�(X)) exact arithmetic operations in dimension n and the least Euclidean norm�(X) of such integer vectors. It matches the best complexity upper bound known for this problem. Experimental data show that the algorithm is better than an already existi ng algorithm in the literature. In application, the algorit hm is used to get a complete method for finding the minimal polyno mial of an unknown complex algebraic number from its approximation, which runs even faster than the corresponding Maple built-in function.',\n",
       "  \"Title: shannon meets carnot mutual information via thermodynamics\\nAbstract: In this contribution, the Gaussian channel is represented as an equivalent thermal system allowing to express its input-output mutual information in terms of thermodynamic quantities. This thermodynamic description of the mutual information is based upon a generalization of the $2^{nd}$ thermodynamic law and provides an alternative proof to the Guo-Shamai-Verd\\\\'{u} theorem, giving an intriguing connection between this remarkable theorem and the most fundamental laws of nature - the laws of thermodynamics.\",\n",
       "  'Title: on list decodability of random rank metric codes\\nAbstract: In the present paper, we consider list decoding for both random rank metric codes and random linear rank metric codes. Firstly, we show that, for arbitrary $0 0$ ($\\\\epsilon$ and $R$ are independent), if $0 0$ and any $0<\\\\rho<1$, with high probability a random $F_q$-linear rank metric codes with rate $R=(1-\\\\rho)(1-b\\\\rho)-\\\\epsilon$ can be list decoded up to a fraction $\\\\rho$ of rank errors with constant list size $L$ satisfying $L\\\\leq O(\\\\exp(1/\\\\epsilon))$.',\n",
       "  'Title: dealing with run time variability in service robotics towards a dsl for non functional properties\\nAbstract: Service robots act in open-ended, natural environments. Therefore, due to combinatorial explosion of potential situations, it is not possible to foresee all eventualities in advance during robot design. In addition, due to limited resources on a mobile robot, it is not feasible to plan any action on demand. Hence, it is necessary to provide a mechanism to express variability at design-time that can be efficiently resolved on the robot at run-time based on the then available information. In this paper, we introduce a DSL to express run- time variability focused on the execution quality of the robot (in terms of non-functional properties like safety and task efficiency) under changing situations and limited resources. We underpin the applicability of our approach by an example integrated into an overall robotics architecture.',\n",
       "  'Title: a characterisation of context sensitive languages by consensus games\\nAbstract: We propose a game for recognising formal languages, in which two players with imperfect information need to coordinate on a common decision, given private input information. The players have a joint objec- tive to avoid an inadmissible decision, in spite of the uncertainty induced by the input. We show that this model of consensus acceptor games characterises context-sensitive languages, and conversely, that winning strategies in such games can be described by context-sensitive languages. This im- plies that it is undecidable whether a consensus game admits a winning strategy, and, even if so, it is PSPACE-hard to execute one. On the pos- itive side, we show that whenever a winning strategy exists, there exists one that can be implemented by a linear bounded automaton.',\n",
       "  'Title: data structures for approximate range counting\\nAbstract: We present new data structures for approximately counting the number of points in orthogonal range. #R##N#There is a deterministic linear space data structure that supports updates in O(1) time and approximates the number of elements in a 1-D range up to an additive term $k^{1/c}$ in $O(\\\\log \\\\log U\\\\cdot\\\\log \\\\log n)$ time, where $k$ is the number of elements in the answer, $U$ is the size of the universe and $c$ is an arbitrary fixed constant. We can estimate the number of points in a two-dimensional orthogonal range up to an additive term $ k^{\\\\rho}$ in $O(\\\\log \\\\log U+ (1/\\\\rho)\\\\log\\\\log n)$ time for any $\\\\rho>0$. We can estimate the number of points in a three-dimensional orthogonal range up to an additive term $k^{\\\\rho}$ in $O(\\\\log \\\\log U + (\\\\log\\\\log n)^3+ (3^v)\\\\log\\\\log n)$ time for $v=\\\\log \\\\frac{1}{\\\\rho}/\\\\log {3/2}+2$.',\n",
       "  'Title: holographic transformation for quantum factor graphs\\nAbstract: Recently, a general tool called a holographic transformation, which transforms an expression of the partition function to another form, has been used for polynomial-time algorithms and for improvement and understanding of the belief propagation. In this work, the holographic transformation is generalized to quantum factor graphs.',\n",
       "  'Title: rooted trees with probabilities revisited\\nAbstract: Rooted trees with probabilities are convenient to represent a class of random processes with memory. They allow to describe and analyze variable length codes for data compression and distribution matching. In this work, the Leaf-Average Node-Sum Interchange Theorem (LANSIT) and the well-known applications to path length and leaf entropy are re-stated. The LANSIT is then applied to informational divergence. Next, the dierential LANSIT is derived, which allows to write normalized functionals of leaf distributions as an average of functionals of branching distributions. Joint distributions of random variables and the corresponding conditional distributions are special cases of leaf distributions and branching distributions. Using the dierential LANSIT, Pinsker’s inequality is formulated for rooted trees with probabilities, with an application to the approximation of product distributions. In particular, it is shown that if the normalized informational divergence of a distribution and a product distribution approaches zero, then the entropy rate approaches the entropy rate of the product distribution.',\n",
       "  'Title: time critical social mobilization\\nAbstract: The World Wide Web is commonly seen as a platform that can harness the collective abilities of large numbers of people to accomplish tasks with unprecedented speed, accuracy, and scale. To explore the Web’s ability for social mobilization, the Defense Advanced Research Projects Agency (DARPA) held the DARPA Network Challenge, in which competing teams were asked to locate 10 red weather balloons placed at locations around the continental United States. Using a recursive incentive mechanism that both spread information about the task and incentivized individuals to act, our team was able to find all 10 balloons in less than 9 hours, thus winning the Challenge. We analyzed the theoretical and practical properties of this mechanism and compared it with other approaches.',\n",
       "  'Title: homomorphic encryption theory and application\\nAbstract: The goal of this chapter is to present a survey of homomorphic encryption techniques and their applications. After a detailed discussion on the introduction and motivation of the chapter, we present some basic concepts of cryptography. The fundamental theories of homomorphic encryption are then discussed with suitable examples. The chapter then provides a survey of some of the classical homomorphic encryption schemes existing in the current literature. Various applications and salient properties of homomorphic encryption schemes are then discussed in detail. The chapter then introduces the most important and recent research direction in the filed - fully homomorphic encryption. A significant number of propositions on fully homomorphic encryption is then discussed. Finally, the chapter concludes by outlining some emerging research trends in this exicting field of cryptography.',\n",
       "  'Title: learning transformations for clustering and classification\\nAbstract: A low-rank transformation learning framework for subspace clustering and classification is here proposed. Many high-dimensional data, such as face images and motion sequences, approximately lie in a union of low-dimensional subspaces. The corresponding subspace clustering problem has been extensively studied in the literature to partition such high-dimensional data into clusters corresponding to their underlying low-dimensional subspaces. However, low-dimensional intrinsic structures are often violated for real-world observations, as they can be corrupted by errors or deviate from ideal models. We propose to address this by learning a linear transformation on subspaces using matrix rank, via its convex surrogate nuclear norm, as the optimization criteria. The learned linear transformation restores a low-rank structure for data from the same subspace, and, at the same time, forces a a maximally separated structure for data from different subspaces. In this way, we reduce variations within subspaces, and increase separation between subspaces for a more robust subspace clustering. This proposed learned robust subspace clustering framework significantly enhances the performance of existing subspace clustering methods. Basic theoretical results here presented help to further support the underlying framework. To exploit the low-rank structures of the transformed subspaces, we further introduce a fast subspace clustering technique, which efficiently combines robust PCA with sparse modeling. When class labels are present at the training stage, we show this low-rank transformation framework also significantly enhances classification performance. Extensive experiments using public datasets are presented, showing that the proposed approach significantly outperforms state-of-the-art methods for subspace clustering and classification.',\n",
       "  'Title: methods for integrating knowledge with the three weight optimization algorithm for hybrid cognitive processing\\nAbstract: In this paper we consider optimization as an approach for quickly and flexibly developing hybrid cognitive capabilities that are efficient, scalable, and can exploit knowledge to improve solution speed and quality. In this context, we focus on the Three-Weight Algorithm, which aims to solve general optimization problems. We propose novel methods by which to integrate knowledge with this algorithm to improve expressiveness, efficiency, and scaling, and demonstrate these techniques on two example problems (Sudoku and circle packing).',\n",
       "  'Title: csma local area networking under dynamic altruism\\nAbstract: In this paper, we consider medium access control of local area networks (LANs) under limitedinformation conditions as befits a distributed system. Rather than assuming “by rule” conformance to a protocol designed to regulate packet-flow rates (e.g., CSMA windowing), we begin with a noncooperative game framework and build a dynamic altruism term into the net utility. The effects of altruism are analyzed at Nash equilibrium for both the ALOHA and CSMA frameworks in the quasistationary (fictitious play) regime. We consider either power or throughput based costs of networking, and the cases of identical or heterogeneous (independent) users/players. In a numerical study we consider diverse players, and we see that the effects of altruism for similar players can be beneficial in the presence of significant congestion, but excessive altruism may lead to underuse of the channel when demand is low.',\n",
       "  'Title: face frontalization for alignment and recognition\\nAbstract: Recently, it was shown that excellent results can be achieved in both face landmark localization and pose-invariant face recognition. These breakthroughs are attributed to the efforts of the community to manually annotate facial images in many different poses and to collect 3D faces data. In this paper, we propose a novel method for joint face landmark localization and frontal face reconstruction (pose correction) using a small set of frontal images only. By observing that the frontal facial image is the one with the minimum rank from all different poses we formulate an appropriate model which is able to jointly recover the facial landmarks as well as the frontalized version of the face. To this end, a suitable optimization problem, involving the minimization of the nuclear norm and the matrix $\\\\ell_1$ norm, is solved. The proposed method is assessed in frontal face reconstruction (pose correction), face landmark localization, and pose-invariant face recognition and verification by conducting experiments on $6$ facial images databases. The experimental results demonstrate the effectiveness of the proposed method.',\n",
       "  'Title: from bounded affine types to automatic timing analysis\\nAbstract: Bounded linear types have proved to be useful for automated resource analysis and control in functional programming languages. In this paper we introduce an affine bounded linear typing discipline on a general notion of resource which can be modeled in a semiring. For this type system we provide both a general type-inference procedure, parameterized by the decision procedure of the semiring equational theory, and a (coherent) categorical semantics. This is a very useful type-theoretic and denotational framework for many applications to resource-sensitive compilation, and it represents a generalization of several existing type systems. As a non-trivial instance, motivated by our ongoing work on hardware compilation, we present a complex new application to calculating and controlling timing of execution in a (recursion-free) higher-order functional programming language with local store.',\n",
       "  'Title: an efficient way to perform the assembly of finite element matrices in matlab and octave\\nAbstract: We describe different optimization techniques to perform the assembly of finite element matrices in Matlab and Octave, from the standard approach to recent vectorized ones, without any low level language used. We finally obtain a simple and efficient vectorized algorithm able to compete in performance with dedicated software such as FreeFEM++. The principle of this assembly algorithm is general, we present it for different matrices in the P1 finite elements case and in linear elasticity. We present numerical results which illustrate the computational costs of the different approaches',\n",
       "  'Title: how auto encoders could provide credit assignment in deep networks via target propagation\\nAbstract: We propose to exploit {\\\\em reconstruction} as a layer-local training signal for deep learning. Reconstructions can be propagated in a form of target propagation playing a role similar to back-propagation but helping to reduce the reliance on derivatives in order to perform credit assignment across many levels of possibly strong non-linearities (which is difficult for back-propagation). A regularized auto-encoder tends produce a reconstruction that is a more likely version of its input, i.e., a small move in the direction of higher likelihood. By generalizing gradients, target propagation may also allow to train deep networks with discrete hidden units. If the auto-encoder takes both a representation of input and target (or of any side information) in input, then its reconstruction of input representation provides a target towards a representation that is more likely, conditioned on all the side information. A deep auto-encoder decoding path generalizes gradient propagation in a learned way that can could thus handle not just infinitesimal changes but larger, discrete changes, hopefully allowing credit assignment through a long chain of non-linear operations. In addition to each layer being a good auto-encoder, the encoder also learns to please the upper layers by transforming the data into a space where it is easier to model by them, flattening manifolds and disentangling factors. The motivations and theoretical justifications for this approach are laid down in this paper, along with conjectures that will have to be verified either mathematically or experimentally, including a hypothesis stating that such auto-encoder mediated target propagation could play in brains the role of credit assignment through many non-linear, noisy and discrete transformations.',\n",
       "  'Title: constrained parametric proposals and pooling methods for semantic segmentation in rgb d images\\nAbstract: We focus on the problem of semantic segmentation based on RGB-D data, with emphasis on analyzing cluttered indoor scenes containing many instances from many visual categories. Our approach is based on a parametric figure-ground intensity and depth-constrained proposal process that generates spatial layout hypotheses at multiple locations and scales in the image followed by a sequential inference algorithm that integrates the proposals into a complete scene estimate. Our contributions can be summarized as proposing the following: (1) a generalization of parametric max flow figure-ground proposal methodology to take advantage of intensity and depth information, in order to systematically and efficiently generate the breakpoints of an underlying spatial model in polynomial time, (2) new region description methods based on second-order pooling over multiple features constructed using both intensity and depth channels, (3) an inference procedure that can resolve conflicts in overlapping spatial partitions, and handles scenes with a large number of objects category instances, of very different scales, (4) extensive evaluation of the impact of depth, as well as the effectiveness of a large number of descriptors, both pre-designed and automatically obtained using deep learning, in a difficult RGB-D semantic segmentation problem with 92 classes. We report state of the art results in the challenging NYU Depth v2 dataset, extended for RMRC 2013 Indoor Segmentation Challenge, where currently the proposed model ranks first, with an average score of 24.61% and a number of 39 classes won. Moreover, we show that by combining second-order and deep learning features, over 15% relative accuracy improvements can be additionally achieved. In a scene classification benchmark, our methodology further improves the state of the art by 24%.',\n",
       "  'Title: cooperative game theoretic solution concepts for top k problems\\nAbstract: The problem of finding the $k$ most critical nodes, referred to as the $top\\\\text{-}k$ problem, is a very important one in several contexts such as information diffusion and preference aggregation in social networks, clustering of data points, etc. It has been observed in the literature that the value allotted to a node by most of the popular cooperative game theoretic solution concepts, acts as a good measure of appropriateness of that node (or a data point) to be included in the $top\\\\text{-}k$ set, by itself. However, in general, nodes having the highest $k$ values are not the desirable $top\\\\text{-}k$ nodes, because the appropriateness of a node to be a part of the $top\\\\text{-}k$ set depends on other nodes in the set. As this is not explicitly captured by cooperative game theoretic solution concepts, it is necessary to post-process the obtained values in order to output the suitable $top\\\\text{-}k$ nodes. In this paper, we propose several such post-processing methods and give reasoning behind each of them, and also propose a standalone algorithm that combines cooperative game theoretic solution concepts with the popular greedy hill-climbing algorithm.',\n",
       "  'Title: regulation and the integrity of spreadsheets in the information supply chain\\nAbstract: Spreadsheets provide many of the key links between information systems, closing the gap between business needs and the capability of central systems. Recent regulations have brought these vulnerable parts of information supply chains into focus. The risk they present to the organisation depends on the role that they fulfil, with generic differences between their use as modeling tools and as operational applications. Four sections of the Sarbanes-Oxley Act (SOX) are particularly relevant to the use of spreadsheets. Compliance with each of these sections is dependent on maintaining the integrity of those spreadsheets acting as operational applications. This can be achieved manually but at high cost. There are a range of commercially available off-the-shelf solutions that can reduce this cost. These may be divided into those that assist in the debugging of logic and more recently the arrival of solutions that monitor the change and user activity taking place in business-critical spreadsheets. ClusterSeven provides one of these monitoring solutions, highlighting areas of operational risk whilst also establishing a database of information to deliver new business intelligence.',\n",
       "  'Title: reconfigurable wireless networks\\nAbstract: Driven by the advent of sophisticated and ubiquitous applications, and the ever-growing need for information, wireless networks are without a doubt steadily evolving into profoundly more complex and dynamic systems. The user demands are progressively rampant, while application requirements continue to expand in both range and diversity. Future wireless networks, therefore, must be equipped with the ability to handle numerous, albeit challenging, requirements. Network reconfiguration, considered as a prominent network paradigm, is envisioned to play a key role in leveraging future network performance and considerably advancing current user experiences. This paper presents a comprehensive overview of reconfigurable wireless networks and an in-depth analysis of reconfiguration at all layers of the protocol stack. Such networks characteristically possess the ability to reconfigure and adapt their hardware and software components and architectures, thus enabling flexible delivery of broad services, as well as sustaining robust operation under highly dynamic conditions. The paper offers a unifying framework for research in reconfigurable wireless networks. This should provide the reader with a holistic view of concepts, methods, and strategies in reconfigurable wireless networks. Focus is given to reconfigurable systems in relatively new and emerging research areas such as cognitive radio networks, cross-layer reconfiguration, and software-defined networks. In addition, modern networks have to be intelligent and capable of self-organization. Thus, this paper discusses the concept of network intelligence as a means to enable reconfiguration in highly complex and dynamic networks. Key processes in network intelligence, such as reasoning, learning, and context awareness, are presented to illustrate how these methods can take reconfiguration to a new level. Finally, the paper is supported with several examples and case studies showing the tremendous impact of reconfiguration on wireless networks.',\n",
       "  'Title: contact representations of sparse planar graphs\\nAbstract: We study representations of graphs by contacts of circular arcs, CCA-representations for short, where the vertices are interior-disjoint circular arcs in the plane and each edge is realized by an endpoint of one arc touching the interior of another. A graph is (2,k)-sparse if every s-vertex subgraph has at most 2s - k edges, and (2, k)-tight if in addition it has exactly 2n - k edges, where n is the number of vertices. Every graph with a CCA- representation is planar and (2, 0)-sparse, and it follows from known results on contacts of line segments that for k >= 3 every (2, k)-sparse graph has a CCA-representation. Hence the question of CCA-representability is open for (2, k)-sparse graphs with 0 <= k <= 2. We partially answer this question by computing CCA-representations for several subclasses of planar (2,0)-sparse graphs. In particular, we show that every plane (2, 2)-sparse graph has a CCA-representation, and that any plane (2, 1)-tight graph or (2, 0)-tight graph dual to a (2, 3)-tight graph or (2, 4)-tight graph has a CCA-representation. Next, we study CCA-representations in which each arc has an empty convex hull. We characterize the plane graphs that have such a representation, based on the existence of a special orientation of the graph edges. Using this characterization, we show that every plane graph of maximum degree 4 has such a representation, but that finding such a representation for a plane (2, 0)-tight graph with maximum degree 5 is an NP-complete problem. Finally, we describe a simple algorithm for representing plane (2, 0)-sparse graphs with wedges, where each vertex is represented with a sequence of two circular arcs (straight-line segments).',\n",
       "  'Title: modularity aspects of disjunctive stable models\\nAbstract: Practically all programming languages allow the programmer to split a program into several modules which brings along several advantages in software development. In this paper, we are interested in the area of answer-set programming where fully declarative and nonmonotonic languages are applied. In this context, obtaining a modular structure for programs is by no means straightforward since the output of an entire program cannot in general be composed from the output of its components. To better understand the effects of disjunctive information on modularity we restrict the scope of analysis to the case of disjunctive logic programs (DLPs) subject to stable-model semantics. We define the notion of a DLP-function, where a well-defined input/output interface is provided, and establish a novel module theorem which indicates the compositionality of stable-model semantics for DLP-functions. The module theorem extends the well-known splitting-set theorem and enables the decomposition of DLP-functions given their strongly connected components based on positive dependencies induced by rules. In this setting, it is also possible to split shared disjunctive rules among components using a generalized shifting technique. The concept of modular equivalence is introduced for the mutual comparison of DLP-functions using a generalization of a translation-based verification method.',\n",
       "  'Title: replica symmetric bound for restricted isometry constant\\nAbstract: We develop a method for evaluating restricted isometry constants (RICs). This evaluation is reduced to the identification of the zero-points of entropy, which is defined for submatrices that are composed of columns selected from a given measurement matrix. Using the replica method developed in statistical mechanics, we assess RICs for Gaussian random matrices under the replica symmetric (RS) assumption. In order to numerically validate the adequacy of our analysis, we employ the exchange Monte Carlo (EMC) method, which has been empirically demonstrated to achieve much higher numerical accuracy than naive Monte Carlo methods. The EMC method suggests that our theoretical estimation of an RIC corresponds to an upper bound that is tighter than in preceding studies. Physical consideration indicates that our assessment of the RIC could be improved by taking into account the replica symmetry breaking.',\n",
       "  'Title: pushdown abstractions of javascript\\nAbstract: We design a family of program analyses for JavaScript that make no approximation in matching calls with returns, exceptions with handlers, and breaks with labels. We do so by starting from an established reduction semantics for JavaScript and systematically deriving its intensional abstract interpretation. Our first step is to transform the semantics into an equivalent low-level abstract machine: the JavaScript Abstract Machine (JAM). We then give an infinite-state yet decidable pushdown machine whose stack precisely models the structure of the concrete program stack. The precise model of stack structure in turn confers precise control-flow analysis even in the presence of control effects, such as exceptions and finally blocks. We give pushdown generalizations of traditional forms of analysis such as k-CFA, and prove the pushdown framework for abstract interpretation is sound and computable.',\n",
       "  'Title: a notion of robustness for cyber physical systems\\nAbstract: Robustness as a system property describes the degree to which a system is able to function correctly in the presence of disturbances, i.e., unforeseen or erroneous inputs. In this paper, we introduce a notion of robustness termed  input-output dynamical stability  for cyber-physical systems (CPS) which merges existing notions of robustness for continuous systems and discrete systems. The notion captures two intuitive aims of robustness: bounded disturbances have bounded effects and the consequences of a sporadic disturbance disappear over time. We present a design methodology for robust CPS which is based on an abstraction and refinement process. We suggest several novel notions of simulation relations to ensure the soundness of the approach. In addition, we show how such simulation relations can be constructed compositionally. The different concepts and results are illustrated throughout the paper with examples.',\n",
       "  'Title: entropy rate for hidden markov chains with rare transitions\\nAbstract: We consider Hidden Markov Chains obtained by passing a Markov Chain with rare transitions through a noisy memoryless channel. We obtain asymptotic estimates for the entropy of the resulting Hidden Markov Chain as the transition rate is reduced to zero. Let (Xn) be a Markov chain with finite state space S and transition matrix P(p) and let (Yn) be the Hidden Markov chain observed by passing (Xn) through a homogeneous noisy memoryless channel (i.e. Y takes values in a set T, and there exists a matrix Q such that P(Yn = jjXn = i;X n−1 −1 ;X 1+1;Y n−1 −1 ;Y 1 n+1) = Qij). We make the additional assumption on the channel that the rows of Q are distinct. In this case we call the channel statistically distinguishing. We assume that P(p) is of the form I + pA where A is a matrix with negative entries on the diagonal, non-negative entries in the off-diagonal terms and zero row sums. We further assume that for small positive p, the Markov chain with transition matrix P(p) is irreducible. Notice that for Markov chains of this form, the invariant distribution (�i)i2 S does not depend on p. In this case, we say that for small positive values of p, the Markov chain is in a rare transition regime. We will adopt the convention that H is used to denote the entropy of a fi- nite partition, whereas h is used to denote the entropy of a process (the en- tropy rate in information theory terminology). Given an irreducible Markov chain with transition matrix P, we let h(P) be the entropy of the Markov chain (i.e. h(P) = − P i;jiPij logPij wherei is the (unique) invariant distribution of the Markov chain and as usual we adopt the convention that 0log0 = 0). We also let Hchan(i) be the entropy of the output of the channel when the input symbol is i (i.e. Hchan(i) = − P j2 T Qij logQij). Let h(Y ) denote the entropy of Y (i.e.',\n",
       "  'Title: memristors can implement fuzzy logic\\nAbstract: In our work we propose implementing fuzzy logic using memristors. Min and max operations are done by antipodally configured memristor circuits that may be assembled into computational circuits. We discuss computational power of such circuits with respect to m-efficiency and experimentally observed behavior of memristive devices. Circuits implemented with real devices are likely to manifest learning behavior. The circuits presented in the work may be applicable for instance in fuzzy classifiers.',\n",
       "  'Title: asymptotic capacity of wireless ad hoc networks with realistic links under a honey comb topology\\nAbstract: We consider the effects of Rayleigh fading and lognormal shadowing in the physical interference model for all the successful transmissions of traffic across the network. New bounds are derived for the capacity of a given random ad hoc wireless network that reflect packet drop or capture probability of the transmission links. These bounds are based on a simplified network topology termed as honey-comb topology under a given routing and scheduling scheme.',\n",
       "  'Title: on the performance of selection cooperation with imperfect channel estimation\\nAbstract: In this paper, we investigate the performance of selection cooperation in the presence of imperfect channel estimation. In particular, we consider a cooperative scenario with multiple relays and amplifyand-forward protocol over frequency flat fading channels. I n the selection scheme, only the “best” relay which maximizes the effective signal-to-noise ratio (SNR) at the receiver end is selected. We present lower and upper bounds on the effective SNR and derive closed-form expressions for the average symbol error rate (ASER), outage probability and average capacity per bandwidth of the received signal in the presence of channel estimation errors. A simulation study is presented to corroborate the analytical results and to demonstrate the performance of relay selection with imperfect channel estimation.',\n",
       "  'Title: informetric analyses of knowledge organization systems koss\\nAbstract: A knowledge organization system (KOS) is made up of concepts and semantic relations between the concepts which represent a knowledge domain terminologically. We distinguish between five approaches to KOSs: nomenclatures, classification systems, thesauri, ontologies and, as a borderline case of KOSs, folksonomies. The research question of this paper is: How can we informetrically analyze the effectiveness of KOSs? Quantitative informetric measures and indicators allow for the description, for comparative analyses as well as for evaluation of KOSs and their quality. We describe the state of the art of KOS evaluation. Most of the evaluation studies found in the literature are about ontologies. We introduce measures of the structure of KOSs (e.g., groundedness, tangledness, fan-out factor, or granularity) and indicators of KOS quality (completeness, consistency, overlap, and use).',\n",
       "  'Title: latent topic models for hypertext\\nAbstract: Latent topic models have been successfully applied as an unsupervised topic discovery technique in large document collections. With the proliferation of hypertext document collection such as the Internet, there has also been great interest in extending these approaches to hypertext [6, 9]. These approaches typically model links in an analogous fashion to how they model words - the document-link co-occurrence matrix is modeled in the same way that the document-word co-occurrence matrix is modeled in standard topic models. In this paper we present a probabilistic generative model for hypertext document collections that explicitly models the generation of links. Specifically, links from a word w to a document d depend directly on how frequent the topic of w is in d, in addition to the in-degree of d. We show how to perform EM learning on this model efficiently. By not modeling links as analogous to words, we end up using far fewer free parameters and obtain better link prediction results.',\n",
       "  'Title: complete security framework for wireless sensor networks\\nAbstract: Security concern for a Sensor Networks and level of security desired may differ according to application specific needs where the sensor networks are deployed. Till now, most of the security solutions proposed for sensor networks are layer wise i.e a particular solution is applicable to single layer itself. So, to integrate them all is a new research challenge. In this paper we took up the challenge and have proposed an integrated comprehensive security framework that will provide security services for all services of sensor network. We have added one extra component i.e. Intelligent Security Agent (ISA) to assess level of security and cross layer interactions. This framework has many components like Intrusion Detection System, Trust Framework, Key Management scheme and Link layer communication protocol. We have also tested it on three different application scenarios in Castalia and Omnet++ simulator.',\n",
       "  'Title: neural dissimilarity indices that predict oddball detection in behaviour\\nAbstract: Neuroscientists have recently shown that images that are difficult to find in visual search elicit similar patterns of firing across a population of recorded neurons. The $L^{1}$ distance between firing rate vectors associated with two images was strongly correlated with the inverse of decision time in behaviour. But why should decision times be correlated with $L^{1}$ distance? What is the decision-theoretic basis? In our decision theoretic formulation, we modeled visual search as an active sequential hypothesis testing problem with switching costs. Our analysis suggests an appropriate neuronal dissimilarity index which correlates equally strongly with the inverse of decision time as the $L^{1}$ distance. We also consider a number of other possibilities such as the relative entropy (Kullback-Leibler divergence) and the Chernoff entropy of the firing rate distributions. A more stringent test of equality of means, which would have provided a strong backing for our modeling fails for our proposed as well as the other already discussed dissimilarity indices. However, test statistics from the equality of means test, when used to rank the indices in terms of their ability to explain the observed results, places our proposed dissimilarity index at the top followed by relative entropy, Chernoff entropy and the $L^{1}$ indices. Computations of the different indices requires an estimate of the relative entropy between two Poisson point processes. An estimator is developed and is shown to have near unbiased performance for almost all operating regions.',\n",
       "  'Title: network maps of technology fields a comparative analysis of relatedness measures\\nAbstract: Network maps of technology fields extracted from patent databases are useful to aid in technology forecasting and road mapping. Constructing such a network requires a measure of the relatedness between pairs of technology fields. Despite the existence of various relatedness measures in the literature, it is unclear how to consistently assess and compare them, and which ones to select for constructing technology network maps. This ambiguity has limited the use of technology network maps for technology forecasting and roadmap analyses. To address this challenge, here we propose a strategy to evaluate alternative relatedness measures and identify the superior ones by comparing the structure properties of resulting technology networks. Using United States patent data, we execute the strategy through a comparative analysis of twelve relatedness measures, which quantify inter-field knowledge input similarity, field-crossing diversification likelihood or frequency of innovation agents, and co-occurrences of technology classes in the same patents. Our comparative analyses suggest two superior relatedness measures, normalized co-reference and inventor diversification likelihood, for constructing technology network maps.',\n",
       "  'Title: continuous double auction mechanism and bidding strategies in cloud computing markets\\nAbstract: Cloud computing has been an emerging model which aims at allowing customers to utilize computing resources hosted by Cloud Service Providers (CSPs). More and more consumers rely on CSPs to supply computing and storage service on the one hand, and CSPs try to attract consumers on favorable terms on the other. In such competitive cloud computing markets, pricing policies are critical to market efficiency. While CSPs often publish their prices and charge users according to the amount of resources they consume, auction mechanism is rarely applied. In fact a feasible auction mechanism is the most effective method for allocation of resources, especially double auction is more efficient and flexible for it enables buyers and sellers to enter bids and offers simultaneously. In this paper we bring up an electronic auction platform for cloud, and a cloud Continuous Double Auction (CDA) mechanism is formulated to match orders and facilitate trading based on the platform. Some evaluating criteria are defined to analyze the efficiency of markets and strategies. Furthermore, the selection of bidding strategies for the auction plays a very important role for each player to maximize its own profit, so we developed a novel bidding strategy for cloud CDA, BH-strategy, which is a two-stage game bidding strategy. At last we designed three simulation scenarios to compare the performance of our strategy with other dominating bidding strategies and proved that BH-strategy has better performance on surpluses, successful transactions and market efficiency. In addition, we discussed that our cloud CDA mechanism is feasible for cloud computing resource allocation.',\n",
       "  'Title: the abc problem for gabor systems\\nAbstract: A Gabor system generated by a window function $\\\\phi$ and a rectangular lattice $a \\\\Z\\\\times \\\\Z/b$ is given by $${\\\\mathcal G}(\\\\phi, a \\\\Z\\\\times \\\\Z/b):=\\\\{e^{-2\\\\pi i n t/b} \\\\phi(t- m a):\\\\ (m, n)\\\\in \\\\Z\\\\times \\\\Z\\\\}.$$ One of fundamental problems in Gabor analysis is to identify window functions $\\\\phi$ and time-frequency shift lattices $a \\\\Z\\\\times \\\\Z/b$ such that the corresponding Gabor system ${\\\\mathcal G}(\\\\phi, a \\\\Z\\\\times \\\\Z/b)$ is a Gabor frame for $L^2(\\\\R)$, the space of all square-integrable functions on the real line $\\\\R$. In this paper, we provide a full classification of triples $(a,b,c)$ for which the Gabor system ${\\\\mathcal G}(\\\\chi_I, a \\\\Z\\\\times \\\\Z/b)$ generated by the ideal window function $\\\\chi_I$ on an interval $I$ of length $c$ is a Gabor frame for $L^2(\\\\R)$. For the classification of such triples $(a, b, c)$ (i.e., the $abc$-problem for Gabor systems), we introduce maximal invariant sets of some piecewise linear transformations and establish the equivalence between Gabor frame property and triviality of maximal invariant sets. We then study dynamic system associated with the piecewise linear transformations and explore various properties of their maximal invariant sets. By performing holes-removal surgery for maximal invariant sets to shrink and augmentation operation for a line with marks to expand, we finally parameterize those triples $(a, b, c)$ for which maximal invariant sets are trivial. The novel techniques involving non-ergodicity of dynamical systems associated with some novel non-contractive and non-measure-preserving transformations lead to our arduous answer to the $abc$-problem for Gabor systems.',\n",
       "  'Title: inference less density estimation using copula bayesian networks\\nAbstract: We consider learning continuous probabilistic graphical models in the face of missing data. For non-Gaussian models, learning the parameters and structure of such models depends on our ability to perform efficient inference, and can be prohibitive even for relatively modest domains. Recently, we introduced the Copula Bayesian Network (CBN) density model - a flexible framework that captures complex high-dimensional dependency structures while offering direct control over the univariate marginals, leading to improved generalization. In this work we show that the CBN model also offers significant computational advantages when training data is partially observed. Concretely, we leverage on the specialized form of the model to derive a computationally amenable learning objective that is a lower bound on the log-likelihood function. Importantly, our energy-like bound circumvents the need for costly inference of an auxiliary distribution, thus facilitating practical learning of highdimensional densities. We demonstrate the effectiveness of our approach for learning the structure and parameters of a CBN model for two reallife continuous domains.',\n",
       "  'Title: optimal detection of intersections between convex polyhedra\\nAbstract: For a polyhedron $P$ in $\\\\mathbb{R}^d$, denote by $|P|$ its combinatorial complexity, i.e., the number of faces of all dimensions of the polyhedra. In this paper, we revisit the classic problem of preprocessing polyhedra independently so that given two preprocessed polyhedra $P$ and $Q$ in $\\\\mathbb{R}^d$, each translated and rotated, their intersection can be tested rapidly. #R##N#For $d=3$ we show how to perform such a test in $O(\\\\log |P| + \\\\log |Q|)$ time after linear preprocessing time and space. This running time is the best possible and improves upon the last best known query time of $O(\\\\log|P| \\\\log|Q|)$ by Dobkin and Kirkpatrick (1990). #R##N#We then generalize our method to any constant dimension $d$, achieving the same optimal $O(\\\\log |P| + \\\\log |Q|)$ query time using a representation of size $O(|P|^{\\\\lfloor d/2\\\\rfloor + \\\\varepsilon})$ for any $\\\\varepsilon>0$ arbitrarily small. This answers an even older question posed by Dobkin and Kirkpatrick 30 years ago. #R##N#In addition, we provide an alternative $O(\\\\log |P| + \\\\log |Q|)$ algorithm to test the intersection of two convex polygons $P$ and $Q$ in the plane.',\n",
       "  \"Title: limits of rush hour logic complexity\\nAbstract: Rush Hour Logic was introduced in [Flake&Baum99] as a model of computation inspired by the ``Rush Hour'' toy puzzle, in which cars can move horizontally or vertically within a parking lot. The authors show how the model supports polynomial space computation, using certain car configurations as building blocks to construct boolean circuits for a cpu and memory. They consider the use of cars of length 3 crucial to their construction, and conjecture that cars of size 2 only, which we'll call `Size 2 Rush Hour', do not support polynomial space computation. We settle this conjecture by showing that the required building blocks are constructible in Size 2 Rush Hour. Furthermore, we consider Unit Rush Hour, which was hitherto believed to be trivial, show its relation to maze puzzles, and provide empirical support for its hardness.\",\n",
       "  'Title: new separation between s f and bs f\\nAbstract: In this note we give a new separation between sensitivity and block sensitivity of Boolean functions: $bs(f)=(2/3)s(f)^2-(1/3)s(f)$.',\n",
       "  'Title: a survey on handover management in mobility architectures\\nAbstract: This work presents a comprehensive and structured taxonomy of available techniques for managing the handover process in mobility architectures. Representative works from the existing literature have been divided into appropriate categories, based on their ability to support horizontal handovers, vertical handovers and multihoming. We describe approaches designed to work on the current Internet (i.e. IPv4-based networks), as well as those that have been devised for the \"future\" Internet (e.g. IPv6-based networks and extensions). Quantitative measures and qualitative indicators are also presented and used to evaluate and compare the examined approaches. This critical review provides some valuable guidelines and suggestions for designing and developing mobility architectures, including some practical expedients (e.g. those required in the current Internet environment), aimed to cope with the presence of NAT/firewalls and to provide support to legacy systems and several communication protocols working at the application layer.',\n",
       "  'Title: many task computing and blue waters\\nAbstract: This report discusses many-task computing (MTC) generically and in the context of the proposed Blue Waters systems, which is planned to be the largest NSF-funded supercomputer when it begins production use in 2012. The aim of this report is to inform the BW project about MTC, including understanding aspects of MTC applications that can be used to characterize the domain and understanding the implications of these aspects to middleware and policies. Many MTC applications do not neatly fit the stereotypes of high-performance computing (HPC) or high-throughput computing (HTC) applications. Like HTC applications, by definition MTC applications are structured as graphs of discrete tasks, with explicit input and output dependencies forming the graph edges. However, MTC applications have significant features that distinguish them from typical HTC applications. In particular, different engineering constraints for hardware and software must be met in order to support these applications. HTC applications have traditionally run on platforms such as grids and clusters, through either workflow systems or parallel programming systems. MTC applications, in contrast, will often demand a short time to solution, may be communication intensive or data intensive, and may comprise very short tasks. Therefore, hardware and software for MTC must be engineered to support the additional communication and I/O and must minimize task dispatch overheads. The hardware of large-scale HPC systems, with its high degree of parallelism and support for intensive communication, is well suited for MTC applications. However, HPC systems often lack a dynamic resource-provisioning feature, are not ideal for task communication via the file system, and have an I/O system that is not optimized for MTC-style applications. Hence, additional software support is likely to be required to gain full benefit from the HPC hardware.',\n",
       "  'Title: identifying reliable annotations for large scale image segmentation\\nAbstract: Challenging computer vision tasks, in particular semantic image segmentation, require large training sets of annotated images. While obtaining the actual images is often unproblematic, creating the necessary annotation is a tedious and costly process. Therefore, one often has to work with unreliable annotation sources, such as Amazon Mechanical Turk or (semi-)automatic algorithmic techniques. In this work, we present a Gaussian process (GP) based technique for simultaneously identifying which images of a training set have unreliable annotation and learning a segmentation model in which the negative effect of these images is suppressed. Alternatively, the model can also just be used to identify the most reliably annotated images from the training set, which can then be used for training any other segmentation method. By relying on \"deep features\" in combination with a linear covariance function, our GP can be learned and its hyperparameter determined efficiently using only matrix operations and gradient-based optimization. This makes our method scalable even to large datasets with several million training instances.',\n",
       "  'Title: improved analysis for subspace pursuit algorithm in terms of restricted isometry constant\\nAbstract: In the context of compressed sensing (CS), both Subspace Pursuit (SP) and Compressive Sampling Matching Pursuit (CoSaMP) are very important iterative greedy recovery algorithms which could reduce the recovery complexity greatly comparing with the well-known $\\\\ell_1$-minimization. Restricted isometry property (RIP) and restricted isometry constant (RIC) of measurement matrices which ensure the convergency of iterative algorithms play key roles for the guarantee of successful reconstructions. In this paper, we show that for the $s$-sparse recovery, the RICs are enlarged to $\\\\delta_{3s}<0.4859$ for SP and $\\\\delta_{4s}<0.5$ for CoSaMP, which improve the known results significantly. The proposed results also apply to almost sparse signal and corrupted measurements.',\n",
       "  'Title: squares of 3 sun free split graphs\\nAbstract: AbstractThesquareofagraphG,denotedbyG 2 ,isobtainedfromGbyputtinganedge between twodistinct verticeswhenevertheirdistanceis two. ThenG is called a square root of G 2 . Deciding whether a given graph has asquarerootisknowntobeNP-complete,eveniftherootisrequiredtobeasplit graph,thatis,agraphinwhichthevertexsetcanbepartitionedintoastablesetandaclique.Wegiveawiderangeofpolynomialtimesolvablecasesfortheproblemofrecognizing ifagivengraph isthesquareofsomespecial kindofsplitgraph. To the best of our knowledge, our result properly contains allpreviously known such cases. Ourpolynomial time algorithms are buildonastructuralinvestigationofgraphsthatadmitasplitsquarerootthatis 3-sun-free, and may pave the way toward a dichotomy theorem forrecognizingsquaresof(3-sun-free)splitgraphs.Keywords: Squareofgraphs,squareofsplitgraphs.2010 MSC:05C75,05C85. 1 Introduction The k-th power of a graph G, written G k , is obtained from G by adding newedges between any two diﬀerent vertices at distance at most k in G. In casek= 2, G',\n",
       "  'Title: tree dynamics for peer to peer streaming\\nAbstract: This paper presents an asynchronous distributed algorithm to manage multiple trees for peer-to-peer streaming in a flow level model. It is assumed that videos are cut into substreams, with or without source coding, to be distributed to all nodes. The algorithm guarantees that each node receives sufficiently many substreams within delay logarithmic in the number of peers. The algorithm works by constantly updating the topology so that each substream is distributed through trees to as many nodes as possible without interference. Competition among trees for limited upload capacity is managed so that both coverage and balance are achieved. The algorithm is robust in that it efficiently eliminates cycles and maintains tree structures in a distributed way. The algorithm favors nodes with higher degree, so it not only works for live streaming and video on demand, but also in the case a few nodes with large degree act as servers and other nodes act as clients. #R##N#A proof of convergence of the algorithm is given assuming instantaneous update of depth information, and for the case of a single tree it is shown that the convergence time is stochastically tightly bounded by a small constant times the log of the number of nodes. These theoretical results are complemented by simulations showing that the algorithm works well even when most assumptions for the theoretical tractability do not hold.',\n",
       "  'Title: stochastic ordering of interferences in large scale wireless networks\\nAbstract: Stochastic orders are binary relations defined on probability distributions which capture intuitive notions like being larger or being more variable. This paper introduces stochastic ordering of interference distributions in large-scale networks modeled as point process. Interference is the main performance-limiting factor in most wireless networks, thus it is important to understand its statistics. Since closed-form results for the distribution of interference for such networks are only available in limited cases, interference of networks are compared using stochastic orders, even when closed form expressions for interferences are not tractable. We show that the interference from a large-scale network depends on the fading distributions with respect to the stochastic Laplace transform order. The condition for path-loss models is also established to have stochastic ordering between interferences. The stochastic ordering of interferences between different networks are also shown. Monte-Carlo simulations are used to supplement our analytical results.',\n",
       "  'Title: earthquake disaster based efficient resource utilization technique in iaas cloud\\nAbstract: Cloud Computing is an emerging area. The main aim of the initial search-and-rescue period after strong earthquakes is to reduce the whole number of mortalities. One main trouble rising in this period is to and the greatest assignment of available resources to functioning zones. For this issue a dynamic optimization model is presented. The model uses thorough descriptions of the operational zones and of the available resources to determine the resource performance and efficiency for different workloads related to the response. A suitable solution method for the model is offered as well. In this paper, Earthquake Disaster Based Resource Scheduling (EDBRS) Framework has been proposed. The allocation of resources to cloud workloads based on urgency (emergency during Earthquake Disaster). Based on this criterion, the resource scheduling algorithm has been proposed. The performance of the proposed algorithm has been assessed with the existing common scheduling algorithms through the CloudSim. The experimental results show that the proposed algorithm outperforms the existing algorithms by reducing execution cost and time of cloud consumer workloads submitted to the cloud.',\n",
       "  'Title: on descriptional complexity of the planarity problem for gauss words\\nAbstract: In this paper we investigate the descriptional complexity of knot theoretic problems and show upper bounds for planarity problem of signed and unsigned knot diagrams represented by Gauss words. Since a topological equivalence of knots can involve knot diagrams with arbitrarily many crossings then Gauss words will be considered as strings over an infinite (unbounded) alphabet. For establishing the upper bounds on recognition of knot properties, we study these problems in a context of automata models over an infinite alphabet.',\n",
       "  'Title: multi access mimo systems with finite rate channel state feedback\\nAbstract: This paper characterizes the effect of finite rate channel state feedback on the sum rate of a multi-access multiple-input multiple-output (MIMO) system. We propose to control the users jointly, specifically, we first choose the users jointly and then select the corresponding beamforming vectors jointly. To quantify the sum rate, this paper introduces the composite Grassmann manifold and the composite Grassmann matrix. By characterizing the distortion rate function on the composite Grassmann manifold and calculating the logdet function of a random composite Grassmann matrix, a good sum rate approximation is derived. According to the distortion rate function on the composite Grassmann manifold, the loss due to finite beamforming decreases exponentially as the feedback bits on beamforming increases.',\n",
       "  'Title: an information theoretic perspective of the poisson approximation via the chen stein method\\nAbstract: The first part of this work considers the entropy of the sum of ( possibly dependent and non-identically distributed) Bernoulli random variables. Upper bounds on the error that follows from an approximation of this entropy by the entropy of a Poisson random variable with the same mean are derived via the Chen-Stein method. The second part of this work derives new lower bounds on the total variation distance and relative entropy between the distribution of the sum of independent Bernoulli random variables and the Poisson distribution. The starting point of the derivation of the new bounds in the second part of this work is an introduction of a new lower bound on the total variation distance, whose derivation generalizes and refines the anal ysis by Barbour and Hall (1984), based on the Chen-Stein method for the Poisson approximation. A new lower bound on the relative entropy between these two distributions is introd uced, and this lower bound is compared to a previously reported upper bound on the relative entropy by Kontoyiannis et al. (2005). The derivation of the new lower bound on the relative entropy follows from the new lower bound on the total variation distance, combined with a distribution-dependent refine ment of Pinsker’s inequality by Ordentlich and Weinberger (2005). Upper and lower bounds on the Bhattacharyya parameter, Chernoff information and Hellinger distance between the distribution of the sum of independent Bernoulli random variables and the Poisson distribution with the same mean are derived as well via some relations between these quantities with the total variation distance and the relative entropy. The analysis in this work combines elements of information theory with the Chen-Stein method for the Poisson approximation. The resulting bounds are easy to compute, and their applicability is exemplified.',\n",
       "  'Title: a set and collection lemma\\nAbstract: A set S is independent if no two vertices from S are adjacent. In this paper we prove that if F is a collection of maximum independent sets of a graph, then there is a matching from S-{intersection of all members of F} into {union of all members of F}-S, for every independent set S. Based on this finding we give alternative proofs for a number of well-known lemmata, as the \"Maximum Stable Set Lemma\" due to Claude Berge and the \"Clique Collection Lemma\" due to Andr\\\\\\'as Hajnal.',\n",
       "  'Title: learning economic parameters from revealed preferences\\nAbstract: A recent line of work, starting with Beigman and Vohra (2006) and Zadimoghaddam and Roth (2012), has addressed the problem of {\\\\em learning} a utility function from revealed preference data. The goal here is to make use of past data describing the purchases of a utility maximizing agent when faced with certain prices and budget constraints in order to produce a hypothesis function that can accurately forecast the {\\\\em future} behavior of the agent. #R##N#In this work we advance this line of work by providing sample complexity guarantees and efficient algorithms for a number of important classes. By drawing a connection to recent advances in multi-class learning, we provide a computationally efficient algorithm with tight sample complexity guarantees ($\\\\Theta(d/\\\\epsilon)$ for the case of $d$ goods) for learning linear utility functions under a linear price model. This solves an open question in Zadimoghaddam and Roth (2012). Our technique yields numerous generalizations including the ability to learn other well-studied classes of utility functions, to deal with a misspecified model, and with non-linear prices.',\n",
       "  'Title: towards adapting imagenet to reality scalable domain adaptation with implicit low rank transformations\\nAbstract: Images seen during test time are often not from the same distribution as images used for learning. This problem, known as domain shift, occurs when training classifiers from object-centric internet image databases and trying to apply them directly to scene understanding tasks. The consequence is often severe performance degradation and is one of the major barriers for the application of classifiers in real-world systems. In this paper, we show how to learn transform-based domain adaptation classifiers in a scalable manner. The key idea is to exploit an implicit rank constraint, originated from a max-margin domain adaptation formulation, to make optimization tractable. Experiments show that the transformation between domains can be very efficiently learned from data and easily applied to new categories. This begins to bridge the gap between large-scale internet image collections and object images captured in everyday life environments.',\n",
       "  'Title: using multiple criteria methods to evaluate community partitions\\nAbstract: Community detection is one of the most studied problems on complex networks. Although hundreds of methods have been proposed so far, there is still no universally accepted formal definition of what is a good community. As a consequence, the problem of the evaluation and the comparison of the quality of the solutions produced by these algorithms is still an open question, despite constant progress on the topic. In this article, we investigate how using a multi-criteria evaluation can solve some of the existing problems of community evaluation, in particular the question of multiple equally-relevant solutions of different granularity. After exploring several approaches, we introduce a new quality function, called MDensity, and propose a method that can be related both to a widely used community detection metric, the Modularity, and to the Precision/Recall approach, ubiquitous in information retrieval.',\n",
       "  'Title: incremental adaptation strategies for neural network language models\\nAbstract: It is today acknowledged that neural network language models outperform backoff language models in applications like speech recognition or statistical machine translation. However, training these models on large amounts of data can take several days. We present efficient techniques to adapt a neural network language model to new data. Instead of training a completely new model or relying on mixture approaches, we propose two new methods: continued training on resampled data or insertion of adaptation layers. We present experimental results in an CAT environment where the post-edits of professional translators are used to improve an SMT system. Both methods are very fast and achieve significant improvements without overfitting the small adaptation data.',\n",
       "  'Title: optimal point to point codes in interference channels an incremental i mmse approach\\nAbstract: A recent result of the authors shows a so-called I-MMSE-like relationship that, for the two-user Gaussian interference channel, an I-MMSE relationship holds in the limit, as n $\\\\to \\\\infty$, between the interference and the interfered-with receiver, assuming that the interfered-with transmission is an optimal point-to-point sequence (achieves the point-to-point capacity). This result was further used to provide a proof of the \"missing corner points\" of the two-user Gaussian interference channel. This paper provides an information theoretic proof of the above-mentioned I-MMSE-like relationship which follows the incremental channel approach, an approach which was used by Guo, Shamai and Verd\\\\\\'u to provide an insightful proof of the original I-MMSE relationship for point-to-point channels. Finally, some additional applications of this result are shown for other multi-user settings: the Gaussian multiple-access channel with interference and specific K-user Gaussian Z-interference channel settings.',\n",
       "  'Title: a combined approach for constraints over finite domains and arrays\\nAbstract: Arrays are ubiquitous in the context of software verication. However, eective reasoning over arrays is still rare in CP, as local reasoning is dramatically ill-conditioned for constraints over arrays. In this paper, we propose an approach com- bining both global symbolic reasoning and local consistency ltering in order to solve constraint systems involving arrays (with accesses, updates and size constraints) and nite-domain constraints over their elements and indexes. Our approach, named fdcc, is based on a combination of a congruence closure algorithm for the standard theory of arrays and a CP solver over nite domains. The tricky part of the work lies in the bi- directional communication mechanism between both solvers. We identify the signicant information to share, and design ways to master the communication overhead. Exper- iments on random instances show that fdcc solves more formulas than any portfolio combination of the two solvers taken in isolation, while overhead is kept reasonable.',\n",
       "  'Title: optimization design and analysis of systematic lt codes over awgn channel\\nAbstract: In this paper, we study systematic Luby Transform (SLT) codes over additive white Gaussian noise (AWGN) channel. We introduce the encoding scheme of SLT codes and give the bipartite graph for iterative belief propagation (BP) decoding algorithm. Similar to low-density parity-check codes, Gaussian approximation (GA) is applied to yield asymptotic performance of SLT codes. Recent work about SLT codes has been focused on providing better encoding and decoding algorithms and design of degree distributions. In our work, we propose a novel linear programming method to optimize the degree distribution. Simulation results show that the proposed distributions can provide better bit-error-ratio (BER) performance. Moreover, we analyze the lower bound of SLT codes and offer closed form expressions.',\n",
       "  'Title: truth and envy in capacitated allocation games\\nAbstract: We study auctions with additive valuations where agents have a limit on the number of items they may receive. We refer to this setting as capacitated allocation games. We seek truthful and envy free mechanisms that maximize the social welfare. I.e., where agents have no incentive to lie and no agent seeks to exchange outcomes with another. In 1983, Leonard showed that VCG with Clarke Pivot payments (which is known to be truthful, individually rational, and have no positive transfers), is also an envy free mechanism for the special case of n items and n unit capacity agents. We elaborate upon this problem and show that VCG with Clarke Pivot payments is envy free if agent capacities are all equal. When agent capacities are not identical, we show that there is no truthful and envy free mechanism that maximizes social welfare if one disallows positive transfers. For the case of two agents (and arbitrary capacities) we show a VCG mechanism that is truthful, envy free, and individually rational, but has positive transfers. We conclude with a host of open problems that arise from our work.',\n",
       "  \"Title: smt based bounded model checking of fixed point digital controllers\\nAbstract: Digital controllers have several advantages with respect to their flexibility and design's simplicity. However, they are subject to problems that are not faced by analog controllers. In particular, these problems are related to the finite word-length implementation that might lead to overflows, limit cycles, and time constraints in fixed-point processors. This paper proposes a new method to detect design's errors in digital controllers using a state-of-the art bounded model checker based on satisfiability modulo theories. The experiments with digital controllers for a ball and beam plant demonstrate that the proposed method can be very effective in finding errors in digital controllers than other existing approaches based on traditional simulations tools.\",\n",
       "  'Title: google matrix of business process management\\nAbstract: Development of efficient business process models and determination of their characteristic properties are subject of intense interdisciplinary research. Here, we consider a business process model as a directed graph. Its nodes correspond to the units identified by the modeler and the link direction indicates the causal dependencies between units. It is of primary interest to obtain the stationary flow on such a directed graph, which corresponds to the steady-state of a firm during the business process. Following the ideas developed recently for the World Wide Web, we construct the Google matrix for our business process model and analyze its spectral properties. The importance of nodes is characterized by Page-Rank and recently proposed CheiRank and 2DRank, respectively. The results show that this two-dimensional ranking gives a significant information about the influence and communication properties of business model units. We argue that the Google matrix method, described here, provides a new efficient tool helping companies to make their decisions on how to evolve in the exceedingly dynamic global market.',\n",
       "  'Title: a combinatorial algebraic approach for the identifiability of low rank matrix completion\\nAbstract: In this paper, we review the problem of matrix completion and expose its intimate relations with algebraic geometry, combinatorics and graph theory. We present the first necessary and sufficient combinatorial conditions for matrices of arbitrary rank to be identifiable from a set of matrix entries, yielding theoretical constraints and new algorithms for the problem of matrix completion. We conclude by algorithmically evaluating the tightness of the given conditions and algorithms for practically relevant matrix sizes, showing that the algebraic-combinatoric approach can lead to improvements over state-of-the-art matrix completion methods.',\n",
       "  'Title: iterated ld problem in non associative key establishment\\nAbstract: We construct new non-associative key establishment protocols for all left self-distributive (LD), multi-LD-, and mutual LD-systems. The hard- ness of these protocols relies on variations of the (simultaneous) iterated LD- problem and its generalizations. We discuss instantiations of these protocols using generalized shifted conjugacy in braid groups and their quotients, LD- conjugacy and f-symmetric conjugacy in groups. We suggest parameter choices for instantiations in braid groups, symmetric groups and several matrix groups.',\n",
       "  'Title: towards ontological support for principle solutions in mechanical engineering\\nAbstract: The engineering design process follows a series of standardized stages of development, which have many aspects in common with software engineering. Among these stages, the principle solution can be regarded as an analogue of the design specification, fixing as it does the way the final product works. It is usually constructed as an abstract sketch (hand-drawn or constructed with a CAD system) where the functional parts of the product are identified, and geometric and topological constraints are formulated. Here, we outline a semantic approach where the principle solution is annotated with ontological assertions, thus making the intended requirements explicit and available for further machine processing; this includes the automated detection of design errors in the final CAD model, making additional use of a background ontology of engineering knowledge. We embed this approach into a document-oriented design workflow, in which the background ontology and semantic annotations in the documents are exploited to trace parts and requirements through the design process and across different applications.',\n",
       "  'Title: formation of robust multi agent networks through self organizing random regular graphs\\nAbstract: Multi-agent networks are often modeled as interaction graphs, where the nodes represent the agents and the edges denote some direct interactions. The robustness of a multi-agent network to perturbations such as failures, noise, or malicious attacks largely depends on the corresponding graph. In many applications, networks are desired to have well-connected interaction graphs with relatively small number of links. One family of such graphs is the random regular graphs. In this paper, we present a decentralized scheme for transforming any connected interaction graph with a possibly non-integer average degree of   $k$       into a connected random   $m$      -regular graph for some   $m\\\\in [k, k+2]$      . Accordingly, the agents improve the robustness of the network while maintaining a similar number of links as the initial configuration by locally adding or removing some edges.',\n",
       "  'Title: fully dynamic bin packing revisited\\nAbstract: We consider the fully dynamic bin packing problem, where items arrive and depart in an online fashion and repacking of previously packed items is allowed. The goal is, of course, to minimize both the number of bins used as well as the amount of repacking. A recently introduced way of measuring the repacking costs at each timestep is the migration factor, defined as the total size of repacked items divided by the size of an arriving or departing item. Concerning the trade-off between number of bins and migration factor, if we wish to achieve an asymptotic competitive ration of $1 + \\\\epsilon$ for the number of bins, a relatively simple argument proves a lower bound of $\\\\Omega(\\\\frac{1}{\\\\epsilon})$ for the migration factor. We establish a nearly matching upper bound of $O(\\\\frac{1}{\\\\epsilon}^4 \\\\log \\\\frac{1}{\\\\epsilon})$ using a new dynamic rounding technique and new ideas to handle small items in a dynamic setting such that no amortization is needed. The running time of our algorithm is polynomial in the number of items $n$ and in $\\\\frac{1}{\\\\epsilon}$. The previous best trade-off was for an asymptotic competitive ratio of $\\\\frac{5}{4}$ for the bins (rather than $1+\\\\epsilon$) and needed an amortized number of $O(\\\\log n)$ repackings (while in our scheme the number of repackings is independent of $n$ and non-amortized).',\n",
       "  'Title: throughput capacity of two hop relay manets under finite buffers\\nAbstract: Since the seminal work of Grossglauser and Tse [1], the two-hop relay algorithm and its variants have been attractive for mobile ad hoc networks (MANETs) due to their simplicity and efficiency. However, most literature assumed an infinite buffer size for each node, which is obviously not applicable to a realistic MANET. In this paper, we focus on the exact throughput capacity study of two-hop relay MANETs under the practical finite relay buffer scenario. The arrival process and departure process of the relay queue are fully characterized, and an ergodic Markov chain-based framework is also provided. With this framework, we obtain the limiting distribution of the relay queue and derive the throughput capacity under any relay buffer size. Extensive simulation results are provided to validate our theoretical framework and explore the relationship among the throughput capacity, the relay buffer size and the number of nodes.',\n",
       "  'Title: bounds on the capacity of ask molecular communication channels with isi\\nAbstract: There are now several works on the use of the additive inverse Gaussian noise (AIGN) model for the random transit time in molecular communication~(MC) channels. The randomness invariably causes inter-symbol interference (ISI) in MC, an issue largely ignored or simplified. In this paper we derive an upper bound and two lower bounds for MC based on amplitude shift keying (ASK) in presence of ISI. The Blahut-Arimoto algorithm~(BAA) is modified to find the input distribution of transmitted symbols to maximize the lower bounds. Our results show that over wide parameter values the bounds are close.',\n",
       "  'Title: ranking the importance level of intermediaries to a criminal using a reliance measure\\nAbstract: Recent research on finding important intermediate nodes in a network suspected to contain criminal activity is highly dependent on network centrality values. Betweenness centrality, for example, is widely used to rank the nodes that act as brokers in the shortest paths connecting all source and all the end nodes in a network. However both the shortest path node betweenness and the linearly scaled betweenness can only show rankings for all the nodes in a network. In this paper we explore the mathematical concept of pair-dependency on intermediate nodes, adapting the concept to criminal relationships and introducing a new source-intermediate reliance measure. To illustrate our measure, we apply it to rank the nodes in the Enron email dataset and the Noordin Top Terrorist networks. We compare the reliance ranking with Google PageRank, Markov centrality as well as betweenness centrality and show that a criminal investigation using the reliance measure, will lead to a different prioritisation in terms of possible people to investigate. While the ranking for the Noordin Top terrorist network nodes yields more extreme differences than for the Enron email transaction network, in the latter the reliance values for the set of finance managers immediately identified another employee convicted of money laundering.',\n",
       "  'Title: approximating the diameter of a graph\\nAbstract: In this paper we consider the fundamental problem of approximating the diameter D of directed or undirected graphs. In a seminal paper, Aingworth, Chekuri, Indyk and Motwani [SIAM J. Comput. 1999] presented an algorithm that computes in e O(m √ n + n 2 ) time an estimate ˆ D for the diameter of an n-node, m-edge graph, such that ⌊2/3D⌋ ≤ ˆ D ≤ D. In this paper we present an algorithm that produces the same estimate in e O(m √ n) expected running time. We then provide strong evidence that a better approximation may be hard to obtain if we insist on an O(m 2 \" ) running time. In particular, we show that if there is some constant \" > 0 so that there is an algorithm for undirected unweighted graphs that runs in O(m 2 \" ) time and produces an approximation ˆ D such that (2/3 + \")D ≤ ˆ D ≤ D, then SAT for CNF formulas onn variables can be solved in O � ((2 − �) n ) time for some constant � > 0, and the strong exponential time hypothesis of [Impagliazzo, Paturi, Zane JCSS’01] is false. Motivated by this somewhat negative result, we study whether it is possible to obtain a better approximation for specific cases. For unweighted directed or undir ected graphs, we show that if D = 3h + z, where h ≥ 0 and z ∈ {0,1,2}, then it is possible to report in ˜ O(min{m 2/3 n 4/3 ,m 2 1/(2h+3) }) time an estimate ˆ',\n",
       "  'Title: weight assignment logic\\nAbstract: We introduce a weight assignment logic for reasoning about quantitative languages of infinite words. This logic is an extension of the classical MSO logic and permits to describe quantitative properties of systems with multiple weight parameters, e.g., the ratio between rewards and costs. We show that this logic is expressively equivalent to unambiguous weighted Buchi automata. We also consider an extension of weight assignment logic which is expressively equivalent to nondeterministic weighted Buchi automata.',\n",
       "  'Title: fast label embeddings for extremely large output spaces\\nAbstract: Many modern multiclass and multilabel problems are characterized by increasingly large output spaces. For these problems, label embeddings have been shown to be a useful primitive that can improve computational and statistical efficiency. In this work we utilize a correspondence between rank constrained estimation and low dimensional label embeddings that uncovers a fast label embedding algorithm which works in both the multiclass and multilabel settings. The result is a randomized algorithm for partial least squares, whose running time is exponentially faster than naive algorithms. We demonstrate our techniques on two large-scale public datasets, from the Large Scale Hierarchical Text Challenge and the Open Directory Project, where we obtain state of the art results.',\n",
       "  'Title: flaglets exact wavelets on the ball\\nAbstract: We summarise the construction of exact axisymmetric scale-discretised wavelets on the sphere and on the ball. The wavelet transform on the ball relies on a novel 3D harmonic transform called the Fourier-Laguerre transform which combines the spherical harmonic transform with damped Laguerre polynomials on the radial half-line. The resulting wavelets, called flaglets, extract scale-dependent, spatially localised features in three-dimensions while treating the tangential and radial structures separately. Both the Fourier-Laguerre and the flaglet transforms are theoretically exact thanks to a novel sampling theorem on the ball. Our implementation of these methods is publicly available and achieves floating-point accuracy when applied to band-limited signals.',\n",
       "  'Title: a sparse bayesian framework for conditioning uncertain geologic models to nonlinear flow measurements\\nAbstract: We present a Bayesian framework for reconstructing hydraulic properties of rock formations from nonlinear dynamic flow data by imposing sparsity on the distribution of the parameters in a sparse transform basis through Laplace prior distribution. Sparse representation of the subsurface flow properties in a compression transform basis (where a compact representation is often possible) lends itself to a natural regularization approach, i.e. sparsity regularization, which has recently been exploited in solving ill-posed subsurface flow inverse problems. The Bayesian estimation approach presented here allows for a probabilistic treatment of the sparse reconstruction problem and has its roots in machine learning and the recently introduced relevance vector machine algorithm for linear inverse problems. We formulate the Bayesian sparse reconstruction algorithm and apply it to nonlinear subsurface inverse problems where solution sparsity in a discrete cosine transform is assumed. The probabilistic description of solution sparsity, as opposed to deterministic regularization, allows for quantification of the estimation uncertainty and avoids the need for specifying a regularization parameter. Several numerical experiments from multiphase subsurface flow application are presented to illustrate the performance of the proposed method and compare it with the regular Bayesian estimation approach that does not impose solution sparsity. While the examples are derived from subsurface flow modeling, the proposed framework can be applied to nonlinear inverse problems in other imaging applications including geophysical and medical imaging and electromagnetic inverse problem.',\n",
       "  'Title: paxoslease diskless paxos for leases\\nAbstract: This paper describes PaxosLease, a distributed algorithm for lease negotiation. PaxosLease is based on Paxos, but does not require disk writes or clock synchrony. PaxosLease is used for master lease negotation in the open-source Keyspace and ScalienDB replicated key-value stores.',\n",
       "  'Title: learning kernel based halfspaces with the zero one loss\\nAbstract: We describe and analyze a new algorithm for agnostically learning kernel-based halfspaces with respect to the \\\\emph{zero-one} loss function. Unlike most previous formulations which rely on surrogate convex loss functions (e.g. hinge-loss in SVM and log-loss in logistic regression), we provide finite time/sample guarantees with respect to the more natural zero-one loss function. The proposed algorithm can learn kernel-based halfspaces in worst-case time $\\\\poly(\\\\exp(L\\\\log(L/\\\\epsilon)))$, for $\\\\emph{any}$ distribution, where $L$ is a Lipschitz constant (which can be thought of as the reciprocal of the margin), and the learned classifier is worse than the optimal halfspace by at most $\\\\epsilon$. We also prove a hardness result, showing that under a certain cryptographic assumption, no algorithm can learn kernel-based halfspaces in time polynomial in $L$.',\n",
       "  'Title: on the optimality of simple schedules for networks with multiple half duplex relays\\nAbstract: This paper studies networks that consist of    $N$    half-duplex relays assisting the communication between a source and a destination. In ISIT’12 Brahma  et al.  conjectured that in Gaussian half-duplex diamond networks (i.e., without a direct link between the source and the destination, and with    $N$    non-interfering relays), an approximately optimal relay scheduling policy (i.e., achieving the cut-set upper bound to within a constant gap uniformly over all channel gains) has at most    $N+1$    active states (i.e., at most    $N+1$    out of the    $2^{N}$    possible relay listen-transmit configurations have a strictly positive probability). Such relay scheduling policies were referred to as simple. In ITW’13, we conjectured that simple approximately optimal relay scheduling policies exist for any Gaussian half-duplex multi-relay network irrespectively of the topology. This paper formally proves this more general version of the conjecture and shows it holds beyond Gaussian noise networks. In particular, for any class of memoryless half-duplex    $N$   -relay networks with independent noises and for which independent inputs are approximately optimal in the cut-set upper bound, an approximately optimal simple relay scheduling policy exists. The key step of the proof is to write the minimum of the submodular cut-set function by means of its Lovasz extension and use the greedy algorithm for submodular polyhedra to highlight structural properties of the optimal solution. This, together with the saddle-point property of min–max problems and the existence of optimal basic feasible solutions for linear programs, proves the conjecture. As an example, for    $N$   -relay Gaussian networks with independent noises, where each node is equipped with multiple antennas and where each antenna can be configured to listen or transmit irrespectively of the others, the existence of an approximately optimal simple relay scheduling policy with at most    $N+1$    active states, irrespectively of the total number of antennas in the system, is proved.',\n",
       "  \"Title: interaction and resistance the recognition of intentions in new human computer interaction\\nAbstract: Just as AI has moved away from classical AI, human-computer interaction (HCI) must move away from what I call 'good old fashioned HCI' to 'new HCI' - it must become a part of cognitive systems research where HCI is one case of the interaction of intelligent agents (we now know that interaction is essential for intelligent agents anyway). For such interaction, we cannot just 'analyze the data', but we must assume intentions in the other, and I suggest these are largely recognized through resistance to carrying out one's own intentions. This does not require fully cognitive agents but can start at a very basic level. New HCI integrates into cognitive systems research and designs intentional systems that provide resistance to the human agent.\",\n",
       "  'Title: optimizations for decision making and planning in description logic dynamic knowledge bases\\nAbstract: Artifact-centric models for business processes recently raised a lot of attention, as they manage to combine structural (i.e. data related) with dynamical (i.e. process related) aspects in a seamless way. Many frameworks developed under this approach, although, are not built explicitly for planning, one of the most prominent operations related to business processes. In this paper, we try to overcome this by proposing a framework named Dynamic Knowledge Bases, aimed at describing rich business domains through Description Logic-based ontologies, and where a set of actions allows the system to evolve by modifying such ontologies. This framework, by offering action rewriting and knowledge partialization, represents a viable and formal environment to develop decision making and planning techniques for DL-based artifact-centric business domains.',\n",
       "  \"Title: inference and evaluation of the multinomial mixture model for text clustering\\nAbstract: In this article, we investigate the use of a probabilistic model for unsupervised clustering in text collections. Unsupervised clustering has become a basic module for many intelligent text processing applications, such as information retrieval, text classification or information extraction. Recent proposals have been made of probabilistic clustering models, which build ''soft'' theme-document associations. These models allow to compute, for each document, a probability vector whose values can be interpreted as the strength of the association between documents and clusters. As such, these vectors can also serve to project texts into a lower-dimensional ''semantic'' space. These models however pose non-trivial estimation problems, which are aggravated by the very high dimensionality of the parameter space. The model considered in this paper consists of a mixture of multinomial distributions over the word counts, each component corresponding to a different theme. We propose a systematic evaluation framework to contrast various estimation procedures for this model. Starting with the expectation-maximization (EM) algorithm as the basic tool for inference, we discuss the importance of initialization and the influence of other features, such as the smoothing strategy or the size of the vocabulary, thereby illustrating the difficulties incurred by the high dimensionality of the parameter space. We empirically show that, in the case of text processing, these difficulties can be alleviated by introducing the vocabulary incrementally, due to the specific profile of the word count distributions. Using the fact that the model parameters can be analytically integrated out, we finally show that Gibbs sampling on the theme configurations is tractable and compares favorably to the basic EM approach.\",\n",
       "  'Title: synchronizing weighted automata\\nAbstract: We introduce two generalizations of synchronizability to automata with transitions weighted in an arbitrary semiring K=(K,+,*,0,1). (or equivalently, to finite sets of matrices in K^nxn.) Let us call a matrix A location-synchronizing if there exists a column in A consisting of nonzero entries such that all the other columns of A are filled by zeros. If additionally all the entries of this designated column are the same, we call A synchronizing. Note that these notions coincide for stochastic matrices and also in the Boolean semiring. A set M of matrices in K^nxn is called (location-)synchronizing if M generates a matrix subsemigroup containing a (location-)synchronizing matrix. The K-(location-)synchronizability problem is the following: given a finite set M of nxn matrices with entries in K, is it (location-)synchronizing? #R##N#Both problems are PSPACE-hard for any nontrivial semiring. We give sufficient conditions for the semiring K when the problems are PSPACE-complete and show several undecidability results as well, e.g. synchronizability is undecidable if 1 has infinite order in (K,+,0) or when the free semigroup on two generators can be embedded into (K,*,1).',\n",
       "  'Title: investigation of commuting hamiltonian in quantum markov network\\nAbstract: Graphical Models have various applications in science and engineering which include physics, bioinformatics, telecommunication and etc. Usage of graphical models needs complex computations in order to evaluation of marginal functions, so there are some powerful methods including mean field approximation, belief propagation algorithm and etc. Quantum graphical models have been recently developed in context of quantum information and computation, and quantum statistical physics, which is possible by generalization of classical probability theory to quantum theory. The main goal of this paper is preparing a primary generalization of Markov network, as a type of graphical models, to quantum case and applying in quantum statistical physics. We have investigated the Markov network and the role of commuting Hamiltonian terms in conditional independence with simple examples of quantum statistical physics.',\n",
       "  'Title: the oblivious transfer capacity of the wiretapped binary erasure channel\\nAbstract: We consider oblivious transfer between Alice and Bob in the presence of an eavesdropper Eve when there is a broadcast channel from Alice to Bob and Eve. In addition to the secrecy constraints of Alice and Bob, Eve should not learn the private data of Alice and Bob. When the broadcast channel consists of two independent binary erasure channels, we derive the oblivious transfer capacity for both 2-privacy (where the eavesdropper may collude with either party) and 1-privacy (where there are no collusions).',\n",
       "  'Title: combinatorial approximation algorithms for maxcut using random walks\\nAbstract: We give the first combinatorial approximation algorithm for Maxcut that beats the trivial 0.5 factor by a constant. The main partitioning procedure is very intuitive, natural, and easily described. It essentially performs a number of random walks and aggregates the information to provide the partition. We can control the running time to get an approximation factor-running time tradeoff. We show that for any constant b > 1.5, there is an O(n^{b}) algorithm that outputs a (0.5+delta)-approximation for Maxcut, where delta = delta(b) is some positive constant. #R##N#One of the components of our algorithm is a weak local graph partitioning procedure that may be of independent interest. Given a starting vertex $i$ and a conductance parameter phi, unless a random walk of length ell = O(log n) starting from i mixes rapidly (in terms of phi and ell), we can find a cut of conductance at most phi close to the vertex. The work done per vertex found in the cut is sublinear in n.',\n",
       "  'Title: the ergodic capacity of phase fading interference networks\\nAbstract: We identify the role of equal strength interference links as bottlenecks on the ergodic sum capacity of a K user phase-fading interference network, i.e., an interference network where the fading process is restricted primarily to independent and uniform phase variations while the channel magnitudes are held fixed across time. It is shown that even though there are K(K-1) cross-links, only about K/2 disjoint and equal strength interference links suffice to determine the capacity of the network regardless of the strengths of the rest of the cross channels. This scenario is called a minimal bottleneck state. It is shown that ergodic interference alignment is capacity optimal for a network in a minimal bottleneck state. The results are applied to large networks. It is shown that large networks are close to bottleneck states with a high probability, so that ergodic interference alignment is close to optimal for large networks. Limitations of the notion of bottleneck states are also highlighted for channels where both the phase and the magnitudes vary with time. It is shown through an example that for these channels, joint coding across different bottleneck states makes it possible to circumvent the capacity bottlenecks.',\n",
       "  'Title: informed rrt optimal sampling based path planning focused via direct sampling of an admissible ellipsoidal heuristic\\nAbstract: Rapidly-exploring random trees (RRTs) are pop- ular in motion planning because they find solutions efficiently to single-query problems. Optimal RRTs (RRT*s) extend RRTs to the problem of finding the optimal solution, but in doing so asymptotically find the optimal path from the initial state to every state in the planning domain. This behaviour is not only inefficient but also inconsistent with their single-query nature. For problems seeking to minimize path length, the subset of states that can improve a solution can be described by a prolate hyperspheroid. We show that unless this subset is sam- pled directly, the probability of improving a solution becomes arbitrarily small in large worlds or high state dimensions. In this paper, we present an exact method to focus the search by directly sampling this subset. The advantages of the presented sampling technique are demonstrated with a new algorithm, Informed RRT*. This method retains the same probabilistic guarantees on complete- ness and optimality as RRT* while improving the convergence rate and final solution quality. We present the algorithm as a simple modification to RRT* that could be further extended by more advanced path-planning algorithms. We show exper- imentally that it outperforms RRT* in rate of convergence, final solution cost, and ability to find difficult passages while demonstrating less dependence on the state dimension and range of the planning problem.',\n",
       "  'Title: greendcn a general framework for achieving energy efficiency in data center networks\\nAbstract: The popularization of cloud computing has raised concerns over the energy consumption that takes place in data centers. In addition to the energy consumed by servers, the energy consumed by large numbers of network devices emerges as a significant problem. Existing work on energy-efficient data center networking primarily focuses on traffic engineering, which is usually adapted from traditional networks. We propose a new framework to embrace the new opportunities brought by combining some special features of data centers with traffic engineering. Based on this framework, we characterize the problem of achieving energy efficiency with a time-aware model, and we prove its NP-hardness with a solution that has two steps. First, we solve the problem of assigning virtual machines (VM) to servers to reduce the amount of traffic and to generate favorable conditions for traffic engineering. The solution reached for this problem is based on three essential principles that we propose. Second, we reduce the number of active switches and balance traffic flows, depending on the relation between power consumption and routing, to achieve energy conservation. Experimental results confirm that, by using this framework, we can achieve up to 50 percent energy savings. We also provide a comprehensive discussion on the scalability and practicability of the framework.',\n",
       "  'Title: games with recurring certainty\\nAbstract: Infinite games where several players seek to coordinate under imperfect information are known to be intractable, unless the information flow is severely restricted. Examples of undecidable cases typically feature a situation where players become uncertain about the current state of the game, and this uncertainty lasts forever. Here we consider games where the players attain certainty about the current state over and over again along any play. For finite-state games, we note that this kind of recurring certainty implies a stronger condition of periodic certainty, that is, the events of state certainty ultimately occur at uniform, regular intervals. We show that it is decidable whether a given game presents recurring certainty, and that, if so, the problem of synthesising coordination strategies under w-regular winning conditions is solvable.',\n",
       "  'Title: new coherence and rip analysis for weak orthogonal matching pursuit\\nAbstract: In this paper we define a new coherence index, named the global 2-coherence, of a given dictionary and study its relationship with the traditional mutual coherence and the restricted isometry constant. By exploring this relationship, we obtain more general results on sparse signal reconstruction using greedy algorithms in the compressive sensing (CS) framework. In particular, we obtain an improved bound over the best known results on the restricted isometry constant for successful recovery of sparse signals using orthogonal matching pursuit (OMP).',\n",
       "  'Title: rappor randomized aggregatable privacy preserving ordinal response\\nAbstract: Randomized Aggregatable Privacy-Preserving Ordinal Response, or RAPPOR, is a technology for crowdsourcing statistics from end-user client software, anonymously, with strong privacy guarantees. In short, RAPPORs allow the forest of client data to be studied, without permitting the possibility of looking at individual trees. By applying randomized response in a novel manner, RAPPOR provides the mechanisms for such collection as well as for efficient, high-utility analysis of the collected data. In particular, RAPPOR permits statistics to be collected on the population of client-side strings with strong privacy guarantees for each client, and without linkability of their reports. This paper describes and motivates RAPPOR, details its differential-privacy and utility guarantees, discusses its practical deployment and properties in the face of different attack models, and, finally, gives results of its application to both synthetic and real-world data.',\n",
       "  'Title: increasing flash memory lifetime by dynamic voltage allocation for constant mutual information\\nAbstract: The read channel in Flash memory systems degrades over time because the Fowler-Nordheim tunneling used to apply charge to the floating gate eventually compromises the integrity of the cell because of tunnel oxide degradation. While degradation is commonly measured in the number of program/erase cycles experienced by a cell, the degradation is proportional to the number of electrons forced into the floating gate and later released by the erasing process. By managing the amount of charge written to the floating gate to maintain a constant read-channel mutual information, Flash lifetime can be extended. This paper proposes an overall system approach based on information theory to extend the lifetime of a flash memory device. Using the instantaneous storage capacity of a noisy flash memory channel, our approach allocates the read voltage of flash cell dynamically as it wears out gradually over time. A practical estimation of the instantaneous capacity is also proposed based on soft information via multiple reads of the memory cells.',\n",
       "  'Title: proof pad a new development environment for acl2\\nAbstract: Most software development projects rely on Integrated Development Environments (IDEs) based on the desktop paradigm, with an interactive, mouse-driven user interface. The standard installation of ACL2, on the other hand, is designed to work closely with Emacs. ACL2 experts, on the whole, like this mode of operation, but students and other new programmers who have learned to program with desktop IDEs often react negatively to the process of adapting to an unfamiliar form of interaction. #R##N#This paper discusses Proof Pad, a new IDE for ACL2. Proof Pad is not the only attempt to provide ACL2 IDEs catering to students and beginning programmers. The ACL2 Sedan and DrACuLa systems arose from similar motivations. Proof Pad builds on the work of those systems, while also taking into account the unique workflow of the ACL2 theorem proving system. #R##N#The design of Proof Pad incorporated user feedback from the outset, and that process continued through all stages of development. Feedback took the form of direct observation of users interacting with the IDE as well as questionnaires completed by users of Proof Pad and other ACL2 IDEs. The result is a streamlined interface and fast, responsive system that supports using ACL2 as a programming language and a theorem proving system. Proof Pad also provides a property-based testing environment with random data generation and automated interpretation of properties as ACL2 theorem definitions.',\n",
       "  \"Title: channels as objects in concurrent object oriented programming\\nAbstract: There is often a sort of a protocol associated to each class, stating when and how certain methods should be called. Given that this protocol is, if at all, described in the documentation accompanying the class, current mainstream object-oriented languages cannot provide for the verification of client code adherence against the sought class behaviour. We have defined a class-based concurrent object-oriented language that formalises such protocols in the form of usage types. Usage types are attached to class definitions, allowing for the specification of (1) the available methods, (2) the tests clients must perform on the result of methods, and (3) the object status - linear or shared - all of which depend on the object's state. Our work extends the recent approach on modular session types by eliminating channel operations, and defining the method call as the single communication primitive in both sequential and concurrent settings. In contrast to previous works, we define a single category for objects, instead of distinct categories for linear and for shared objects, and let linear objects evolve into shared ones. We introduce a standard sync qualifier to prevent thread interference in certain operations on shared objects. We formalise the language syntax, the operational semantics, and a type system that enforces by static typing that methods are called only when available, and by a single client if so specified in the usage type. We illustrate the language via a complete example.\",\n",
       "  'Title: approximate optimal cooperative decentralized control for consensus in a topological network of agents with uncertain nonlinear dynamics\\nAbstract: Efforts in this paper seek to combine graph theory with adaptive dynamic programming (ADP) as a reinforcement learning (RL) framework to determine forward-in-time, real-time, approximate optimal controllers for distributed multi-agent systems with uncertain nonlinear dynamics. A decentralized continuous time-varying control strategy is proposed, using only local communication feedback from two-hop neighbors on a communication topology that has a spanning tree. An actor-critic-identifier architecture is proposed that employs a nonlinear state derivative estimator to estimate the unknown dynamics online and uses the estimate thus obtained for value function approximation.',\n",
       "  'Title: the role of peer influence in churn in wireless networks\\nAbstract: Subscriber churn remains a top challenge for wireless carriers. These carriers need to understand the determinants of churn to confidently apply effective retention strategies to ensure their profitability and growth. In this paper, we look at the effect of peer influence on churn and we try to disentangle it from other effects that drive simultaneous churn across friends but that do not relate to peer influence. We analyze a random sample of roughly 10 thousand subscribers from large dataset from a major wireless carrier over a period of 10 months. We apply survival models and generalized propensity score to identify the role of peer influence. We show that the propensity to churn increases when friends do and that it increases more when many strong friends churn. Therefore, our results suggest that churn managers should consider strategies aimed at preventing group churn. We also show that survival models fail to disentangle homophily from peer influence over-estimating the effect of peer influence.',\n",
       "  'Title: off the grid spectral compressed sensing with prior information\\nAbstract: Recent research in off-the-grid compressed sensing (CS) has demonstrated that, under certain conditions, one can successfully recover a spectrally sparse signal from a few time-domain samples even though the dictionary is continuous. In this paper, we extend off-the-grid CS to applications where some prior information about spectrally sparse signal is known. We specifically consider cases where a few contributing frequencies or poles, but not their amplitudes or phases, are known a priori. Our results show that equipping off-the-grid CS with the known-poles algorithm can increase the probability of recovering all the frequency components.',\n",
       "  'Title: a regularized graph layout framework for dynamic network visualization\\nAbstract: Many real-world networks, including social and information networks, are dynamic structures that evolve over time. Such dynamic networks are typically visualized using a sequence of static graph layouts. In addition to providing a visual representation of the network structure at each time step, the sequence should preserve the mental map between layouts of consecutive time steps to allow a human to interpret the temporal evolution of the network. In this paper, we propose a framework for dynamic network visualization in the on-line setting where only present and past graph snapshots are available to create the present layout. The proposed framework creates regularized graph layouts by augmenting the cost function of a static graph layout algorithm with a grouping penalty, which discourages nodes from deviating too far from other nodes belonging to the same group, and a temporal penalty, which discourages large node movements between consecutive time steps. The penalties increase the stability of the layout sequence, thus preserving the mental map. We introduce two dynamic layout algorithms within the proposed framework, namely dynamic multidimensional scaling and dynamic graph Laplacian layout. We apply these algorithms on several data sets to illustrate the importance of both grouping and temporal regularization for producing interpretable visualizations of dynamic networks.',\n",
       "  'Title: area coverage under low sensor density\\nAbstract: This paper presents a solution to the problem of monitoring a region of interest (RoI) using a set of nodes that is not sufficient to achieve the required degree of monitoring coverage. In particular, sensing coverage of wireless sensor networks (WSNs) is a crucial issue in projects due to failure of sensors. The lack of sensor equipment resources hinders the traditional method of using mobile robots to move around the RoI to collect readings. Instead, our solution employs supervised neural networks to produce the values of the uncovered locations by extracting the non-linear relation among randomly deployed sensor nodes throughout the area. Moreover, we apply a hybrid backpropagation method to accelerate the learning convergence speed to a local minimum solution. We use a real-world data set from meteorological deployment for experimental validation and analysis.',\n",
       "  \"Title: fast resource scheduling in hetnets with d2d support\\nAbstract: Resource allocation in LTE networks is known to be an NP-hard problem. In this paper, we address an even more complex scenario: an LTE-based, 2-tier heterogeneous network where D2D mode is supported under the network control. All communications (macrocell, microcell and D2D-based) share the same frequency bands, hence they may interfere. We then determine (i) the network node that should serve each user and (ii) the radio resources to be scheduled for such communication. To this end, we develop an accurate model of the system and apply approximate dynamic programming to solve it. Our algorithms allow us to deal with realistic, large-scale scenarios. In such scenarios, we compare our approach to today's networks where eICIC techniques and proportional fairness scheduling are implemented. Results highlight that our solution increases the system throughput while greatly reducing energy consumption. We also show that D2D mode can effectively support content delivery without significantly harming macrocells or microcells traffic, leading to an increased system capacity. Interestingly, we find that D2D mode can be a low-cost alternative to microcells.\",\n",
       "  'Title: interference prediction in mobile ad hoc networks with a general mobility model\\nAbstract: In a mobile ad hoc network (MANET), effective prediction of time-varying interferences can enable adaptive transmission designs and therefore improve the communication performance. This paper investigates interference prediction in MANETs with a finite number of nodes by proposing and using a general-order linear model for node mobility. The proposed mobility model can well approximate node dynamics of practical MANETs. In contrast to previous studies on interference statistics, we are able through this model to give a best estimate of the time-varying interference at any time rather than long-term average effects. Specifically, we propose a compound Gaussian point process functional as a general framework to obtain analytical results on the mean value and moment-generating function of the interference prediction. With a series form of this functional, we give the necessary and sufficient condition for when the prediction is essentially equivalent to that from a binomial point process (BPP) network in the limit as time goes to infinity. These conditions permit one to rigorously determine when the commonly used BPP approximations are valid. Finally, our simulation results corroborate the effectiveness and accuracy of the analytical results on interference prediction and also show the advantages of our method in dealing with complex mobilities.',\n",
       "  'Title: how to transfer zero shot object recognition via hierarchical transfer of semantic attributes\\nAbstract: Attribute based knowledge transfer has proven very successful in visual object analysis and learning previously unseen classes. However, the common approach learns and transfers attributes without taking into consideration the embedded structure between the categories in the source set. Such information provides important cues on the intraattribute variations. We propose to capture these variations in a hierarchical model that expands the knowledge source with additional abstraction levels of attributes. We also provide a novel transfer approach that can choose the appropriate attributes to be shared with an unseen class. We evaluate our approach on three public datasets: a Pascal, Animals with Attributes and CUB-200-2011 Birds. The experiments demonstrate the effectiveness of our model with significant improvement over state-of-the-art.',\n",
       "  \"Title: iterative concave rank approximation for recovering low rank matrices\\nAbstract: In this paper, we propose a new algorithm for recovery of low-rank matrices from compressed linear measurements. The underlying idea of this algorithm is to closely approximate the rank function with a smooth function of singular values, and then minimize the resulting approximation subject to the linear constraints. The accuracy of the approximation is controlled via a scaling parameter δ, where a smaller δ corresponds to a more accurate fitting. The consequent optimization problem for any finite δ is nonconvex. Therefore, to decrease the risk of ending up in local minima, a series of optimizations is performed, starting with optimizing a rough approximation (a large δ) and followed by successively optimizing finer approximations of the rank with smaller δ's. To solve the optimization problem for any δ > 0, it is converted to a new program in which the cost is a function of two auxiliary positive semidefinite variables. The paper shows that this new program is concave and applies a majorize-minimize technique to solve it which, in turn, leads to a few convex optimization iterations. This optimization scheme is also equivalent to a reweighted Nuclear Norm Minimization (NNM). For any δ > 0, we derive a necessary and sufficient condition for the exact recovery which are weaker than those corresponding to NNM. On the numerical side, the proposed algorithm is compared to NNM and a reweighted NNM in solving affine rank minimization and matrix completion problems showing its considerable and consistent superiority in terms of success rate.\",\n",
       "  'Title: a general quantitative cryptanalysis of permutation only multimedia ciphers against plaintext attacks\\nAbstract: In recent years secret permutations have been widely used for protecting different types of multimedia data, including speech files, digital images and videos. Based on a general model of permutation-only multimedia ciphers, this paper performs a quantitative cryptanalysis on the performance of these kind of ciphers against plaintext attacks. When the plaintext is of size MxN and with L different levels of values, the following quantitative cryptanalytic findings have been concluded under the assumption of a uniform distribution of each element in the plaintext: (1) all permutation-only multimedia ciphers are practically insecure against known/chosen-plaintext attacks in the sense that only O(log\"L(MN)) known/chosen plaintexts are sufficient to recover not less than (in an average sense) half elements of the plaintext; (2) the computational complexity of the known/chosen-plaintext attack is only O(n.(MN)^2), where n is the number of known/chosen plaintexts used. When the plaintext has a non-uniform distribution, the number of required plaintexts and the computational complexity is also discussed. Experiments are given to demonstrate the real performance of the known-plaintext attack for a typical permutation-only image cipher.',\n",
       "  \"Title: an efficient assignment of drainage direction over flat surfaces in raster digital elevation models\\nAbstract: In processing raster digital elevation models (DEMs) it is often necessary to assign drainage directions over flats-that is, over regions with no local elevation gradient. This paper presents an approach to drainage direction assignment which is not restricted by a flat's shape, number of outlets, or surrounding topography. Flow is modeled by superimposing a gradient away from higher terrain with a gradient towards lower terrain resulting in a drainage field exhibiting flow convergence, an improvement over methods which produce regions of parallel flow. This approach builds on previous work by Garbrecht and Martz (1997), but presents several important improvements. The improved algorithm guarantees that flats are only resolved if they have outlets. The algorithm does not require iterative application; a single pass is sufficient to resolve all flats. The algorithm presents a clear strategy for identifying flats and their boundaries. The algorithm is not susceptible to loss of floating-point precision. Furthermore, the algorithm is efficient, operating in O(N) time whereas the older algorithm operates in O ( N 3 / 2 ) time. In testing, the improved algorithm ran 6.5 times faster than the old for a 100i?100 cell flat and 69 times faster for a 700i?700 cell flat. In tests on actual DEMs, the improved algorithm finished its processing 38-110 times sooner while running on a single processor than a parallel implementation of the old algorithm did while running on 16 processors. The improved algorithm is an optimal, accurate, easy-to-implement drop-in replacement for the original. Pseudocode is provided in the paper and working source code is provided in the Supplemental Materials. HighlightsWe present an improved algorithm to model ow directions in at regions of DEMs.The algorithm works regardless of the topography surrounding the at region.The algorithm produces convergent ows away from higher and towards lower terrain.The algorithm has a number of sanity checks which guarantee correct output.The algorithm works in O(N) time and supplants an older O(N3\\\\2) time algorithm.\",\n",
       "  'Title: whittlesearch interactive image search with relative attribute feedback\\nAbstract: We propose a novel mode of feedback for image search, where a user describes which properties of exemplar images should be adjusted in order to more closely match his/her mental model of the image sought. For example, perusing image results for a query \"black shoes\", the user might state, \"Show me shoe images like these, but sportier.\" Offline, our approach first learns a set of ranking functions, each of which predicts the relative strength of a nameable attribute in an image (e.g., sportiness). At query time, the system presents the user with a set of exemplar images, and the user relates them to his/her target image with comparative statements. Using a series of such constraints in the multi-dimensional attribute space, our method iteratively updates its relevance function and re-ranks the database of images. To determine which exemplar images receive feedback from the user, we present two variants of the approach: one where the feedback is user-initiated and another where the feedback is actively system-initiated. In either case, our approach allows a user to efficiently \"whittle away\" irrelevant portions of the visual feature space, using semantic language to precisely communicate her preferences to the system. We demonstrate our technique for refining image search for people, products, and scenes, and we show that it outperforms traditional binary relevance feedback in terms of search speed and accuracy. In addition, the ordinal nature of relative attributes helps make our active approach efficient--both computationally for the machine when selecting the reference images, and for the user by requiring less user interaction than conventional passive and active methods.',\n",
       "  'Title: optimized imaging using non rigid registration\\nAbstract: Abstract   The extraordinary improvements of modern imaging devices offer access to data with unprecedented information content. However, widely used image processing methodologies fall far short of exploiting the full breadth of information offered by numerous types of scanning probe, optical, and electron microscopies. In many applications, it is necessary to keep measurement intensities below a desired threshold. We propose a methodology for extracting an increased level of information by processing a series of data sets suffering, in particular, from high degree of spatial uncertainty caused by complex multiscale motion during the acquisition process. An important role is played by a non-rigid pixel-wise registration method that can cope with low signal-to-noise ratios. This is accompanied by formulating objective quality measures which replace human intervention and visual inspection in the processing chain. Scanning transmission electron microscopy of siliceous zeolite material exhibits the above-mentioned obstructions and therefore serves as orientation and a test of our procedures.',\n",
       "  'Title: conflict driven asp solving with external sources\\nAbstract: Answer Set Programming (ASP) is a well-known problem solving approach based on nonmonotonic logic programs and efficient solvers. To enable access to external information, hex-programs extend programs with external atoms, which allow for a bidirectional communication between the logic program and external sources of computation (e.g., description logic reasoners and Web resources). Current solvers evaluate hex-programs by a translation to ASP itself, in which values of external atoms are guessed and verified after the ordinary answer set computation. This elegant approach does not scale with the number of external accesses in general, in particular in presence of nondeterminism (which is instrumental for ASP). In this paper, we present a novel, native algorithm for evaluating hex-programs which uses learning techniques. In particular, we extend conflict-driven ASP solving techniques, which prevent the solver from running into the same conflict again, from ordinary to hex-programs. We show how to gain additional knowledge from external source evaluations and how to use it in a conflict-driven algorithm. We first target the uninformed case, i.e., when we have no extra information on external sources, and then extend our approach to the case where additional meta-information is available. Experiments show that learning from external sources can significantly decrease both the runtime and the number of considered candidate compatible sets.',\n",
       "  'Title: validity of altmetrics data for measuring societal impact a study using data from altmetric and f1000prime\\nAbstract: Can altmetric data be validly used for the measurement of societal impact? The current study seeks to answer this question with a comprehensive dataset (about 100,000 records) from very disparate sources (F1000, Altmetric, and an in-house database based on Web of Science). In the F1000 peer review system, experts attach particular tags to scientific papers which indicate whether a paper could be of interest for science or rather for other segments of society. The results show that papers with the tag \"good for teaching\" do achieve higher altmetric counts than papers without this tag - if the quality of the papers is controlled. At the same time, a higher citation count is shown especially by papers with a tag that is specifically scientifically oriented (\"new finding\"). The findings indicate that papers tailored for a readership outside the area of research should lead to societal impact. If altmetric data is to be used for the measurement of societal impact, the question arises of its normalization. In bibliometrics, citations are normalized for the papers\\' subject area and publication year. This study has taken a second analytic step involving a possible normalization of altmetric data. As the results show there are particular scientific topics which are of especial interest for a wide audience. Since these more or less interesting topics are not completely reflected in Thomson Reuters\\' journal sets, a normalization of altmetric data should not be based on the level of subject categories, but on the level of topics.',\n",
       "  'Title: matched filtering from limited frequency samples\\nAbstract: In this paper, we study a simple correlation-based strategy for estimating the unknown delay and amplitude of a signal based on a small number of noisy, randomly chosen frequency-domain samples. We model the output of this “compressive matched filter” as a random process whose mean equals the scaled, shifted autocorrelation function of the template signal. Using tools from the theory of empirical processes, we prove that the expected maximum deviation of this process from its mean decreases sharply as the number of measurements increases, and we also derive a probabilistic tail bound on the maximum deviation. Putting all of this together, we bound the minimum number of measurements required to guarantee that the empirical maximum of this random process occurs sufficiently close to the true peak of its mean function. We conclude that for broad classes of signals, this compressive matched filter will successfully estimate the unknown delay (with high probability and within a prescribed tolerance) using a number of random frequency-domain samples that scales inversely with the signal-to-noise ratio and only logarithmically in the observation bandwidth and the possible range of delays.',\n",
       "  'Title: linear index coding via graph homomorphism\\nAbstract: It is known that the minimum broadcast rate of a linear index code over $\\\\mathbb{F}_q$ is equal to the $minrank_q$ of the underlying digraph. In [3] it is proved that for $\\\\mathbb{F}_2$ and any positive integer $k$, $minrank_q(G)\\\\leq k$ iff there exists a homomorphism from the complement of the graph $G$ to the complement of a particular undirected graph family called \"graph family $\\\\{G_k\\\\}$\". As observed in [2], by combining these two results one can relate the linear index coding problem of undirected graphs to the graph homomorphism problem. In [4], a direct connection between linear index coding problem and graph homomorphism problem is introduced. In contrast to the former approach, the direct connection holds for digraphs as well and applies to any field size. More precisely, in [4], a graph family $\\\\{H_k^q\\\\}$ is introduced and shown that whether or not the scalar linear index of a digraph $G$ is less than or equal to $k$ is equivalent to the existence of a graph homomorphism from the complement of $G$ to the complement of $H_k^q$. #R##N#Here, we first study the structure of the digraphs $H_k^q$. Analogous to the result of [2] about undirected graphs, we prove that $H_k^q$\\'s are vertex transitive digraphs. Using this, and by applying a lemma of Hell and Nesetril [5], we derive a class of necessary conditions for digraphs $G$ to satisfy $lind_q(G)\\\\leq k$. Particularly, we obtain new lower bounds on $lind_q(G)$. #R##N#Our next result is about the computational complexity of scalar linear index of a digraph. It is known that deciding whether the scalar linear index of an undirected graph is equal to $k$ or not is NP-complete for $k\\\\ge 3$ and is polynomially decidable for $k=1,2$ [3]. For digraphs, it is shown in [6] that for the binary alphabet, the decision problem for $k=2$ is NP-complete. We use graph homomorphism framework to extend this result to arbitrary alphabet.',\n",
       "  'Title: leader contention based user matching for 802 11 multiuser mimo networks\\nAbstract: In multiuser MIMO (MU-MIMO) LANs, the achievable throughput of a client depends on who is transmitting concurrently with it. Existing MU-MIMO MAC protocols, however, enable clients to use the traditional 802.11 contention to contend for concurrent transmission opportunities on the uplink. Such a contention-based protocol not only wastes lots of channel time on multiple rounds of contention but also fails to maximally deliver the gain of MU-MIMO because users randomly join concurrent transmissions without considering their channel characteristics. To address such inefficiency, this paper introduces MIMOMate, a leader-contention-based MU-MIMO MAC protocol that matches clients as concurrent transmitters according to their channel characteristics to maximally deliver the MU-MIMO gain while ensuring all users fairly share concurrent transmission opportunities. Furthermore, MIMOMate elects the leader of the matched users to contend for transmission opportunities using traditional 802.11 CSMA/CA. It hence requires only a single contention overhead for concurrent streams and can be compatible with legacy 802.11 devices. A prototype implementation in USRP N200 shows that MIMOMate achieves an average throughput gain of 1.42× and $1.52× over the traditional contention-based protocol for two- and three-antenna AP scenarios, respectively, and also provides fairness for clients.',\n",
       "  'Title: new classes of quadratic bent functions in polynomial forms\\nAbstract: In this paper, we propose a new construction of quadratic bent functions in polynomial forms. Right Euclid algorithm in skew-polynomial rings over finite fields of characteristic 2 is applied in the proof.',\n",
       "  'Title: about the generalized lm inverse and the weighted moore penrose inverse\\nAbstract: The recursive method for computing the generalized LM-inverse of a constant rectangular matrix augmented by a column vector is proposed in Udwadia and Phohomsiri (2007) [16,17]. The corresponding algorithm for the sequential determination of the generalized LM-inverse is established in the present paper. We prove that the introduced algorithm for computing the generalized LM-inverse and the algorithm for the computation of the weighted Moore-Penrose inverse developed by Wang and Chen (1986) in [23] are equivalent algorithms. Both of the algorithms are implemented in the present paper using the package MATHEMATICA. Several rational test matrices and randomly generated constant matrices are tested and the CPU time is compared and discussed.',\n",
       "  'Title: the deterministic time linearity of service provided by fading channels\\nAbstract: In the paper, we study the service process S(t) of an independent and identically distributed (i.i.d.) Nakagami-m fading channel, which is defined as the amount of service provided, i.e., the integral of the instantaneous channel capacity over time t. By using the Moment Generation Function (MGF) approach and the infinitely divisible law, it is proved that, other than certain generally recognized curve form or a stochastic process, the channel service process S(t) is a deterministic linear function of time t, namely, S(t)=cm* · t where cm* is a constant determined by the fading parameter m. Furthermore, we extend it to general i.i.d. fading channels and present an explicit form of the constant service rate cp*. The obtained work provides such a new insight on the system design of joint source/channel coding that there exists a coding scheme such that a receiver can decode with zero error probability and zero high layer queuing delay, if the transmitter maintains a constant data rate no more than cp*. Finally, we verify our analysis through Monte Carlo simulations.',\n",
       "  'Title: promoting truthful behavior in participatory sensing mechanisms\\nAbstract: In this letter, the interplay between a class of nonlinear estimators and strategic sensors is studied in several participatory-sensing scenarios. It is shown that for the class of estimators, if the strategic sensors have access to noiseless measurements of the to-be-estimated-variable, truth-telling is an equilibrium of the game that models the interplay between the sensors and the estimator. Furthermore, performance of the proposed estimators is examined in the case that the strategic sensors form coalitions and in the presence of noise.',\n",
       "  'Title: relatives in the same university faculty nepotism or merit\\nAbstract: In many countries culture, practice or regulations inhibit the co-presence of relatives within the university faculty. We test the legitimacy of such attitudes and provisions, investigating the phenomenon of nepotism in Italy, a nation with high rates of favoritism. We compare the individual research performance of \"children\" who have \"parents\" in the same university against that of the \"non-children\" with the same academic rank and seniority, in the same field. The results show non-significant differences in performance. Analyses of career advancement show that children\\'s research performance is on average superior to that of their colleagues who did not advance. The study\\'s findings do not rule out the existence of nepotism, which has been actually recorded in a low percentage of cases, but do not prove either the most serious presumed consequences of nepotism, namely that relatives who are poor performers are getting ahead of non-relatives who are better performers. In light of these results, many attitudes and norms concerning parental ties in academia should be reconsidered.',\n",
       "  'Title: inertial parameter identification including friction and motor dynamics\\nAbstract: Identification of inertial parameters is fundamental for the implementation of torque-based control in humanoids. At the same time, good models of friction and actuator dynamics are critical for the low-level control of joint torques. We propose a novel method to identify inertial, friction and motor parameters in a single procedure. The identification exploits the measurements of the PWM of the DC motors and a 6-axis force/torque sensor mounted inside the kinematic chain. The partial least-square (PLS) method is used to perform the regression. We identified the inertial, friction and motor parameters of the right arm of the iCub humanoid robot. We verified that the identified model can accurately predict the force/torque sensor measurements and the motor voltages. Moreover, we compared the identified parameters against the CAD parameters, in the prediction of the force/torque sensor measurements. Finally, we showed that the estimated model can effectively detect external contacts, comparing it against a tactile-based contact detection. The presented approach offers some advantages with respect to other state-of-the-art methods, because of its completeness (i.e. it identifies inertial, friction and motor parameters) and simplicity (only one data collection, with no particular requirements).',\n",
       "  'Title: verifiable source code documentation in controlled natural language\\nAbstract: Writing documentation about software internals is rarely considered a rewarding activity. It is highly time-consuming and the resulting documentation is fragile when the software is continuously evolving in a multi-developer setting. Unfortunately, traditional programming environments poorly support the writing and maintenance of documentation. Consequences are severe as the lack of documentation on software structure negatively impacts the overall quality of the software product. We show that using a controlled natural language with a reasoner and a query engine is a viable technique for verifying the consistency and accuracy of documentation and source code. Using ACE, a state-of-the-art controlled natural language, we present positive results on the comprehensibility and the general feasibility of creating and verifying documentation. As a case study, we used automatic documentation verification to identify and fix severe flaws in the architecture of a non-trivial piece of software. Moreover, a user experiment shows that our language is faster and easier to learn and understand than other formal languages for software documentation.',\n",
       "  'Title: how to improve the outcome of performance evaluations in terms of percentiles for citation frequencies of my papers\\nAbstract: Using empirical data I demonstrate that the result of performance evaluations by percentiles can be drastically influenced by the proper choice of the journal in which a manuscript is published.',\n",
       "  'Title: a polyhedral approximation framework for convex and robust distributed optimization\\nAbstract: In this paper, we consider a general problem setup for a wide class of convex and robust distributed optimization problems in peer-to-peer networks. In this setup, convex constraint sets are distributed to the network processors who have to compute the optimizer of a linear cost function subject to the constraints. We propose a novel fully distributed and asynchronous algorithm, named cutting-plane consensus, to solve the problem, based on a polyhedral outer approximation of the constraint sets. Processors running the algorithm compute and exchange linear approximations of their locally feasible sets. Independently of the number of processors in the network, each processor stores only a small number of linear constraints, making the algorithm scalable to large networks. The cutting-plane consensus algorithm is presented and analyzed for the general framework. Specifically, we prove the correctness of the algorithm, and show its robustness against communication or processor failures. Then, the cutting-plane consensus algorithm is specified to three different classes of distributed optimization problems, namely 1) inequality constrained problems, 2) robust optimization problems, and 3) almost separable optimization problems. For each one of these problem classes we solve a concrete problem and present computational results. That is, we show how to solve: position estimation in wireless sensor networks, a distributed robust linear program, and a distributed microgrid control problem.',\n",
       "  'Title: cooperative estimation for synchronization of heterogeneous multi agent systems using relative information\\nAbstract: Abstract   In this paper, we present a distributed estimation setup where local agents estimate their states from relative measurements received from their neighbours. In the case of heterogeneous multi-agent systems, where only relative measurements are available, this is of high relevance. The objective is to improve the scalability of the existing distributed estimation algorithms by restricting the agents to estimating only their local states and those of immediate neighbours. The presented estimation algorithm also guarantees robust performance against model and measurement disturbances. It is shown that it can be integrated into output synchronization algorithms.',\n",
       "  'Title: a cooperative q learning approach for real time power allocation in femtocell networks\\nAbstract: In this paper, we address the problem of distributed interference management of cognitive femtocells that share the same frequency range with macrocells (primary user) using distributed multi-agent Q-learning. We formulate and solve three problems representing three different Q-learning algorithms: namely, centralized, distributed and partially distributed power control using Q-learning (CPC-Q, DPC-Q and PDPC-Q). CPCQ, although not of practical interest, characterizes the global optimum. Each of DPC-Q and PDPC-Q works in two different learning paradigms: Independent (IL) and Cooperative (CL). The former is considered the simplest form for applying Qlearning in multi-agent scenarios, where all the femtocells learn independently. The latter is the proposed scheme in which femtocells share partial information during the learning process in order to strike a balance between practical relevance and performance. In terms of performance, the simulation results showed that the CL paradigm outperforms the IL paradigm and achieves an aggregate femtocells capacity that is very close to the optimal one. For the practical relevance issue, we evaluate the robustness and scalability of DPC-Q, in real time, by deploying new femtocells in the system during the learning process, where we showed that DPC-Q in the CL paradigm is scalable to large number of femtocells and more robust to the network dynamics compared to the IL paradigm',\n",
       "  'Title: fusing text and image for event detection in twitter\\nAbstract: In this contribution, we develop an accurate and effective event detection method to detect events from a Twitter stream, which uses visual and textual information to improve the performance of the mining process. The method monitors a Twitter stream to pick up tweets having texts and images and stores them into a database. This is followed by applying a mining algorithm to detect an event. The procedure starts with detecting events based on text only by using the feature of the bag-of-words which is calculated using the term frequency-inverse document frequency (TF-IDF) method. Then it detects the event based on image only by using visual features including histogram of oriented gradients (HOG) descriptors, grey-level cooccurrence matrix (GLCM), and color histogram. K nearest neighbours (Knn) classification is used in the detection. The final decision of the event detection is made based on the reliabilities of text only detection and image only detection. The experiment result showed that the proposed method achieved high accuracy of 0.94, comparing with 0.89 with texts only, and 0.86 with images only .',\n",
       "  'Title: artificial intelligence markup language a brief tutorial\\nAbstract: The purpose of this paper is to serve as a reference guide for the development of chatterbots implemented with the AIML language. In order to achieve this, the main concepts in Pattern Recognition area are described because the AIML uses such theoretical framework in their syntactic and semantic structures. After that, AIML language is described and each AIML command/tag is followed by an application example. Also, the usage of AIML embedded tags for the handling of sequence dialogue limitations between humans and machines is shown. Finally, computer systems that assist in the design of chatterbots with the AIML language are classified and described.',\n",
       "  'Title: anomaly detection in online social networks\\nAbstract: Anomalies in online social networks can signify irregular, and often illegal behaviour. Anomalies in online social networks can signify irregular, and often illegal behaviour. Detection of such anomalies has been used to identify malicious individuals, including spammers, sexual predators, and online fraudsters. In this paper we survey existing computational techniques for detecting anomalies in online social networks. We characterise anomalies as being either static or dynamic, and as being labelled or unlabelled, and survey methods for detecting these different types of anomalies. We suggest that the detection of anomalies in online social networks is composed of two sub-processes; the selection and calculation of network features, and the classification of observations from this feature space. In addition, this paper provides an overview of the types of problems that anomaly detection can address and identifies key areas of future research.',\n",
       "  'Title: efficient synthesis of network updates\\nAbstract: Software-defined networking (SDN) is revolutionizing the networking industry, but current SDN programming platforms do not provide automated mechanisms for updating global configurations on the fly. Implementing updates by hand is challenging for SDN programmers because networks are distributed systems with hundreds or thousands of interacting nodes. Even if initial and final configurations are correct, naively updating individual nodes can lead to incorrect transient behaviors, including loops, black holes, and access control violations. This paper presents an approach for automatically synthesizing updates that are guaranteed to preserve specified properties. We formalize network updates as a distributed programming problem and develop a synthesis algorithm based on counterexample-guided search and incremental model checking. We describe a prototype implementation, and present results from experiments on real-world topologies and properties demonstrating that our tool scales to updates involving over one-thousand nodes.',\n",
       "  'Title: coalgebraic trace semantics for continuous probabilistic transition systems\\nAbstract: Coalgebras in a Kleisli category yield a generic definition of trace#N#semantics for various types of labelled transition systems. In this paper we#N#apply this generic theory to generative probabilistic transition systems, short#N#PTS, with arbitrary (possibly uncountable) state spaces. We consider the#N#sub-probability monad and the probability monad (Giry monad) on the category of#N#measurable spaces and measurable functions. Our main contribution is that the#N#existence of a final coalgebra in the Kleisli category of these monads is#N#closely connected to the measure-theoretic extension theorem for sigma-finite#N#pre-measures. In fact, we obtain a practical definition of the trace measure#N#for both finite and infinite traces of PTS that subsumes a well-known result#N#for discrete probabilistic transition systems. Finally we consider two example#N#systems with uncountable state spaces and apply our theory to calculate their#N#trace measures.',\n",
       "  'Title: using built in domain specific modeling support to guide model based test generation\\nAbstract: We present a model-based testing approach to support automated test generation with domain-specific concepts. This includes a language expert who is an expert at building test models and domain experts who are experts in the domain of the system under test. First, we provide a framework to support the language expert in building test models using a full (Java) programming language with the help of simple but powerful modeling elements of the framework. Second, based on the model built with this framework, the toolset automatically forms a domain-specific modeling language that can be used to further constrain and guide test generation from these models by a domain expert. This makes it possible to generate a large set of test cases covering the full model, chosen (constrained) parts of the model, or manually define specific test cases on top of the model while using concepts familiar to the domain experts.',\n",
       "  'Title: cooperative relaying under spatially and temporally correlated interference\\nAbstract: We analyze the performance of an interference-limited decode-and-forward cooperative relaying system that comprises a source, a destination, and    $N$   relays, arbitrarily placed on the plane and suffering from interference by a set of interferers placed according to a spatial Poisson process. In each transmission attempt, first, the transmitter sends a packet; subsequently, a single one of the relays that received the packet correctly, if such a relay exists, retransmits it. We consider both selection combining and maximal ratio combining at the destination, Rayleigh fading, and interferer mobility. We derive expressions for the probability that a single transmission attempt is successful, as well as for the distribution of the transmission attempts until a packet is successfully transmitted. Results provide design guidelines applicable to a wide range of systems. Overall, the temporal and spatial characteristics of the interference play a significant role in shaping the system performance. Maximal ratio combining is only helpful when relays are close to the destination; in harsh environments, having many relays is particularly helpful, and relay placement is critical; the performance improves when interferer mobility increases; and a tradeoff exists between energy efficiency and throughput.',\n",
       "  'Title: an information theoretic location verification system for wireless networks\\nAbstract: As location-based applications become ubiquitous in emerging wireless networks, a reliable Location Verification System (LVS) will be of growing importance. In this paper we propose, for the first time, a rigorous information-theoretic framework for an LVS. The theoretical framework we develop illustrates how the threshold used in the detection of a spoofed location can be optimized in terms of the mutual information between the input and output data of the LVS. In order to verify the legitimacy of our analytical framework we have carried out detailed numerical simulations. Our simulations mimic the practical scenario where a system deployed using our framework must make a binary Yes/No “malicious decision” to each snapshot of the signal strength values obtained by base stations. The comparison between simulation and analysis shows excellent agreement. Our optimized LVS framework provides a defence against location spoofing attacks in emerging wireless networks such as those envisioned for Intelligent Transport Systems, where verification of location information is of paramount importance.',\n",
       "  'Title: petri nets with time and cost\\nAbstract: We consider timed Petri nets, i.e., unbounded Petri nets where each token carries a real-valued clock. Transition arcs are labeled with time intervals, which specify constraints on the ages of tokens. Our cost model assigns token storage costs per time unit to places, and firing costs to transitions. We study the cost to reach a given control-state. In general, a cost-optimal run may not exist. However,we show that the infimum of the costs is computable.',\n",
       "  'Title: novelty detection under multi label multi instance framework\\nAbstract: Novelty detection plays an important role in machine learning and signal processing. This paper studies novelty detection in a new setting where the data object is represented as a bag of instances and associated with multiple class labels, referred to as multi-instance multi-label (MIML) learning. Contrary to the common assumption in MIML that each instance in a bag belongs to one of the known classes, in novelty detection, we focus on the scenario where bags may contain novel-class instances. The goal is to determine, for any given instance in a new bag, whether it belongs to a known class or a novel class. Detecting novelty in the MIML setting captures many real-world phenomena and has many potential applications. For example, in a collection of tagged images, the tag may only cover a subset of objects existing in the images. Discovering an object whose class has not been previously tagged can be useful for the purpose of soliciting a label for the new object class. To address this novel problem, we present a discriminative framework for detecting new class instances. Experiments demonstrate the effectiveness of our proposed method, and reveal that the presence of unlabeled novel instances in training bags is helpful to the detection of such instances in testing stage.',\n",
       "  'Title: analysis and design of multi hop diffusion based molecular communication networks\\nAbstract: In this paper, we consider a multi-hop molecular communication network consisting of one nanotransmitter, one nanoreceiver, and multiple nanotransceivers acting as relays. We consider three different relaying schemes to improve the range of diffusion-based molecular communication. In the first scheme, different types of messenger molecules are utilized in each hop of the multi-hop network. In the second and third schemes, we assume that two types of molecules and one type of molecule are utilized in the network, respectively. We identify self-interference, backward intersymbol interference (backward-ISI), and forward-ISI as the performance-limiting effects for the second and third relaying schemes. Furthermore, we consider two relaying modes analogous to those used in wireless communication systems, namely full-duplex and half-duplex relaying. We propose the adaptation of the decision threshold as an effective mechanism to mitigate self-interference and backward-ISI at the relay for full-duplex and half-duplex transmission. We derive closed-form expressions for the expected end-to-end error probability of the network for the three considered relaying schemes. Furthermore, we derive closed-form expressions for the optimal number of molecules released by the nanotransmitter and the optimal detection threshold of the nanoreceiver for minimization of the expected error probability of each hop.',\n",
       "  'Title: the henchman problem measuring secrecy by the minimum distortion in a list\\nAbstract: We introduce a new measure of information-theoretic secrecy based on rate-distortion theory and study it in the context of the Shannon cipher system. Whereas rate-distortion theory is traditionally concerned with a single reconstruction sequence, in this work we suppose that an eavesdropper produces a list of $2^{nR_{\\\\sf L}}$ reconstruction sequences and measure secrecy by the minimum distortion over the entire list. We show that this setting is equivalent to one in which an eavesdropper must reconstruct a single sequence, but also receives side information about the source sequence and public message from a rate-limited henchman (a helper for an adversary). We characterize the optimal tradeoff of secret key rate, list rate, and eavesdropper distortion. The solution hinges on a problem of independent interest: lossy compression of a codeword drawn uniformly from a random codebook. We also characterize the solution to the lossy communication version of the problem in which distortion is allowed at the legitimate receiver. The analysis in both settings is greatly aided by a recent technique for proving source coding results with the use of a likelihood encoder.',\n",
       "  'Title: strategic port graph rewriting an interactive modelling and analysis framework\\nAbstract: We present strategic portgraph rewriting as a basis for the implementation of visual modelling and analysis tools. The goal is to facilitate the specification, analysis and simulation of complex systems, using port graphs. A system is represented by an initial graph and a collection of graph rewriting rules, together with a user-defined strategy to control the application of rules. The strategy language includes constructs to deal with graph traversal and management of rewriting positions in the graph. We give a small-step operational semantics for the language, and describe its implementation in the graph transformation and visualisation tool PORGY.',\n",
       "  'Title: oversampling increases the pre log of noncoherent rayleigh fading channels\\nAbstract: We analyze the capacity of a continuous-time, time- selective, Rayleigh block-fading channel in the high signal-to-noise ratio regime. The fading process is assumed stationary within each block and to change independently from block to block; further- more, its realizations are not known a priori to the transmitter and the receiver (noncoherent setting). A common approach to analyzing the capacity of this channel is to assume that the receiver performs matched filtering followed by sampling at symbol rate (symbol matched filtering). This yields a discrete-time channel in which each transmitted symbol corresponds to one output sample. Liang & Veeravalli (2004) showed that the capacity of this discrete- time channel grows logarithmically with the signal-to noise ratio (SNR), with a capacity pre-log equal to 1 − Q/N . Here, N is the number of symbols transmitted within one fading block, and Q is the rank of the covariance matrix of the discrete-time channel gains within each fading block. In this paper, we show that sym- bol matched filtering is not a capacity-achieving strategy for the underlying continuous-time channel. Specifically, we analyze the capacity pre-log of the discrete-time channel obtained by oversam- pling the continuous-time channel output, i.e., by sampling it faster than at symbol rate. We prove that by oversampling by a factor two one gets a capacity pre-log that is at least as large as 1 − 1/N . Since the capacity pre-log corresponding to symbol-rate sampling is 1−Q/N , our result implies indeed that symbol matched filtering is not capacity achieving at high SNR.',\n",
       "  'Title: iec 61499 vs 61131 a comparison based on misperceptions\\nAbstract: The IEC 61131 standard has been widely#R##N#accepted in the industrial automation domain. However, it is claimed that the#R##N#standard does not address today the new requirements of complex industrial#R##N#systems, which include among others, portability, interoperability, increased#R##N#reusability and distribution. To address these restrictions, the IEC has#R##N#initiated the task of developing the IEC 61499, which is presented as a mature#R##N#technology to enable intelligent automation in various domains. This standard#R##N#was not accepted by industry even though it is highly promoted by the academic#R##N#community. In this paper, it is argued that IEC 61499 has been promoted by#R##N#academy based on unsubstantiated claims on its main features, i.e., reusability, portability, interoperability, event-driven#R##N#execution. A number of misperceptions are presented and discussed in this paper#R##N#to show that the comparison, which appears in the literature, between IEC 61499#R##N#and 61131 is not substantiated.',\n",
       "  'Title: the entropy of conditional markov trajectories\\nAbstract: To quantify the randomness of Markov trajectories with fixed initial and final states, Ekroot and Cover proposed a closed-form expression for the entropy of trajectories of an irreducible finite state Markov chain. Numerous applications, including the study of random walks on graphs, require the computation of the entropy of Markov trajectories conditional on a set of intermediate states. However, the expression of Ekroot and Cover does not allow for computing this quantity. In this paper, we propose a method to compute the entropy of conditional Markov trajectories through a transformation of the original Markov chain into a Markov chain that exhibits the desired conditional distribution of trajectories. Moreover, we express the entropy of Markov trajectories-a global quantity-as a linear combination of local entropies associated with the Markov chain states.',\n",
       "  'Title: distributive laws and decidable properties of sos specifications\\nAbstract: Some formats of well-behaved operational specifications, correspond to natural transformations of certain types (for example, GSOS and coGSOS laws). These transformations have a common generalization: distributive laws of monads over comonads. We prove that this elegant theoretical generalization has limited practical benefits: it does not translate to any concrete rule format that would be complete for specifications that contain both GSOS and coGSOS rules. This is shown for the case of labeled transition systems and deterministic stream systems.',\n",
       "  'Title: on covert acoustical mesh networks in air\\nAbstract: Covert channels can be used to circumvent system and network policies by establishing communications that have not been considered in the design of the computing system. We construct a covert channel between different computing systems that utilizes audio modulation/demodulation to exchange data between the computing systems over the air medium. The underlying network stack is based on a communication system that was originally designed for robust underwater communication. We adapt the communication system to implement covert and stealthy communications by utilizing the ultrasonic frequency range. We further demonstrate how the scenario of covert acoustical communication over the air medium can be extended to multi-hop communications and even to wireless mesh networks. A covert acoustical mesh network can be conceived as a meshed botnet or malnet that is accessible via inaudible audio transmissions. Different applications of covert acoustical mesh networks are presented, including the use for remote keylogging over multiple hops. It is shown that the concept of a covert acoustical mesh network renders many conventional security concepts useless, as acoustical communications are usually not considered. Finally, countermeasures against covert acoustical mesh networks are discussed, including the use of lowpass filtering in computing systems and a host-based intrusion detection system for analyzing audio input and output in order to detect any irregularities.',\n",
       "  'Title: high energy first hef heuristic for energy efficient target coverage problem\\nAbstract: Target coverage problem in wireless sensor networks is concerned with maximizing the lifetime of the network while continuously monitoring a set of targets. A sensor covers targets which are within the sensing range. For a set of sensors and a set of targets, the sensor-target coverage relationship is assumed to be known. A sensor cover is a set of sensors that covers all the targets. The target coverage problem is to determine a set of sensor covers with maximum aggregated lifetime while constraining the life of each sensor by its initial battery life. The problem is proved to be NP-complete and heuristic algorithms to solve this problem are proposed. In the present study, we give a unified interpretation of earlier algorithms and propose a new and efficient algorithm. We show that all known algorithms are based on a common reasoning though they seem to be derived from different algorithmic paradigms. We also show that though some algorithms guarantee bound on the quality of the solution, this bound is not meaningful and not practical too. Our interpretation provides a better insight to the solution techniques. We propose a new greedy heuristic which prioritizes sensors on residual battery life. We show empirically that the proposed algorithm outperforms all other heuristics in terms of quality of solution. Our experimental study over a large set of randomly generated problem instances also reveals that a very na\\\\\"ive greedy approach yields solutions which is reasonably (appx. 10%) close to the actual optimal solutions.',\n",
       "  \"Title: dynamic models of reputation and competition in job market matching\\nAbstract: A fundamental decision faced by a firm hiring employees --- and a familiar one to anyone who has dealt with the academic job market, for example --- is deciding what caliber of candidates to pursue. Should the firm try to increase its reputation by making offers to higher-quality candidates, despite the risk that the candidates might reject the offers and leave the firm empty-handed? Or is it better to play it safe and go for weaker candidates who are more likely to accept the offer? The question acquires an added level of complexity once we take into account the effect one hiring cycle has on the next: hiring better employees in the current cycle increases the firm's reputation, which in turn increases its attractiveness for higher-quality candidates in the next hiring cycle. These considerations introduce an interesting temporal dynamic aspect to the rich line of research on matching models for job markets, in which long-range planning and evolving reputational effects enter into the strategic decisions made by competing firms.   The full set of ingredients in such recruiting decisions is complex, and this has made it difficult to model the fundamental strategic tension at the core of the problem. Here we develop a model based on two competing firms to try capturing as cleanly as possible the elements that we believe constitute this strategic tension: the trade-off between short-term recruiting success and long-range reputation-building; the inefficiency that results from underemployment of people who are not ranked highest; and the influence of earlier accidental outcomes on long-term reputations.   Our model exhibits all these phenomena in a stylized setting, governed by a parameter $q$ that captures the difference in strength between the top candidate in each hiring cycle and the next best. Building on an economic model of competition between parties of unequal strength, we show that when $q$ is relatively low, the efficiency of the job market is improved by long-range reputational effects, but when $q$ is relatively high, taking future reputations into account can sometimes reduce the efficiency. While this trade-off arises naturally in the model, the multi-period nature of the strategic reasoning it induces adds new sources of complexity, and our analysis reveals interesting connections between competition with evolving reputations and the the dynamics of urn processes.\",\n",
       "  'Title: uplink performance evaluation of massive mu mimo systems\\nAbstract: body abstract heading (also in Bold) Abstract— The present paper deals with an OFDM-based uplink within a multi-user MIMO (MU-MIMO) system where a massive MIMO approach is employed. In this context, the linear detectors Minimum Mean-Squared Error (MMSE), Zero Forcing (ZF) and Maximum Ratio Combining (MRC) are considered and assessed. This papers includes Bit Error Rate (BER) results for uncoded QPSK/OFDM transmissions through a flat Rayleigh fading channel under the assumption of perfect power control and channel estimation. BER results are obtained through Monte Carlo simulations. Performance results are dis- cussed in detail and we confirm the achievable \"massive MIMO\" effects, even for a reduced complexity detection technique, when the number of receive antennas at BS is much larger than the number of transmit antennas. Index Terms— Massive MU-MIMO, uplink detection, OFDM.',\n",
       "  'Title: tactics for reasoning modulo ac in coq\\nAbstract: We present a set of tools for rewriting modulo associativity and commutativity (AC) in Coq, solving a long-standing practical problem. We use two building blocks: first, an extensible reflexive decision procedure for equality modulo AC; second, an OCaml plug-in for pattern matching modulo AC. We handle associative only operations, neutral elements, uninterpreted function symbols, and user-defined equivalence relations. By relying on type-classes for the reification phase, we can infer these properties automatically, so that end-users do not need to specify which operation is A or AC, or which constant is a neutral element.',\n",
       "  'Title: faster radix sort via virtual memory and write combining\\nAbstract: Sorting algorithms are the deciding factor for the performance of common operations such as removal of duplicates or database sort-merge joins. This work focuses on 32-bit integer keys, optionally paired with a 32-bit value. We present a fast radix sorting algorithm that builds upon a microarchitecture-aware variant of counting sort. Taking advantage of virtual memory and making use of write-combining yields a per-pass throughput corresponding to at least 88 % of the system’s peak memory bandwidth. Our implementation outperforms Intel’s recently published radix sort by a factor of 1.5. It also compares favorably to the reported performance of an algorithm for Fermi GPUs when data-transfer overhead is included. These results indicate that scalar, bandwidth-sensitive sorting algorithms remain competitive on current architectures. Various other memory-intensive applications can benefit from the techniques described herein.',\n",
       "  'Title: detecting intentional packet drops on the internet via tcp ip side channels extended version\\nAbstract: We describe a method for remotely detecting intentional packet drops on the Internet via side channel inferences. That is, given two arbitrary IP addresses on the Internet that meet some simple requirements, our proposed technique can discover packet drops (e.g., due to censorship) between the two remote machines, as well as infer in which direction the packet drops are occurring. The only major requirements for our approach are a client with a global IP Identifier (IPID) and a target server with an open port. We require no special access to the client or server. Our method is robust to noise because we apply intervention analysis based on an autoregressive-moving-average (ARMA) model. In a measurement study using our method featuring clients from multiple continents, we observed that, of all measured client connections to Tor directory servers that were censored, 98% of those were from China, and only 0.63% of measured client connections from China to Tor directory servers were not censored. This is congruent with current understandings about global Internet censorship, leading us to conclude that our method is effective.',\n",
       "  'Title: cognitive and cache enabled d2d communications in cellular networks\\nAbstract: Caching popular contents for frequent access is an attractive way of employing the redundancy of user requests. Exploiting cognition to the cache-enabled D2D in the multichannel cellular network is the main focus of this paper. We contribute to analyzing the cache-based content delivery in a twotier heterogeneous network (HetNet) composed of base stations (BSs) and device-to-device (D2D) pairs, where the D2D accesses the networks with overlay spectrum sharing. Node locations are first modeled as mutually independent Poisson Point Processes (PPPs), and the service queueing process is formulated. The corresponding tier association and cognitive access protocol are developed. The D2D transmitter (TX) performs overlay spectrum sensing within its spectrum sensing region (SSR) to detect the idleness of cellular channels. Then the number of BSs and D2D TXs in the SSR are analyzed. We further elaborate the probability mass function (PMF) of the delay and the queue length, with modeling the traffic dynamics of request arrivals and departures at the BS and D2D TX as the discrete-time multiserver queue with priorities. Moreover, impacts of some key network parameters, e.g., the content popularity, the request density and the caching storage, on the system performance are investigated to provide a valuable insight.',\n",
       "  'Title: using transmit only sensors to reduce deployment cost of wireless sensor networks\\nAbstract: We consider a hybrid wireless sensor network with regular and transmit-only sensors. The transmit-only sensors do not have the receiver circuit (or have a very low data-rate one), hence are cheaper and less energy consuming, but their transmissions cannot be coordinated. Regular sensors, also called cluster-heads, are responsible for receiving information from the transmit-only sensors and forwarding it to sinks. The main goal of such a hybrid network is to reduce the cost of deployment while achieving some performance goals (minimum coverage, sensing rate, etc). In this paper we are interested in the communication between the transmit-only sensors and the cluster-heads. Since the sensors have no feedback, their transmission schedule is random. The cluster-heads, on the contrary, adapt their reception policy to achieve the performance goals. Using a mathematical model of random access networks developed in [1] we define and evaluate packet admission policies for different performance criteria. We show that the proposed hybrid network architecture, using the optimal policies, can achieve substantial dollar cost and power consumption savings as compared to conventional architectures while providing the same performance guarantees.',\n",
       "  'Title: high sir transmission capacity of wireless networks with general fading and node distribution\\nAbstract: In many wireless systems, interference is the main performance-limiting factor, and is primarily dictated by the locations of concurrent transmitters. In many earlier works, the locations of the transmitters is often modeled as a Poisson point process for analytical tractability. While analytically convenient, the PPP only accurately models networks whose nodes are placed independently and use ALOHA as the channel access protocol, which preserves the independence. Correlations between transmitter locations in non-Poisson networks, which model intelligent access protocols, makes the outage analysis extremely difficult. In this paper, we take an alternative approach and focus on an asymptotic regime where the density of interferers η goes to 0. We prove for general node distributions and fading statistics that the success probability Ps ~ 1 - γηκ for η → 0, and provide values of γ and κ for a number of important special cases. We show that κ is lower bounded by 1 and upper bounded by a value that depends on the path loss exponent and the fading. This new analytical framework is then used to characterize the transmission capacity of a very general class of networks, defined as the maximum spatial density of active links given an outage constraint.',\n",
       "  'Title: towards a multi criteria development distribution model an analysis of existing task distribution approaches\\nAbstract: Distributing development tasks in the context of global software development bears both many risks and many opportunities. Nowadays, distributed development is often driven by only a few factors or even just a single factor such as workforce costs. Risks and other relevant factors such as workforce capabilities, the innovation potential of different regions, or cultural factors are often not recognized sufficiently. This could be improved by using empirically-based multi-criteria distribution models. Currently, there is a lack of such decision models for distributing software development work. This article focuses on mechanisms for such decision support. First, requirements for a distribution model are formulated based on needs identified from practice. Then, distribution models from different domains are surveyed, compared, and analyzed in terms of suitability. Finally, research questions and directions for future work are given.',\n",
       "  'Title: proportional fair mu mimo in 802 11 wlans\\nAbstract: We consider the proportional fair rate allocation in an 802.11 WLAN that supports multi-user MIMO (MU-MIMO) transmission by one or more stations. We characterise, for the first time, the proportional fair allocation of MU-MIMO spatial streams and station transmission opportunities. While a number of features carry over from the case without MU-MIMO, in general neither flows nor stations need to be allocated equal airtime when MU-MIMO is available.',\n",
       "  'Title: dynamic autotuning of adaptive fast multipole methods on hybrid multicore cpu and gpu systems\\nAbstract: Dynamic autotuning of adaptive fast multipole methods on hybrid multicore CPU and GPU systems',\n",
       "  'Title: codes between mbr and msr points with exact repair property\\nAbstract: In this paper, distributed storage systems with exact repair are studied. Constructions for exact-regenerating codes between the minimum storage regenerating (MSR) and the minimum bandwidth regenerating (MBR) points are given. To the best of our knowledge, no previous construction of exact-regenerating codes between MBR and MSR points is done except in the works by Tian et al. and Sasidharan et al. In contrast to their works, the methods used here are elementary. In this paper, it is shown that in the case that the parameters \\\\(n\\\\) , \\\\(k\\\\) , and \\\\(d\\\\) are close to each other, the given construction is close to optimal when comparing with the known functional repair capacity. This is done by showing that when the distances of the parameters \\\\(n\\\\) , \\\\(k\\\\) , and \\\\(d\\\\) are fixed but the actual values approach to infinity, the fraction of the performance of constructed codes with exact repair and the known capacity of codes with functional repair, approaches to one. Also, a simple variation of the constructed codes with almost the same performance is given. Also some bounds for the capacity of exact-repairing codes are given. These bounds illustrate the relationships between storage codes with different parameters.',\n",
       "  'Title: efficient reconciliation protocol for discrete variable quantum key distribution\\nAbstract: Reconciliation is an essential part of any secret-key agreement protocol and hence of a Quantum Key Distribution (QKD) protocol, where two legitimate parties are given correlated data and want to agree on a common string in the presence of an adversary, while revealing a minimum amount of information. In this paper, we show that for discrete-variable QKD protocols, this problem can be advantageously solved with Low Density Parity Check (LDPC) codes optimized for the binary symmetric channel (BSC). In particular, we demonstrate that our method leads to a significant improvement of the achievable secret key rate, with respect to earlier interactive reconciliation methods used in QKD.',\n",
       "  'Title: the evolution of navigable small world networks\\nAbstract: Small-world networks, which combine randomized and structured elements, are seen as prevalent in nature. Several random graph models have been given for small-world networks, with one of the most fruitful, introduced by Jon Kleinberg [10], showing in which type of graphs it is possible to route, or navigate, between vertices with very little knowledge of the graph itself. Kleinberg’s model is static, with random edges added to a fixed grid. In this paper we introduce, analyze and test a randomized algorithm which successively rewires a graph with every application. The resulting process gives a model for the evolution of small-world networks with properties similar to those studied by Kleinberg.',\n",
       "  'Title: likelihood gradient evaluation using square root covariance filters\\nAbstract: Using the array form of numerically stable square-root implementation methods for Kalman filtering formulas, we construct a new square-root algorithm for the log-likelihood gradient (score) evaluation. This avoids the use of the conventional Kalman filter with its inherent numerical instabilities and improves the robustness of computations against roundoff errors. The new algorithm is developed in terms of covariance quantities and based on the ldquocondensed formrdquo of the array square-root filter.',\n",
       "  'Title: a tensor approach to learning mixed membership community models\\nAbstract: Community detection is the task of detecting hidden communities from observed interactions. Guaranteed community detection has so far been mostly limited to models with non-overlapping communities such as the stochastic block model. In this paper, we remove this restriction, and provide guaranteed community detection for a family of probabilistic network models with overlapping communities, termed as the mixed membership Dirichlet model, first introduced by Airoldi et al. This model allows for nodes to have fractional memberships in multiple communities and assumes that the community memberships are drawn from a Dirichlet distribution. Moreover, it contains the stochastic block model as a special case. We propose a unified approach to learning these models via a tensor spectral decomposition method. Our estimator is based on low-order moment tensor of the observed network, consisting of 3-star counts. Our learning method is fast and is based on simple linear algebraic operations, e.g. singular value decomposition and tensor power iterations. We provide guaranteed recovery of community memberships and model parameters and present a careful finite sample analysis of our learning method. As an important special case, our results match the best known scaling requirements for the (homogeneous) stochastic block model.',\n",
       "  'Title: optimizing continued fraction expansion based iir realization of fractional order differ integrators with genetic algorithm\\nAbstract: Rational approximation of fractional order (FO) differ-integrators via Continued Fraction Expansion (CFE) is a well known technique. In this paper, the nominal structures of various generating functions are optimized using Genetic Algorithm (GA) to minimize the deviation in magnitude and phase response between the original FO element and the rationalized discrete time filter in Infinite Impulse Response (IIR) structure. The optimized filter based realizations show better approximation of the FO elements in comparison with the existing methods and is demonstrated by the frequency response of the IIR filters.',\n",
       "  'Title: towards randomized testing of q monomials in multivariate polynomials\\nAbstract: Given any fixed integer $q\\\\ge 2$, a $q$-monomial is of the format $\\\\displaystyle x^{s_1}_{i_1}x^{s_2}_{i_2}...x_{i_t}^{s_t}$ such that $1\\\\le s_j \\\\le q-1$, $1\\\\le j \\\\le t$. $q$-monomials are natural generalizations of multilinear monomials. Recent research on testing multilinear monomials and $q$-monomails for prime $q$ in multivariate polynomials relies on the property that $Z_q$ is a field when $q\\\\ge 2 $ is prime. When $q>2$ is not prime, it remains open whether the problem of testing $q$-monomials can be solved in some compatible complexity. In this paper, we present a randomized $O^*(7.15^k)$ algorithm for testing $q$-monomials of degree $k$ that are found in a multivariate polynomial that is represented by a tree-like circuit with a polynomial size, thus giving a positive, affirming answer to the above question. Our algorithm works regardless of the primality of $q$ and improves upon the time complexity of the previously known algorithm for testing $q$-monomials for prime $q>7$.',\n",
       "  'Title: convergence of tâtonnement in fisher markets\\nAbstract: Analyzing simple and natural price-adjustment processes that converge to a market equilibrium is a fundamental question in economics. Such an analysis may have implications in economic theory, computational economics, and distributed systems. Tatonnement, proposed by Walras in 1874, is a process by which prices go up in response to excess demand, and down in response to excess supply. This paper analyzes the convergence of a time-discrete tatonnement process, a problem that recently attracted considerable attention of computer scientists. We prove that the simple tatonnement process that we consider converges (efficiently) to equilibrium pri ces and allocation in markets with nested CES-Leontief utilities, generalizing some of the previous convergence proofs for more restricted types of utility functions.',\n",
       "  'Title: dolfin automated finite element computing\\nAbstract: We describe here a library aimed at automating the solution of partial differential equations using the finite element method. By employing novel techniques for automated code generation, the library combines a high level of expressiveness with efficient computation. Finite element variational forms may be expressed in near mathematical notation, from which low-level code is automatically generated, compiled, and seamlessly integrated with efficient implementations of computational meshes and high-performance linear algebra. Easy-to-use object-oriented interfaces to the library are provided in the form of a C++ library and a Python module. This article discusses the mathematical abstractions and methods used in the design of the library and its implementation. A number of examples are presented to demonstrate the use of the library in application code.',\n",
       "  'Title: differential equations for algebraic functions\\nAbstract: It is classical that univariate algebraic functions satisfy linear differential equations with polynomial coefficients. Linear recurrences follow for the coefficients of their power series expansions. We show that the linear differential equation of minimal order has coefficients whose degree is cubic in the degree of the function. We also show that there exists a linear differential equation of order linear in the degree whose coefficients are only of quadratic degree. Furthermore, we prove the existence of recurrences of order and degree close to optimal. We study the complexity of computing these differential equations and recurrences. We deduce a fast algorithm for the expansion of algebraic series.',\n",
       "  'Title: test bed based comparison of single and parallel tcp and the impact of parallelism on throughput and fairness in heterogenous networks\\nAbstract: Parallel Transport Control Protocol (TCP) has been used to effectively utilize bandwidth for data intensive applications over high Bandwidth-Delay Product (BDP) networks. On the other hand, it has been argued that, a single based TCP connection with proper modification such as HSTCP can emulate and capture the robustness of parallel TCP and can well replace it. In this work a Comparison between Single-Based and the proposed parallel TCP has been conducted to show the differences in their performance measurements such as throughput performance and throughput ratio, as well as the link sharing Fairness also has been observed to show the impact of using the proposed Parallel TCP on the existing Single-Based TCP connections. The experiment results show that, single-based TCP cannot overcome Parallel TCP especially in heterogeneous networks where the packet losses are common. Furthermore, the proposed parallel TCP does not affect TCP fairness which makes parallel TCP highly recommended to effectively utilize bandwidth for data intensive applications.',\n",
       "  'Title: navigation domain representation for interactive multiview imaging\\nAbstract: Enabling users to interactively navigate through different viewpoints of a static scene is a new interesting functionality in 3D streaming systems. While it opens exciting perspectives toward rich multimedia applications, it requires the design of novel representations and coding techniques to solve the new challenges imposed by the interactive navigation. In particular, the encoder must prepare a priori a compressed media stream that is flexible enough to enable the free selection of multiview navigation paths by different streaming media clients. Interactivity clearly brings new design constraints: the encoder is unaware of the exact decoding process, while the decoder has to reconstruct information from incomplete subsets of data since the server generally cannot transmit images for all possible viewpoints due to resource constrains. In this paper, we propose a novel multiview data representation that permits us to satisfy bandwidth and storage constraints in an interactive multiview streaming system. In particular, we partition the multiview navigation domain into segments, each of which is described by a reference image (color and depth data) and some auxiliary information. The auxiliary information enables the client to recreate any viewpoint in the navigation segment via view synthesis. The decoder is then able to navigate freely in the segment without further data request to the server; it requests additional data only when it moves to a different segment. We discuss the benefits of this novel representation in interactive navigation systems and further propose a method to optimize the partitioning of the navigation domain into independent segments, under bandwidth and storage constraints. Experimental results confirm the potential of the proposed representation; namely, our system leads to similar compression performance as classical inter-view coding, while it provides the high level of flexibility that is required for interactive streaming. Because of these unique properties, our new framework represents a promising solution for 3D data representation in novel interactive multimedia services.',\n",
       "  'Title: on the metric distortion of nearest neighbour graphs on random point sets\\nAbstract: We study the graph constructed on a Poisson point process in $d$ dimensions by connecting each point to the $k$ points nearest to it. This graph a.s. has an infinite cluster if $k > k_c(d)$ where $k_c(d)$, known as the critical value, depends only on the dimension $d$. This paper presents an improved upper bound of 188 on the value of $k_c(2)$. We also show that if $k \\\\geq 188$ the infinite cluster of $\\\\NN(2,k)$ has an infinite subset of points with the property that the distance along the edges of the graphs between these points is at most a constant multiplicative factor larger than their Euclidean distance. Finally we discuss in detail the relevance of our results to the study of multi-hop wireless sensor networks.',\n",
       "  'Title: an analysis of device free and device based wifi localization systems\\nAbstract: WiFi-based localization became one of the main indoor localization techniques due to the ubiquity of WiFi connectivity. However, indoor environments exhibit complex wireless propagation characteristics. Typically, these characteristics are captured by constructing a fingerprint map for the different locations in the area of interest. This finger print requires significant overhead in manual construction, and thus has been one of the major drawbacks of WiFi-based localization. In this paper, the authors present an automated tool for finger print constructions and leverage it to study novel scenarios for device-based and device-free WiFi-based localization that are difficult to evaluate in a real environment. In a particular, the authors examine the effect of changing the access points AP mounting location, AP technology upgrade, crowd effect on calibration and operation, among others; on the accuracy of the localization system. The authors present the analysis for the two classes of WiFi-based localization: device-based and device-free. The authors analysis highlights factors affecting the localization system accuracy, how to tune it for better localization, and provides insights for both researchers and practitioners.',\n",
       "  'Title: directed information graphs\\nAbstract: We propose a graphical model for representing networks of stochastic processes, the minimal generative model graph. It is based on reduced factorizations of the joint distribution over time. We show that under appropriate conditions, it is unique and consistent with another type of graphical model, the directed information graph, which is based on a generalization of Granger causality. We demonstrate how directed information quantifies Granger causality in a particular sequential prediction setting. We also develop efficient methods to estimate the topological structure from data that obviate estimating the joint statistics. One algorithm assumes upper-bounds on the degrees and uses the minimal dimension statistics necessary. In the event that the upper-bounds are not valid, the resulting graph is nonetheless an optimal approximation. Another algorithm uses near-minimal dimension statistics when no bounds are known but the distribution satisfies a certain criterion. Analogous to how structure learning algorithms for undirected graphical models use mutual information estimates, these algorithms use directed information estimates. We characterize the sample-complexity of two plug-in directed information estimators and obtain confidence intervals. For the setting when point estimates are unreliable, we propose an algorithm that uses confidence intervals to identify the best approximation that is robust to estimation error. Lastly, we demonstrate the effectiveness of the proposed algorithms through analysis of both synthetic data and real data from the Twitter network. In the latter case, we identify which news sources influence users in the network by merely analyzing tweet times.',\n",
       "  'Title: design and analysis of a multi carrier differential chaos shift keying communication system\\nAbstract: A new Multi-Carrier Differential Chaos Shift Keying (MC-DCSK) modulation is presented in this paper. The system endeavors to provide a good trade-off between robustness, energy efficiency and high data rate, while still being simple compared to conventional multi-carrier spread spectrum systems. This system can be seen as a parallel extension of the DCSK modulation where one chaotic reference sequence is transmitted over a predefined subcarrier frequency. Multiple modulated data streams are transmitted over the remaining subcarriers. This transmitter structure increases the spectral efficiency of the conventional DCSK system and uses less energy. The receiver design makes this system easy to implement where no radio frequency (RF) delay circuit is needed to demodulate received data. Various system design parameters are discussed throughout the paper, including the number of subcarriers, the spreading factor, and the transmitted energy. Once the design is explained, the bit error rate performance of the MC-DCSK system is computed and compared to the conventional DCSK system under multipath Rayleigh fading and an additive white Gaussian noise (AWGN) channels. Simulation results confirm the advantages of this new hybrid design.',\n",
       "  'Title: depechemood a lexicon for emotion analysis from crowd annotated news\\nAbstract: While many lexica annotated with words polarity are available for sentiment analysis, very few tackle the harder task of emotion analysis and are usually quite limited in coverage. In this paper, we present a novel approach for extracting - in a totally automated way - a high-coverage and high-precision lexicon of roughly 37 thousand terms annotated with emotion scores, called DepecheMood. Our approach exploits in an original way \\'crowd-sourced\\' affective annotation implicitly provided by readers of news articles from rappler.com. By providing new state-of-the-art performances in unsupervised settings for regression and classification tasks, even using a na\\\\\"{\\\\i}ve approach, our experiments show the beneficial impact of harvesting social media data for affective lexicon building.',\n",
       "  'Title: a framework for specifying prototyping and reasoning about computational systems\\nAbstract: This thesis concerns the development of a framework that facilitates the design and analysis of formal systems. Specifically, this framework is intended to provide (1) a specification language which supports the concise and direct description of a system based on its informal presentation, (2) a mechanism for animating the specification language so that descriptions written in it can quickly and effectively be turned into prototypes of the systems they are about, and (3) a logic for proving properties of descriptions provided in the specification language and thereby of the systems they encode. A defining characteristic of the proposed framework is that it is based on two separate but closely intertwined logics. One of these is a specification logic that facilitates the description of computational structure while the other is a logic that exploits the special characteristics of the specification logic to support reasoning about the computational behavior of systems that are described using it. Both logics embody a natural treatment of binding structure by using the λ-calculus as a means for representing objects and by incorporating special mechanisms for working with such structure. By using this technique, they lift the treatment of binding from the object language into the domain of the relevant meta logic, thereby allowing the specification or analysis components to focus on the more essential logical aspects of the systems that are encoded. #R##N#One focus of this thesis is on developing a rich and expressive reasoning logic that is of use within the described framework. This work exploits a previously developed capability of definitions for embedding recursive specifications into the reasoning logic; this notion of definitions is complemented by a device for a case-analysis style reasoning over the descriptions they encode. Use is also made of a special kind of judgment called a generic judgment for reflecting object language binding into the meta logic and thereby for reasoning about such structure. Existing methods have, however, had a shortcoming in how they combine these two devices. Generic judgments lead to the introduction of syntactic objects called nominal constants into formulas and terms. The manner in which such objects are introduced often ensures that they satisfy certain properties which are necessary to take note of in the reasoning process. Unfortunately, this has heretofore not been possible to do. To overcome this problem, we introduce a special binary relation between terms called nominal abstraction and show this can be combined with definitions to encode the desired properties. The treatment of definitions is further enriched by endowing them with the capability of being interpreted inductively or co-inductively. The resulting logic is shown to be consistent and examples are presented to demonstrate its richness and usefulness in reasoning tasks. #R##N#This thesis is also concerned with the practical application of the logical machinery it develops. Specifically, it describes an interactive, tactic-style theorem prover called Abella that realizes the reasoning logic. Abella embodies the use of lemmas in proofs and also provides intuitively well-motivated tactics for inductive and co-inductive reasoning. The idea of reasoning using two-levels of logic is exploited in this context. This form of reasoning, pioneered by McDowell and Miller, embeds the specification logic explicitly into the reasoning logic and then reasons about particular specifications through this embedding. The usefulness of this approach is demonstrated by showing that general properties can be proved about the specification logic and then used as lemmas to simplify the overall reasoning process. We use these ideas together with Abella to develop several interesting and challenging proofs. The examples considered include ones in the recently proposed POPLmark challenge and a formalization of Girard’s proof of strong normalization for the simply-typed λ-calculus. We also explore the notion of adequacy that relates theorems proved using Abella to the properties of the object systems that are ultimately of primary interest. (Abstract shortened by UMI.)',\n",
       "  'Title: hierarchical hidden markov model in detecting activities of daily living in wearable videos for studies of dementia\\nAbstract: This paper presents a method for indexing activities of daily living in videos acquired from wearable cameras. It addresses the problematic of analyzing the complex multimedia data acquired from wearable devices, which has been recently a growing concern due to the increasing amount of this kind of multimedia data. In the context of dementia diagnosis by doctors, patient activities are recorded in the environment of their home using a lightweight wearable device, to be later visualized by the medical practitioners. The recording mode poses great challenges since the video data consists in a single sequence shot where strong motion and sharp lighting changes often appear. Because of the length of the recordings, tools for an efficient navigation in terms of activities of interest are crucial. Our work introduces a video structuring approach that combines automatic motion based segmentation of the video and activity recognition by a hierarchical two-level Hidden Markov Model. We define a multi-modal description space over visual and audio features, including mid-level features such as motion, location, speech and noise detections. We show their complementarities globally as well as for specific activities. Experiments on real data obtained from the recording of several patients at home show the difficulty of the task and the promising results of the proposed approach.',\n",
       "  'Title: the rank and hanna neumann property of some submonoids of a free monoid\\nAbstract: This work aims at further investigations on the work of Giambruno and Restivo [5] to find the rank of the intersection of two finitely generated submonoids of a free monoid. In this connection, we obtain the rank of a finitely generated submonoid of a free monoid that is accepted by semi-flower automaton with two bpi’s. Further, when the product automaton of two deterministic semi-flower automata with a unique bpi is semi-flower with two bpi’s, we obtain a sucient condition on the product automaton in order to satisfy the Hanna Neumann property.',\n",
       "  'Title: non linear estimation is easy\\nAbstract: Non-linear state estimation and some related topics like parametric estimation, fault diagnosis and perturbation attenuation are tackled here via a new methodology in numerical differentiation. The corresponding basic system theoretic definitions and properties are presented within the framework of differential algebra, which permits to handle system variables and their derivatives of any order. Several academic examples and their computer simulations, with online estimations, illustrate our viewpoint.',\n",
       "  'Title: fast and scalable structural svm with slack rescaling\\nAbstract: We present an efficient method for training slack-rescaled structural SVM. Although finding the most violating label in a margin-rescaled formulation is often easy since the target function decomposes with respect to the structure, this is not the case for a slack-rescaled formulation, and finding the most violated label might be very difficult. Our core contribution is an efficient method for finding the most-violating-label in a slack-rescaled formulation, given an oracle that returns the most-violating-label in a (slightly modified) margin-rescaled formulation. We show that our method enables accurate and scalable training for slack-rescaled SVMs, reducing runtime by an order of magnitude compared to previous approaches to slack-rescaled SVMs.',\n",
       "  'Title: a degree centrality in multi layered social network\\nAbstract: Multi-layered social networks reflect complex relationships existing in modern interconnected IT systems. In such a network each pair of nodes may be linked by many edges that correspond to different communication or collaboration user activities. Multi-layered degree centrality for multi-layered social networks is presented in the paper. Experimental studies were carried out on data collected from the real Web 2.0 site. The multi-layered social network extracted from this data consists of ten distinct layers and the network analysis was performed for different degree centralities measures.',\n",
       "  'Title: four conceptions of instruction sequence faults\\nAbstract: The notion of an instruction sequence fault is considered from various perspectives. Four different viewpoints on what constitutes a fault, or how to use the notion of a fault, are formulated. An integration of these views is proposed.',\n",
       "  'Title: a game theoretic analysis of incentives in content production and sharing over peer to peer networks\\nAbstract: Peer-to-peer (P2P) networks can be easily deployed to distribute user-generated content at a low cost, but the free-rider problem hinders the efficient utilization of P2P networks. Using game theory, we investigate incentive schemes to overcome the free-rider problem in content production and sharing. We build a basic model and obtain two benchmark outcomes: 1) the non-cooperative outcome without any incentive scheme and 2) the cooperative outcome. We then propose and examine three incentive schemes based on pricing, reciprocation, and intervention. We also study a brute-force scheme that enforces full sharing of produced content. We find that 1) cooperative peers share all produced content while non-cooperative peers do not share at all without an incentive scheme; 2) by utilizing the P2P network efficiently, the cooperative outcome achieves higher social welfare than the non-cooperative outcome does; 3) a cooperative outcome can be achieved among non-cooperative peers by introducing an incentive scheme based on pricing, reciprocation, or intervention; and 4) enforced full sharing has ambiguous welfare effects on peers. In addition to describing the solutions of different formulations, we discuss enforcement and informational requirements to implement each solution, aiming to offer a guideline for protocol design for P2P networks.',\n",
       "  'Title: subjective and objective quality assessment of image a survey\\nAbstract: With the increasing demand for image-based applications, the efficient and reliable evaluation of image quality has increased in importance. Measuring the image quality is of fundamental importance for numerous image processing applications, where the goal of image quality assessment (IQA) methods is to automatically evaluate the quality of images in agreement with human quality judgments. Numerous IQA methods have been proposed over the past years to fulfill this goal. In this paper, a survey of the quality assessment methods for conventional image signals, as well as the newly emerged ones, which includes the high dynamic range (HDR) and 3-D images, is presented. A comprehensive explanation of the subjective and objective IQA and their classification is provided. Six widely used subjective quality datasets, and performance measures are reviewed. Emphasis is given to the full-reference image quality assessment (FR-IQA) methods, and 9 often-used quality measures (including mean squared error (MSE), structural similarity index (SSIM), multi-scale structural similarity index (MS-SSIM), visual information fidelity (VIF), most apparent distortion (MAD), feature similarity measure (FSIM), feature similarity measure for color images (FSIMC), dynamic range independent measure (DRIM), and tone-mapped images quality index (TMQI)) are carefully described, and their performance and computation time on four subjective quality datasets are evaluated. Furthermore, a brief introduction to 3-D IQA is provided and the issues related to this area of research are reviewed.',\n",
       "  \"Title: edge intersection graphs of l shaped paths in grids\\nAbstract: In this paper we continue the study of the edge intersection graphs of one (or zero) bend paths on a rectangular grid. That is, the edge intersection graphs where each vertex is represented by one of the following shapes: źź , ź , źs , ź , and we consider zero bend paths (i.e., ź and-) to be degenerate źź 's. These graphs, called B 1 -EPG graphs, were first introduced by Golumbic etźal. (2009). We consider the natural subclasses of B 1 -EPG formed by the subsets of the four single bend shapes (i.e., { źź } , { źź , ź } , { źź , ź } , and { źź , ź , ź } ) and we denote the classes by źź , źź , ź , źź , ź , and źź , ź , ź respectively. Note: all other subsets are isomorphic to these up to 90 degree rotation. We show that testing for membership in each of these classes is NP-complete and observe the expected strict inclusions and incomparability (i.e., źź ź źź , ź , źź , ź ź źź , ź , ź ź B 1 -EPG and źź , ź is incomparable with źź , ź ). Additionally, we give characterizations and polytime recognition algorithms for special subclasses of Split ź ź źź .\",\n",
       "  'Title: convergence analysis using the edge laplacian robust consensus of nonlinear multi agent systems via iss method\\nAbstract: This study develops an original and innovative matrix representation with respect to the information flow for networked multi-agent system. To begin with, the general concepts of the edge Laplacian of digraph are proposed with its algebraic properties. Benefit from this novel graph-theoretic tool, we can build a bridge between the consensus problem and the edge agreement problem; we also show that the edge Laplacian sheds a new light on solving the leaderless consensus problem. Based on the edge agreement framework, the technical challenges caused by unknown but bounded disturbances and inherently nonlinear dynamics can be well handled. In particular, we design an integrated procedure for a new robust consensus protocol that is based on a blend of algebraic graph theory and the newly developed cyclic-small-gain theorem. Besides, to highlight the intricate relationship between the original graph and cyclic-small-gain theorem, the concept of edge-interconnection graph is introduced for the first time. Finally, simulation results are provided to verify the theoretical analysis.',\n",
       "  'Title: on the complexity of computing the capacity of codes that avoid forbidden difference patterns\\nAbstract: Some questions related to the computation of the capacity of codes that avoid forbidden difference patterns are analysed. The maximal number of n-bit sequences whose pairwise differences do not contain some given forbidden difference patterns is known to increase exponentially with n; the coefficient of the exponent is the capacity of the forbidden patterns. In this paper, new inequalities for the capacity are given that allow for the approximation of the capacity with arbitrary high accuracy. The computational cost of the algorithm derived from these inequalities is fixed once the desired accuracy is given. Subsequently, a polynomial time algorithm is given for determining if the capacity of a set is positive while the same problem is shown to be NP-hard when the sets of forbidden patterns are defined over an extended set of symbols. Finally, the existence of extremal norms is proved for any set of matrices arising in the capacity computation. Based on this result, a second capacity approximating algorithm is proposed. The usefulness of this algorithm is illustrated by computing exactly the capacity of particular codes that were only known approximately',\n",
       "  'Title: the long and the short of it summarising event sequences with serial episodes\\nAbstract: An ideal outcome of pattern mining is a small set of informative patterns, containing no redundancy or noise, that identifies the key structure of the data at hand. Standard frequent pattern miners do not achieve this goal, as due to the pattern explosion typically very large numbers of highly redundant patterns are returned.   We pursue the ideal for sequential data, by employing a pattern set mining approach - an approach where, instead of ranking patterns individually, we consider results as a whole. Pattern set mining has been successfully applied to transactional data, but has been surprisingly understudied for sequential data.   In this paper, we employ the MDL principle to identify the set of sequential patterns that summarises the data best. In particular, we formalise how to encode sequential data using sets of serial episodes, and use the encoded length as a quality score. As search strategy, we propose two approaches: the first algorithm selects a good pattern set from a large candidate set, while the second is a parameter-free any-time algorithm that mines pattern sets directly from the data. Experimentation on synthetic and real data demonstrates we efficiently discover small sets of informative patterns.',\n",
       "  'Title: sequential complexity as a descriptor for musical similarity\\nAbstract: We propose string compressibility as a descriptor of temporal structure in audio, for the purpose of determining musical similarity. Our descriptors are based on computing trackwise compression rates of quantized audio features, using multiple temporal resolutions and quantization granularities. To verify that our descriptors capture musically relevant information, we incorporate our descriptors into similarity rating prediction and song year prediction tasks. We base our evaluation on a dataset of 15500 track excerpts of Western popular music, for which we obtain 7800 web-sourced pairwise similarity ratings. To assess the agreement among similarity ratings, we perform an evaluation under controlled conditions, obtaining a rank correlation of 0.33 between intersected sets of ratings. Combined with bag-of-features descriptors, we obtain performance gains of 31.1% and 10.9% for similarity rating prediction and song year prediction. For both tasks, analysis of selected descriptors reveals that representing features at multiple time scales benefits prediction accuracy.',\n",
       "  'Title: a classification for community discovery methods in complex networks\\nAbstract: Many real-world networks are intimately organized according to a community structure. Much research effort has been devoted to develop methods and algorithms that can efficiently highlight this hidden structure of a network, yielding a vast literature on what is called today community detection. Since network representation can be very complex and can contain different variants in the traditional graph model, each algorithm in the literature focuses on some of these properties and establishes, explicitly or implicitly, its own definition of community. According to this definition, each proposed algorithm then extracts the communities, which typically reflect only part of the features of real communities. The aim of this survey is to provide a ‘user manual’ for the community discovery problem. Given a meta definition of what a community in a social network is, our aim is to organize the main categories of community discovery methods based on the definition of community they adopt. Given a desired definition of community and the features of a problem (size of network, direction of edges, multidimensionality, and so on) this review paper is designed to provide a set of approaches that researchers could focus on. The proposed classification of community discovery methods is also useful for putting into perspective the many open directions for further research. © 2011 Wiley Periodicals, Inc. Statistical Analysis and Data Mining 4: 512–546, 2011 © 2011 Wiley Periodicals, Inc.',\n",
       "  'Title: the cost of an epidemic over a complex network a random matrix approach\\nAbstract: In this paper we quantify the total economic impact of an epidemic over a#R##N#complex network using tools from random matrix theory. Incorporating the direct#R##N#and indirect costs of infection, we calculate the disease cost in the large#R##N#graph limit for an SIS (Susceptible - Infected - Susceptible) infection#R##N#process. We also give an upper bound on this cost for arbitrary finite graphs#R##N#and illustrate both calculated costs using extensive simulations on random and#R##N#real-world networks. We extend these calculations by considering the total#R##N#social cost of an epidemic, accounting for both the immunization and disease#R##N#costs for various immunization strategies and determining the optimal#R##N#immunization. Our work focuses on the transient behavior of the epidemic, in#R##N#contrast to previous research, which typically focuses on determining the#R##N#steady-state system equilibrium.',\n",
       "  'Title: the secrecy capacity of compound gaussian mimo wiretap channels\\nAbstract: Strong secrecy capacity of compound wiretap channels is studied. The known lower bounds for the secrecy capacity of compound finite-state memoryless channels under discrete alphabets are extended to arbitrary uncertainty sets and continuous alphabets under the strong secrecy criterion. The conditions under which these bounds are tight are given. Under the saddle-point condition, the compound secrecy capacity is shown to be equal to that of the worst-case channel. Based on this, the compound Gaussian MIMO wiretap channel is studied under the spectral norm constraint and without the degradedness assumption. First, it is assumed that only the eavesdropper channel is unknown, but is known to have a bounded spectral norm (maximum channel gain). The compound secrecy capacity is established in a closed form and the optimal signaling is identified: the compound capacity equals the worst-case channel capacity thus establishing the saddle-point property; the optimal signaling is Gaussian and on the eigenvectors of the legitimate channel and the worst-case eavesdropper is isotropic. The eigenmode power allocation somewhat resembles the standard water-filling but is not identical to it. More general uncertainty sets are considered and the existence of a maximum element is shown to be sufficient for a saddle-point to exist, so that signaling on the worst-case channel achieves the compound capacity of the whole class of channels. The case of rank-constrained eavesdropper is considered and the respective compound secrecy capacity is established. Subsequently, the case of additive uncertainty in the legitimate channel, in addition to the unknown eavesdropper channel, is studied. Its compound secrecy capacity and the optimal signaling are established in a closed-form as well, revealing the same saddle-point property.',\n",
       "  'Title: choreographies and behavioural contracts on the way to dynamic updates\\nAbstract: We survey our work on choreographies and behavioural contracts in multiparty interactions. In particular theories of behavioural contracts are presented which enable reasoning about correct service composition (contract compliance) and service substitutability (contract refinement preorder) under different assumptions concerning service communication: synchronous address or name based communication with patient non-preemptable or impatient invocations, or asynchronous communication. Correspondingly relations between behavioural contracts and choreographic descriptions are considered, where a contract for each communicating party is, e.g., derived by projection. The considered relations are induced as the maximal preoders which preserve contract compliance and global traces: we show maximality to hold (permitting services to be discovered/substituted independently for each party) when contract refinement preorders with all the above asymmetric communication means are considered and, instead, not to hold if the standard symmetric CCS/pi-calculus communication is considered (or when directly relating choreographies to behavioral contracts via a preorder, no matter the communication mean). The obtained maximal preorders are then characterized in terms of a new form of testing, called compliance testing, where not only tests must succeed but also the system under test (thus relating to controllability theory), and compared with classical preorders such as may/must testing, trace inclusion, etc. Finally, recent work about adaptable choreographies and behavioural contracts is presented, where the theory above is extended to update mechanisms allowing choreographies/contracts to be modified at run-time by internal (self-adaptation) or external intervention.',\n",
       "  \"Title: graph homomorphisms circular colouring and fractional covering by h cuts\\nAbstract: A graph homomorphism is a vertex map which carries edges from a source graph to edges in a target graph. The instances of the Weighted Maximum H-Colourable Subgraph problem (MAX H-COL) are edge-weighted graphs G and the objective is to find a subgraph of G that has maximal total edge weight, under the condition that the subgraph has a homomorphism to H; note that for H=K_k this problem is equivalent to MAX k-CUT. Farnqvist et al. have introduced a parameter on the space of graphs that allows close study of the approximability properties of MAX H-COL. Specifically, it can be used to extend previously known (in)approximability results to larger classes of graphs. Here, we investigate the properties of this parameter on circular complete graphs K_{p/q}, where 2 <= p/q <= 3. The results are extended to K_4-minor-free graphs and graphs with bounded maximum average degree. We also consider connections with Samal's work on fractional covering by cuts: we address, and decide, two conjectures concerning cubical chromatic numbers.\",\n",
       "  'Title: the θ 5 graph is a spanner\\nAbstract: Given a set of points in the plane, we show that the ?-graph with 5 cones is a geometric spanner with spanning ratio at most 50 + 22 5 ? 9.960 . This is the first constant upper bound on the spanning ratio of this graph. The upper bound uses a constructive argument that gives a (possibly self-intersecting) path between any two vertices, of length at most 50 + 22 5 times the Euclidean distance between the vertices. We also give a lower bound on the spanning ratio of 1 2 ( 11 5 - 17 ) ? 3.798 .',\n",
       "  'Title: labelrank a stabilized label propagation algorithm for community detection in networks\\nAbstract: An important challenge in big data analysis nowadays is detection of cohesive groups in large-scale networks, including social networks, genetic networks, communication networks and so. In this paper, we propose LabelRank, an efficient algorithm detecting communities through label propagation. A set of operators is introduced to control and stabilize the propagation dynamics. These operations resolve the randomness issue in traditional label propagation algorithms (LPA), stabilizing the discovered communities in all runs of the same network. Tests on real-world networks demonstrate that LabelRank significantly improves the quality of detected communities compared to LPA, as well as other popular algorithms.',\n",
       "  'Title: topical interests and the mitigation of search engine bias\\nAbstract: Search engines have become key media for our scientific, economic, and social activities by enabling people to access information on the web despite its size and complexity. On the down side, search engines bias the traffic of users according to their page ranking strategies, and it has been argued that they create a vicious cycle that amplifies the dominance of established and already popular sites. This bias could lead to a dangerous monopoly of information. We show that, contrary to intuition, empirical data do not support this conclusion; popular sites receive far less traffic than predicted. We discuss a model that accurately predicts traffic data patterns by taking into consideration the topical interests of users and their searching behavior in addition to the way search engines rank pages. The heterogeneity of user interests explains the observed mitigation of search engines’ popularity bias.',\n",
       "  'Title: parallel interleaver design for a high throughput hspa lte multi standard turbo decoder\\nAbstract: To meet the evolving data rate requirements of emerging wireless communication technologies, many parallel architectures have been proposed to implement high throughput turbo decoders. However, concurrent memory reading/writing in parallel turbo decoding architectures leads to severe memory conflict problem, which has become a major bottleneck for high throughput turbo decoders. In this paper, we propose a flexible and efficient VLSI architecture to solve the memory conflict problem for highly parallel turbo decoders targeting multi-standard 3G/4G wireless communication systems. To demonstrate the effectiveness of the proposed parallel interleaver architecture, we implemented an HSPA +/LTE/LTE-Advanced multi-standard turbo decoder with a 45 nm CMOS technology. The implemented turbo decoder consists of 16 Radix-4 MAP decoder cores, and the chip core area is 2.43 mm 2. When clocked at 600 MHz, this turbo decoder can achieve a maximum decoding throughput of 826 Mbps in the HSPA+ mode and 1.67 Gbps in the LTE/LTE-Advanced mode, exceeding the peak data rate requirements for both standards.',\n",
       "  'Title: android malware detection using parallel machine learning classifiers\\nAbstract: Mobile malware has continued to grow at an alarming rate despite on-going mitigation efforts. This has been much more prevalent on Android due to being an open platform that is rapidly overtaking other competing platforms in the mobile smart devices market. Recently, a new generation of Android malware families has emerged with advanced evasion capabilities which make them much more difficult to detect using conventional methods. This paper proposes and investigates a parallel machine learning based classification approach for early detection of Android malware. Using real malware samples and benign applications, a composite classification model is developed from parallel combination of heterogeneous classifiers. The empirical evaluation of the model under different combination schemes demonstrates its efficacy and potential to improve detection accuracy. More importantly, by utilizing several classifiers with diverse characteristics, their strengths can be harnessed not only for enhanced Android malware detection but also quicker white box analysis by means of the more interpretable constituent classifiers.',\n",
       "  'Title: multi view constrained clustering with an incomplete mapping between views\\nAbstract: Multi-view learning algorithms typically assume a complete bipartite mapping between the different views in order to exchange information during the learning process. However, many applications provide only a partial mapping between the views, creating a challenge for current methods. To address this problem, we propose a multi-view algorithm based on constrained clustering that can operate with an incomplete mapping. Given a set of pairwise constraints in each view, our approach propagates these constraints using a local similarity measure to those instances that can be mapped to the other views, allowing the propagated constraints to be transferred across views via the partial mapping. It uses co-EM to iteratively estimate the propagation within each view based on the current clustering model, transfer the constraints across views, and then update the clustering model. By alternating the learning process between views, this approach produces a unified clustering model that is consistent with all views. We show that this approach significantly improves clustering performance over several other methods for transferring constraints and allows multi-view clustering to be reliably applied when given a limited mapping between the views. Our evaluation reveals that the propagated constraints have high precision with respect to the true clusters in the data, explaining their benefit to clustering performance in both single- and multi-view learning scenarios.',\n",
       "  \"Title: assembling thefacebook using heterogeneity to understand online social network assembly\\nAbstract: Online social networks represent a popular and diverse class of social media systems. Despite this variety, each of these systems undergoes a general process of online social network assembly, which represents the complicated and heterogeneous changes that transform newly born systems into mature platforms. However, little is known about this process. For example, how much of a network's assembly is driven by simple growth? How does a network's structure change as it matures? How does network structure vary with adoption rates and user heterogeneity, and do these properties play different roles at different points in the assembly? We investigate these and other questions using a unique dataset of online connections among the roughly one million users at the first 100 colleges admitted to Facebook, captured just 20 months after its launch. We first show that different vintages and adoption rates across this population of networks reveal temporal dynamics of the assembly process, and that assembly is only loosely related to network growth. We then exploit natural experiments embedded in this dataset and complementary data obtained via Internet archaeology to show that different subnetworks matured at different rates toward similar end states. These results shed light on the processes and patterns of online social network assembly, and may facilitate more effective design for online social systems.\",\n",
       "  \"Title: diffusing private data over networks\\nAbstract: The emergence of social and technological networks has enabled rapid sharing of data and information. This has resulted in significant privacy concerns where private information can be either leaked or inferred from public data. The problem is significantly harder for social networks where we may reveal more information to our friends than to strangers. Nonetheless, our private information can still leak to strangers as our friends are their friends and so on. In order to address this important challenge, in this paper, we present a privacy-preserving mechanism that enables private data to be diffused over a network. In particular, whenever a user wants to access another users' data, the proposed mechanism returns a differentially private response that ensures that the amount of private data leaked depends on the distance between the two users in the network. While allowing global statistics to be inferred by users acting as analysts, our mechanism guarantees that no individual user, or a group of users, can harm the privacy guarantees of any other user. We illustrate our mechanism with two examples: one on synthetic data where the users share their GPS coordinates; and one on a Facebook ego-network where a user shares her infection status.\",\n",
       "  'Title: finger based technique fbt an innovative system for improved usability for the blind users dynamic interaction with mobile touch screen devices\\nAbstract: This paper presents Finger Based Technique (FBT) prototypes, a novel interaction system for blind users, which is especially designed and developed for non-visual touch screen devices and their applications. The FBT prototypes were developed with virtual keys to be identified based on finger holding positions. Two different models namely the single digit FBT and double digit FBT were propounded. FBT technique were applied using two different phone dialer applications: a single digit virtual key for the single digit FBT model and a double digit virtual key with audio feedback enabling touch as input gesture for the later one. An evaluation with 7 blind participants showed that single digit FBT was significantly faster and more accurate than double digit FBT. In addition to that, single digit FBT was found to be much faster than iPhone VoiceOver entry speeds in performing similar tasks. Furthermore, our research also suggests 11 accessible regions for quick access or navigation in flat touch screen based smart phones for blind users. These accessible regions will serve as a usability design framework and facilitate the developers to place the widget for the blind user for dynamic interaction with the touch screen devices. As far as is known to the authors, this is a novel suggestion.',\n",
       "  'Title: fast algorithms for game theoretic centrality measures\\nAbstract: In this dissertation, we analyze the computational properties of game-theoretic centrality measures. The key idea behind game-theoretic approach to network analysis is to treat nodes as players in a cooperative game, where the value of each coalition of nodes is determined by certain graph properties. Next, the centrality of any individual node is determined by a chosen game-theoretic solution concept (notably, the Shapley value) in the same way as the payoff of a player in a cooperative game. On one hand, the advantage of game-theoretic centrality measures is that nodes are ranked not only according to their individual roles but also according to how they contribute to the role played by all possible subsets of nodes. On the other hand, the disadvantage is that the game-theoretic solution concepts are typically computationally challenging. The main contribution of this dissertation is that we show that a wide variety of game-theoretic solution concepts on networks can be computed in polynomial time. Our focus is on centralities based on the Shapley value and its various extensions, such as the Semivalues and Coalitional Semivalues. Furthermore, we prove #P-hardness of computing the Shapley value in connectivity games and propose an algorithm to compute it. Finally, we analyse computational properties of generalized version of cooperative games in which order of player matters. We propose a new representation for such games, called generalized marginal contribution networks, that allows for polynomial computation in the size of the representation of two dedicated extensions of the Shapley value to this class of games.',\n",
       "  'Title: directional global three part image decomposition\\nAbstract: We consider the task of image decomposition, and we introduce a new model coined directional global three-part decomposition (DG3PD) for solving it. As key ingredients of the DG3PD model, we introduce a discrete multi-directional total variation norm and a discrete multi-directional G-norm. Using these novel norms, the proposed discrete DG3PD model can decompose an image into two or three parts. Existing models for image decomposition by Vese and Osher (J. Sci. Comput. 19(1–3):553–572, 2003), by Aujol and Chambolle (Int. J. Comput. Vis. 63(1):85–104, 2005), by Starck et al. (IEEE Trans. Image Process. 14(10):1570–1582, 2005), and by Thai and Gottschlich are included as special cases in the new model. Decomposition of an image by DG3PD results in a cartoon image, a texture image, and a residual image. Advantages of the DG3PD model over existing ones lie in the properties enforced on the cartoon and texture images. The geometric objects in the cartoon image have a very smooth surface and sharp edges. The texture image yields oscillating patterns on a defined scale which are both smooth and sparse. Moreover, the DG3PD method achieves the goal of perfect reconstruction by summation of all components better than the other considered methods. Relevant applications of DG3PD are a novel way of image compression as well as feature extraction for applications such as latent fingerprint processing and optical character recognition.',\n",
       "  'Title: green cognitive relaying opportunistically switching between data transmission and energy harvesting\\nAbstract: Energy efficiency has become an encouragement, and more than this, a requisite for the design of next-generation wireless communications standards. In current work, a dual-hop cognitive (secondary) relaying system is considered, incorporating multiple amplify-and-forward relays, a rather cost-effective solution. First, the secondary relays sense the wireless channel, scanning for a primary network activity, and then convey their reports to a secondary base station (SBS). Afterwards, the SBS, based on these reports and its own estimation, decides cooperatively the presence of primary transmission or not. In the former scenario, all the secondary nodes start to harvest energy from the transmission of primary node(s). In the latter scenario, the system initiates secondary communication via a best relay selection policy. Performance evaluation of this system is thoroughly investigated, by assuming realistic channel conditions, i.e., non-identical link-distances, Rayleigh fading, and outdated channel estimation. The detection and outage probabilities as well as the average harvested energy are derived as new closed-form expressions. In addition, an energy efficiency optimization problem is analytically formulated and solved, while a necessary condition in terms of power consumption minimization for each secondary node is presented. From a green communications standpoint, it turns out that energy harvesting greatly enhances the resources of secondary nodes, especially when primary activity is densely present.',\n",
       "  'Title: predicting the sentiment polarity and rating of yelp reviews\\nAbstract: Online reviews of businesses have become increasingly important in recent years, as customers and even competitors use them to judge the quality of a business. Yelp is one of the most popular websites for users to write such reviews, and it would be useful for them to be able to predict the sentiment or even the star rating of a review. In this paper, we develop two classifiers to perform positive/negative classification and 5-star classification. We use Naive Bayes, Support Vector Machines, and Logistic Regression as models, and achieved the best accuracy with Logistic Regression: 92.90% for positive/negative classification, and 63.92% for 5-star classification. These results demonstrate the quality of the Logistic Regression model using only the text of the review, yet there is a promising opportunity for improvement with more data, more features, and perhaps different models.',\n",
       "  'Title: beauty and brains detecting anomalous pattern co occurrences\\nAbstract: Our world is filled with both beautiful and brainy people, but how often does a Nobel Prize winner also wins a beauty pageant? Let us assume that someone who is both very beautiful and very smart is more rare than what we would expect from the combination of the number of beautiful and brainy people. Of course there will still always be some individuals that defy this stereotype; these beautiful brainy people are exactly the class of anomaly we focus on in this paper. They do not posses intrinsically rare qualities, it is the unexpected combination of factors that makes them stand out. #R##N#In this paper we define the above described class of anomaly and propose a method to quickly identify them in transaction data. Further, as we take a pattern set based approach, our method readily explains why a transaction is anomalous. The effectiveness of our method is thoroughly verified with a wide range of experiments on both real world and synthetic data.',\n",
       "  'Title: towards a direct by need evaluator for dependently typed languages\\nAbstract: We present a C-language implementation of the lambda-pi calculus by extending the (call-by-need) stack machine of Ariola, Chang and Felleisen to hold types, using a typeless- tagless- final interpreter strategy. It has the advantage of expressing all operations as folds over terms, including by-need evaluation, recovery of the initial syntax-tree encoding for any term, and eliminating most garbage-collection tasks. These are made possible by a disciplined approach to handling the spine of each term, along with a robust stack-based API. Type inference is not covered in this work, but also derives several advantages from the present stack transformation. Timing and maximum stack space usage results for executing benchmark problems are presented. We discuss how the design choices for this interpreter allow the language to be used as a high-level scripting language for automatic distributed parallel execution of common scientific computing workflows.',\n",
       "  'Title: adm cle approach for detecting slow variables in continuous time markov chains and dynamic data\\nAbstract: A method for detecting intrinsic slow variables in high-dimensional stochastic chemical reaction networks is developed and analyzed. It combines anisotropic diffusion maps (ADM) with approximations based on the chemical Langevin equation (CLE). The resulting approach, called ADM-CLE, has the potential of being more efficient than the ADM method for a large class of chemical reaction systems, because it replaces the computationally most expensive step of ADM (running local short bursts of simulations) by using an approximation based on the CLE. The ADM-CLE approach can be used to estimate the stationary distribution of the detected slow variable, without any a-priori knowledge of it. If the conditional distribution of the fast variables can be obtained analytically, then the resulting ADM-CLE approach does not make any use of Monte Carlo simulations to estimate the distributions of both slow and fast variables.',\n",
       "  'Title: real time sign language fingerspelling recognition using convolutional neural networks from depth map\\nAbstract: Sign language recognition is important for natural and convenient communication between deaf community and hearing majority. We take the highly efficient initial step of automatic fingerspelling recognition system using convolutional neural networks (CNNs) from depth maps. In this work, we consider relatively larger number of classes compared with the previous literature. We train CNNs for the classification of 31 alphabets and numbers using a subset of collected depth data from multiple subjects. While using different learning configurations, such as hyper-parameter selection with and without validation, we achieve 99.99% accuracy for observed signers and 83.58% to 85.49% accuracy for new signers. The result shows that accuracy improves as we include more data from different subjects during training. The processing time is 3 ms for the prediction of a single image. To the best of our knowledge, the system achieves the highest accuracy and speed. The trained model and dataset is available on our repository.',\n",
       "  'Title: a game theoretic perspective on communication for omniscience\\nAbstract: We propose a coalition game model for the problem of communication for omniscience (CO). In this game model, the core contains all achievable rate vectors for CO with sum-rate being equal to a given value. Any rate vector in the core distributes the sum-rate among users in a way that makes all users willing to cooperate in CO. We give the necessary and sufficient condition for the core to be nonempty. Based on this condition, we derive the expression of the minimum sum-rate for CO and show that this expression is consistent with the results in multivariate mutual information (MMI) and coded cooperative data exchange (CCDE). We prove that the coalition game model is convex if the sum-rate is no less than the minimal value. In this case, the core is non-empty and a rate vector in the core that allocates the sum-rate among the users in a fair manner can be found by calculating the Shapley value.',\n",
       "  \"Title: improper gaussian signaling in full duplex relay channels with residual self interference\\nAbstract: We study the potential employment of improper Gaussian signaling (IGS) in full-duplex cooperative settings with residual self-interference (RSI). IGS is recently shown to outperform traditional proper Gaussian signaling (PGS) in several interference-limited channel settings. In this work, IGS is employed in an attempt to alleviate the RSI adverse effect in full-duplex relaying (FDR). To this end, we derive a tight upper bound expression for the end-to-end outage probability in terms of the relay signal parameters represented in its power and circularity coefficient. We further show that the derived upper bound is either monotonic or unimodal in the relay's circularity coefficient. This result allows for easily locating the global optimal point using known numerical methods. Based on the analysis, IGS allows FDR systems to operate even with high RSI. It is shown that, while the communication totally fails with PGS as the RSI increases, the IGS outage probability approaches a fixed value that depends on the channel statistics and target rate. The obtained results show that IGS can leverage higher relay power budgets than PGS to improve the performance, meanwhile it relieves its RSI impact via tuning the signal impropriety.\",\n",
       "  \"Title: an action language for multi agent domains foundations\\nAbstract: In multi-agent domains (MADs), an agent's action may not just change the world and the agent's knowledge and beliefs about the world, but also may change other agents' knowledge and beliefs about the world and their knowledge and beliefs about other agents' knowledge and beliefs about the world. The goals of an agent in a multi-agent world may involve manipulating the knowledge and beliefs of other agents' and again, not just their knowledge/belief about the world, but also their knowledge about other agents' knowledge about the world. Our goal is to present an action language (mA+) that has the necessary features to address the above aspects in representing and RAC in MADs. mA+ allows the representation of and reasoning about different types of actions that an agent can perform in a domain where many other agents might be present---such as world-altering actions, sensing actions, and announcement/communication actions. It also allows the specification of agents' dynamic awareness of action occurrences which has future implications on what agents' know about the world and other agents' knowledge about the world. mA+ considers three different types of awareness: full,- partial- awareness, and complete oblivion of an action occurrence and its effects. This keeps the language simple, yet powerful enough to address a large variety of knowledge manipulation scenarios in MADs. The semantics of mA+ relies on the notion of state, which is described by a pointed Kripke model and is used to encode the agent's knowledge and the real state of the world. It is defined by a transition function that maps pairs of actions and states into sets of states. We illustrate properties of the action theories, including properties that guarantee finiteness of the set of initial states and their practical implementability. Finally, we relate mA+ to other related formalisms that contribute to RAC in MADs.\",\n",
       "  'Title: on ultralimits of sparse graph classes\\nAbstract: The notion of nowhere denseness is one of the central concepts of the recently developed theory of sparse graphs. We study the properties of nowhere dense graph classes by investigating appropriate limit objects defined using the ultraproduct construction. It appears that different equivalent definitions of nowhere denseness, for example via quasi-wideness or the splitter game, correspond to natural notions for the limit objects that are conceptually simpler and allow for less technically involved reasonings.',\n",
       "  'Title: political speech generation\\nAbstract: In this report we present a system that can generate political speeches for a desired political party. Furthermore, the system allows to specify whether a speech should hold a supportive or opposing opinion. The system relies on a combination of several state-of-the-art NLP methods which are discussed in this report. These include n-grams, Justeson & Katz POS tag filter, recurrent neural networks, and latent Dirichlet allocation. Sequences of words are generated based on probabilities obtained from two underlying models: A language model takes care of the grammatical correctness while a topic model aims for textual consistency. Both models were trained on the Convote dataset which contains transcripts from US congressional floor debates. Furthermore, we present a manual and an automated approach to evaluate the quality of generated speeches. In an experimental evaluation generated speeches have shown very high quality in terms of grammatical correctness and sentence transitions.',\n",
       "  'Title: a preliminary study on the learning informativeness of data subsets\\nAbstract: Estimating the internal state of a robotic system is complex: this is performed from multiple heterogeneous sensor inputs and knowledge sources. Discretization of such inputs is done to capture saliences, represented as symbolic information, which often presents structure and recurrence. As these sequences are used to reason over complex scenarios, a more compact representation would aid exactness of technical cognitive reasoning capabilities, which are today constrained by computational complexity issues and fallback to representational heuristics or human intervention. Such problems need to be addressed to ensure timely and meaningful human-robot interaction. Our work is towards understanding the variability of learning informativeness when training on subsets of a given input dataset. This is in view of reducing the training size while retaining the majority of the symbolic learning potential. We prove the concept on human-written texts, and conjecture this work will reduce training data size of sequential instructions, while preserving semantic relations, when gathering information from large remote sources.',\n",
       "  'Title: cutting recursive autoencoder trees\\nAbstract: Deep Learning models enjoy considerable success in Natural Language Processing. While deep architectures produce useful representations that lead to improvements in various tasks, they are often difficult to interpret. This makes the analysis of learned structures particularly difficult. In this paper, we rely on empirical tests to see whether a particular structure makes sense. We present an analysis of the Semi-Supervised Recursive Autoencoder, a well-known model that produces structural representations of text. We show that for certain tasks, the structure of the autoencoder can be significantly reduced without loss of classification accuracy and we evaluate the produced structures using human judgment.',\n",
       "  'Title: entropy and syntropy in the context of five valued logics\\nAbstract: This paper presents a five-valued representation of bifuzzy sets. This representation is related to a five-valued logic that uses the following values: true, false, inconsistent, incomplete and ambiguous. In the framework of five-valued representation, formulae for similarity, entropy and syntropy of bifuzzy sets are constructed.',\n",
       "  \"Title: exploring the influence of scale on artist attribution\\nAbstract: Previous work has shown that the artist of an artwork can be identified by use of computational methods that analyse digital images. However, the digitised artworks are often investigated at a coarse scale discarding many of the important details that may define an artist's style. In recent years high resolution images of artworks have become available, which, combined with increased processing power and new computational techniques, allow us to analyse digital images of artworks at a very fine scale. In this work we train and evaluate a Convolutional Neural Network (CNN) on the task of artist attribution using artwork images of varying resolutions. To this end, we combine two existing methods to enable the application of high resolution images to CNNs. By comparing the attribution performances obtained at different scales, we find that in most cases finer scales are beneficial to the attribution performance, whereas for a minority of the artists, coarser scales appear to be preferable. We conclude that artist attribution would benefit from a multi-scale CNN approach which vastly expands the possibilities for computational art forensics.\",\n",
       "  'Title: proof terms for infinitary rewriting progress report\\nAbstract: One of the main foundations for this work is the theory of countable ordinals;(citation needed) and (citation needed) are good references on this subject.We want to point out some deﬁnitions and results which are critical in orderto prove some of the basic properties of inﬁnitary proof terms.In order to deal with inﬁnitary composition, we will need to obtain the sum ofa sequence including ωordinals. Thus we will resort to the following deﬁnition,cfr. (citation needed).Deﬁnition 1.1 (Ordinal inﬁnitary sum). Let hα',\n",
       "  'Title: prediction of links and weights in networks by reliable routes\\nAbstract: Link prediction aims to uncover missing links or predict the emergence of future relationships from the current network structure. Plenty of algorithms have been developed for link prediction in unweighted networks, but only a few have been extended to weighted networks. In this paper, we present what we call a “reliable-route method” to extend unweighted local similarity indices to weighted ones. Using these indices, we can predict both the existence of links and their weights. Experiments on various real-world networks suggest that our reliable-route weighted resource-allocation index performs noticeably better than others with respect to weight prediction. For existence prediction it is either the highest or very close to the highest. Further analysis shows a strong positive correlation between the clustering coefficient and prediction accuracy. Finally, we apply our method to the prediction of missing protein-protein interactions and their confidence scores from known PPI networks. Once again, our reliable-route method shows the highest accuracy.',\n",
       "  'Title: efficient coding for multi source networks using g acs k orner common information\\nAbstract: Consider a multi-source network coding problem with correlated sources. While the fundamental limits are known, achieving them, in general, involves a computational burden due to the complex decoding process. Efficient solutions, on the other hand, are by large based on source and network coding separation, thus imposing strict topological constraints on the networks which can be solved. #R##N#In this work, we introduce a novel notion of separation of source and network coding using G\\\\\\'acs-K\\\\\"orner Common Information (CI). Unlike existing notions of separation, the sufficient condition for this separation to hold depends on the source structure rather than the network topology. Using the suggested separation scheme, we tackle three important multi-source problems. The first is the multi-source multicast. We construct efficient, zero error source codes, and via properties of the CI completely characterize the resulting rate region. The second is broadcast with side information. We establish a duality between this problem and the classical problem of degraded message set broadcast, and give two code constructions and their associated regions. Finally, we consider the Ahlswede-Korner problem in a network, and give an efficient solution which is tight under the CI constraints.',\n",
       "  'Title: map support detection for greedy sparse signal recovery algorithms in compressive sensing\\nAbstract: A reliable support detection is essential for a greedy algorithm to reconstruct a sparse signal accurately from compressed and noisy measurements. This paper proposes a novel support detection method for greedy algorithms, which is referred to as maximum a posteriori (MAP) support detection. Unlike existing support detection methods that identify support indices with the largest correlation value in magnitude per iteration, the proposed method selects them with the largest likelihood ratios computed under the true and null support hypotheses by simultaneously exploiting the distributions of a sensing matrix, a sparse signal, and noise. Leveraging this technique, MAP-Matching Pursuit (MAP-MP) is first presented to show the advantages of exploiting the proposed support detection method, and a sufficient condition for perfect signal recovery is derived for the case when the sparse signal is binary. Subsequently, a set of iterative greedy algorithms, called MAP-generalized Orthogonal Matching Pursuit (MAP-gOMP), MAP-Compressive Sampling Matching Pursuit (MAP-CoSaMP), and MAP-Subspace Pursuit (MAP-SP) are presented to demonstrate the applicability of the proposed support detection method to existing greedy algorithms. From empirical results, it is shown that the proposed greedy algorithms with highly reliable support detection can be better, faster, and easier to implement than basis pursuit via linear programming.',\n",
       "  'Title: integrity verification for outsourcing uncertain frequent itemset mining\\nAbstract: In recent years, due to the wide applications of uncertain data (e.g., noisy data), uncertain frequent itemsets (UFI) mining over uncertain databases has attracted much attention, which differs from the corresponding deterministic problem from the generalized definition and resolutions. As the most costly task in association rule mining process, it has been shown that outsourcing this task to a service provider (e.g.,the third cloud party) brings several benefits to the data owner such as cost relief and a less commitment to storage and computational resources. However, the correctness integrity of mining results can be corrupted if the service provider is with random fault or not honest (e.g., lazy, malicious, etc). Therefore, in this paper, we focus on the integrity and verification issue in UFI mining problem during outsourcing process, i.e., how the data owner verifies the mining results. Specifically, we explore and extend the existing work on deterministic FI outsourcing verification to uncertain scenario. For this purpose, We extend the existing outsourcing FI mining work to uncertain area w.r.t. the two popular UFI definition criteria and the approximate UFI mining methods. Specifically, We construct and improve the basic/enhanced verification scheme with such different UFI definition respectively. After that, we further discuss the scenario of existing approximation UFP mining, where we can see that our technique can provide good probabilistic guarantees about the correctness of the verification. Finally, we present the comparisons and analysis on the schemes proposed in this paper.',\n",
       "  'Title: optimal staged self assembly of general shapes\\nAbstract: We analyze the number of tile types $t$, bins $b$, and stages necessary to assemble $n \\\\times n$ squares and scaled shapes in the staged tile assembly model. For $n \\\\times n$ squares, we prove $\\\\mathcal{O}(\\\\frac{\\\\log{n} - tb - t\\\\log t}{b^2} + \\\\frac{\\\\log \\\\log b}{\\\\log t})$ stages suffice and $\\\\Omega(\\\\frac{\\\\log{n} - tb - t\\\\log t}{b^2})$ are necessary for almost all $n$. For shapes $S$ with Kolmogorov complexity $K(S)$, we prove $\\\\mathcal{O}(\\\\frac{K(S) - tb - t\\\\log t}{b^2} + \\\\frac{\\\\log \\\\log b}{\\\\log t})$ stages suffice and $\\\\Omega(\\\\frac{K(S) - tb - t\\\\log t}{b^2})$ are necessary to assemble a scaled version of $S$, for almost all $S$. We obtain similarly tight bounds when the more powerful flexible glues are permitted.',\n",
       "  'Title: an encoding of array verification problems into array free horn clauses\\nAbstract: Automatically verifying safety properties of programs is hard, and it is even harder if the program acts upon arrays or other forms of maps. Many approaches exist for verifying programs operating upon Boolean and integer values (e.g. abstract interpretation, counterexample-guided abstraction refinement using interpolants), but transposing them to array properties has been fraught with difficulties.#R##N##R##N#In contrast to most preceding approaches, we do not introduce a new abstract domain or a new interpolation procedure for arrays. Instead, we generate an abstraction as a scalar problem and feed it to a preexisting solver, with tunable precision.#R##N##R##N#Our transformed problem is expressed using Horn clauses, a common format with clear and unambiguous logical semantics for verification problems. An important characteristic of our encoding is that it creates a nonlinear Horn problem, with tree unfoldings, even though following “flatly” the control-graph structure ordinarily yields a linear Horn problem, with linear unfoldings. That is, our encoding cannot be expressed by an encoding into another control-flow graph problem, and truly leverages the capacity of the Horn clause format.#R##N##R##N#We illustrate our approach with a completely automated proof of the functional correctness of selection sort.',\n",
       "  'Title: supervised dimensionality reduction via distance correlation maximization\\nAbstract: In our work, we propose a novel formulation for supervised dimensionality reduction based on a nonlinear dependency criterion called Statistical Distance Correlation, Szekely et. al. (2007). We propose an objective which is free of distributional assumptions on regression variables and regression model assumptions. Our proposed formulation is based on learning a low-dimensional feature representation $\\\\mathbf{z}$, which maximizes the squared sum of Distance Correlations between low dimensional features $\\\\mathbf{z}$ and response $y$, and also between features $\\\\mathbf{z}$ and covariates $\\\\mathbf{x}$. We propose a novel algorithm to optimize our proposed objective using the Generalized Minimization Maximizaiton method of \\\\Parizi et. al. (2015). We show superior empirical results on multiple datasets proving the effectiveness of our proposed approach over several relevant state-of-the-art supervised dimensionality reduction methods.',\n",
       "  'Title: daleel simplifying cloud instance selection using machine learning\\nAbstract: Decision making in cloud environments is quite challenging due to the diversity in service offerings and pricing models, especially considering that the cloud market is an incredibly fast moving one. In addition, there are no hard and fast rules; each customer has a specific set of constraints (e.g. budget) and application requirements (e.g. minimum computational resources). Machine learning can help address some of the complicated decisions by carrying out customer-specific analytics to determine the most suitable instance type(s) and the most opportune time for starting or migrating instances. We employ machine learning techniques to develop an adaptive deployment policy, providing an optimal match between the customer demands and the available cloud service offerings. We provide an experimental study based on extensive set of job executions over a major public cloud infrastructure.',\n",
       "  'Title: a truthful mechanism with biparameter learning for online crowdsourcing\\nAbstract: We study a problem of allocating divisible jobs, arriving online, to workers in a crowdsourcing setting which involves learning two parameters of strategically behaving workers. Each job is split into a certain number of tasks that are then allocated to workers. Each arriving job has to be completed within a deadline and each task has to be completed satisfying an upper bound on probability of failure. The job population is homogeneous while the workers are heterogeneous in terms of costs, completion times, and times to failure. The job completion time and time to failure of each worker are stochastic with fixed but unknown means. The requester is faced with the challenge of learning two separate parameters of each (strategically behaving) worker simultaneously, namely, the mean job completion time and the mean time to failure. The time to failure of a worker depends on the duration of the task handled by the worker. Assuming non-strategic workers to start with, we solve this biparameter learning problem by applying the Robust UCB algorithm. Then, we non-trivially extend this algorithm to the setting where the workers are strategic about their costs. Our proposed mechanism is dominant strategy incentive compatible and ex-post individually rational with asymptotically optimal regret performance.',\n",
       "  'Title: high performance python for direct numerical simulations of turbulent flows\\nAbstract: Abstract   Direct Numerical Simulations (DNS) of the Navier Stokes equations is an invaluable research tool in fluid dynamics. Still, there are few publicly available research codes and, due to the heavy number crunching implied, available codes are usually written in low-level languages such as C/C++ or Fortran. In this paper we describe a pure scientific Python pseudo-spectral DNS code that nearly matches the performance of C++ for thousands of processors and billions of unknowns. We also describe a version optimized through Cython, that is found to match the speed of C++. The solvers are written from scratch in Python, both the mesh, the MPI domain decomposition, and the temporal integrators. The solvers have been verified and benchmarked on the Shaheen supercomputer at the KAUST supercomputing laboratory, and we are able to show very good scaling up to several thousand cores.  A very important part of the implementation is the mesh decomposition (we implement both slab and pencil decompositions) and 3D parallel Fast Fourier Transforms (FFT). The mesh decomposition and FFT routines have been implemented in Python using serial FFT routines (either NumPy, pyFFTW or any other serial FFT module), NumPy array manipulations and with MPI communications handled by MPI for Python ( mpi4py ). We show how we are able to execute a 3D parallel FFT in Python for a slab mesh decomposition using 4 lines of compact Python code, for which the parallel performance on Shaheen is found to be slightly better than similar routines provided through the FFTW library. For a pencil mesh decomposition 7 lines of code is required to execute a transform.',\n",
       "  'Title: system power minimization to access non contiguous spectrum in cognitive radio networks\\nAbstract: Wireless transmission using non-contiguous chunks of spectrum is becoming increasingly important due to a variety of scenarios such as: secondary users avoiding incumbent users in TV white space; anticipated spectrum sharing between commercial and military systems; and spectrum sharing among uncoordinated interferers in unlicensed bands. Multi-Channel Multi-Radio (MCMR) platforms and Non-Contiguous Orthogonal Frequency Division Multiple Access (NC-OFDMA) technology are the two commercially viable transmission choices to access these non-contiguous spectrum chunks. Fixed MC-MRs do not scale with increasing number of non-contiguous spectrum chunks due to their fixed set of supporting radio front ends. NC-OFDMA allows nodes to access these non-contiguous spectrum chunks and put null sub-carriers in the remaining chunks. However, nulling sub-carriers increases the sampling rate (spectrum span) which, in turn, increases the power consumption of radio front ends. Our work characterizes this trade-off from a cross-layer perspective, specifically by showing how the slope of ADC/DAC power consumption versus sampling rate curve influences scheduling decisions in a multi-hop network. Specifically, we provide a branch and bound algorithm based mixed integer linear programming solution that performs joint power control, spectrum span selection, scheduling and routing in order to minimize the system power of multi-hop NC-OFDMA networks. We also provide a low complexity (O(E^2 M^2)) greedy algorithm where M and E denote the number of channels and links respectively. Numerical simulations suggest that our approach reduces system power by 30% over classical transmit power minimization based cross-layer algorithms.',\n",
       "  'Title: improved bounds for shortest paths in dense distance graphs\\nAbstract: We study the problem of computing shortest paths in so-called dense distance graphs. Every planar graph $G$ on $n$ vertices can be partitioned into a set of $O(n/r)$ edge-disjoint regions (called an $r$-division) with $O(r)$ vertices each, such that each region has $O(\\\\sqrt{r})$ vertices (called boundary vertices) in common with other regions. A dense distance graph of a region is a complete graph containing all-pairs distances between its boundary nodes. A dense distance graph of an $r$-division is the union of the $O(n/r)$ dense distance graphs of the individual pieces. Since the introduction of dense distance graphs by Fakcharoenphol and Rao, computing single-source shortest paths in dense distance graphs has found numerous applications in fundamental planar graph algorithms. #R##N#Fakcharoenphol and Rao proposed an algorithm (later called FR-Dijkstra) for computing single-source shortest paths in a dense distance graph in $O\\\\left(\\\\frac{n}{\\\\sqrt{r}}\\\\log{n}\\\\log{r}\\\\right)$ time. We show an $O\\\\left(\\\\frac{n}{\\\\sqrt{r}}\\\\left(\\\\frac{\\\\log^2{r}}{\\\\log^2\\\\log{r}}+\\\\log{n}\\\\log^{\\\\epsilon}{r}\\\\right)\\\\right)$ time algorithm for this problem, which is the first improvement to date over FR-Dijkstra for the important case when $r$ is polynomial in $n$. In this case, our algorithm is faster by a factor of $O(\\\\log^2{\\\\log{n}})$ and implies improved upper bounds for such planar graph problems as multiple-source multiple-sink maximum flow, single-source all-sinks maximum flow, and (dynamic) exact distance oracles.',\n",
       "  'Title: sketching for sequential change point detection\\nAbstract: We study sequential change-point detection procedures based on linear sketches of high-dimensional signal vectors using generalized likelihood ratio (GLR) statistics. The GLR statistics allow for an unknown post-change mean that represents an anomaly or novelty. We consider both fixed and time-varying projections, derive theoretical approximations to two fundamental performance metrics: the average run length (ARL) and the expected detection delay (EDD); these approximations are shown to be highly accurate by numerical simulations. We further characterize the relative performance measure of the sketching procedure compared to that without sketching and show that there can be little performance loss when the signal strength is sufficiently large, and enough number of sketches are used. Finally, we demonstrate the good performance of sketching procedures using simulation and real-data examples on solar flare detection and failure detection in power networks.',\n",
       "  'Title: emergence of consensus in a multi robot network from abstract models to empirical validation\\nAbstract: Consensus dynamics in decentralised multiagent systems are subject to intense studies, and several different models have been proposed and analysed. Among these, the naming game stands out for its simplicity and applicability to a wide range of phenomena and applications, from semiotics to engineering. Despite the wide range of studies available, the implementation of theoretical models in real distributed systems is not always straightforward, as the physical platform imposes several constraints that may have a bearing on the consensus dynamics. In this paper, we investigate the effects of an implementation of the naming game for the kilobot robotic platform, in which we consider concurrent execution of games and physical interferences. Consensus dynamics are analysed in the light of the continuously evolving communication network created by the robots, highlighting how the different regimes crucially depend on the robot density and on their ability to spread widely in the experimental arena. We find that physical interferences reduce the benefits resulting from robot mobility in terms of consensus time, but also result in lower cognitive load for individual agents.',\n",
       "  'Title: constructions of snake in the box codes under ell_ infty metric for rank modulation\\nAbstract: In the rank modulation scheme, Gray codes are very useful in the realization of flash memories. For a Gray code in this scheme, two adjacent codewords are obtained by using one \"push-to-the-top\" operation. Moreover, snake-in-the-box codes under the $\\\\ell_{\\\\infty}$-metric are Gray codes, which can be capable of detecting one $\\\\ell_{\\\\infty}$-error. In this paper, we give two constructions of $\\\\ell_{\\\\infty}$-snakes. On the one hand, inspired by Yehezkeally and Schwartz\\'s construction, we present a new construction of the $\\\\ell_{\\\\infty}$-snake. The length of this $\\\\ell_{\\\\infty}$-snake is longer than the length of the $\\\\ell_{\\\\infty}$-snake constructed by Yehezkeally and Schwartz. On the other hand, we also give another construction of $\\\\ell_{\\\\infty}$-snakes by using $\\\\mathcal{K}$-snakes and obtain the longer $\\\\ell_{\\\\infty}$-snakes than the previously known ones.',\n",
       "  'Title: an improved analysis of the er spud dictionary learning algorithm\\nAbstract: In \"dictionary learning\" we observe $Y = AX + E$ for some $Y\\\\in\\\\mathbb{R}^{n\\\\times p}$, $A \\\\in\\\\mathbb{R}^{m\\\\times n}$, and $X\\\\in\\\\mathbb{R}^{m\\\\times p}$. The matrix $Y$ is observed, and $A, X, E$ are unknown. Here $E$ is \"noise\" of small norm, and $X$ is column-wise sparse. The matrix $A$ is referred to as a {\\\\em dictionary}, and its columns as {\\\\em atoms}. Then, given some small number $p$ of samples, i.e.\\\\ columns of $Y$, the goal is to learn the dictionary $A$ up to small error, as well as $X$. The motivation is that in many applications data is expected to sparse when represented by atoms in the \"right\" dictionary $A$ (e.g.\\\\ images in the Haar wavelet basis), and the goal is to learn $A$ from the data to then use it for other applications. #R##N#Recently, [SWW12] proposed the dictionary learning algorithm ER-SpUD with provable guarantees when $E = 0$ and $m = n$. They showed if $X$ has independent entries with an expected $s$ non-zeroes per column for $1 \\\\lesssim s \\\\lesssim \\\\sqrt{n}$, and with non-zero entries being subgaussian, then for $p\\\\gtrsim n^2\\\\log^2 n$ with high probability ER-SpUD outputs matrices $A\\', X\\'$ which equal $A, X$ up to permuting and scaling columns (resp.\\\\ rows) of $A$ (resp.\\\\ $X$). They conjectured $p\\\\gtrsim n\\\\log n$ suffices, which they showed was information theoretically necessary for {\\\\em any} algorithm to succeed when $s \\\\simeq 1$. Significant progress was later obtained in [LV15]. #R##N#We show that for a slight variant of ER-SpUD, $p\\\\gtrsim n\\\\log(n/\\\\delta)$ samples suffice for successful recovery with probability $1-\\\\delta$. We also show that for the unmodified ER-SpUD, $p\\\\gtrsim n^{1.99}$ samples are required even to learn $A, X$ with polynomially small success probability. This resolves the main conjecture of [SWW12], and contradicts the main result of [LV15], which claimed that $p\\\\gtrsim n\\\\log^4 n$ guarantees success whp.',\n",
       "  \"Title: large peg army maneuvers\\nAbstract: Despite its long history, the classical game of peg solitaire continues to attract the attention of the scientific community. In this paper, we consider two problems with an algorithmic flavour which are related with this game, namely Solitaire-Reachability and Solitaire-Army. In the first one, we show that deciding whether there is a sequence of jumps which allows a given initial configuration of pegs to reach a target position is NP-complete. Regarding Solitaire-Army, the aim is to successfully deploy an army of pegs in a given region of the board in order to reach a target position. By solving an auxiliary problem with relaxed constraints, we are able to answer some open questions raised by Cs\\\\'ak\\\\'any and Juh\\\\'asz (Mathematics Magazine, 2000). To appreciate the combinatorial beauty of our solutions, we recommend to visit the gallery of animations provided at this http URL\",\n",
       "  'Title: scanning and parsing languages with ambiguities and constraints the lamb and fence algorithms\\nAbstract: Traditional language processing tools constrain language designers to specific kinds of grammars. In contrast, model-based language processing tools decouple language design from language processing. These tools allow the occurrence of lexical and syntactic ambiguities in language specifications and the declarative specification of constraints for resolving them. As a result, these techniques require scanners and parsers able to parse context-free grammars, handle ambiguities, and enforce constraints for disambiguation. In this paper, we present Lamb and Fence. Lamb is a scanning algorithm that supports ambiguous token definitions and the specification of custom pattern matchers and constraints. Fence is a chart parsing algorithm that supports ambiguous context-free grammars and the definition of constraints on associativity, composition, and precedence, as well as custom constraints. Lamb and Fence, in conjunction, enable the implementation of the ModelCC model-based language processing tool.',\n",
       "  'Title: overload control in sip networks a heuristic approach based on mathematical optimization\\nAbstract: The Session Initiation Protocol (SIP) is an application-layer control protocol for creating, modifying and terminating multimedia sessions. An open issue is the control of overload that occurs when a SIP server lacks sufficient CPU and memory resources to process all messages. We prove that the problem of overload control in SIP network with a set of n servers and limited resources is in the form of NP-hard. This paper proposes a Load-Balanced Call Admission Controller (LB-CAC), based on a heuristic mathematical model to determine an optimal resource allocation in such a way that maximizes call admission rates regarding the limited resources of the SIP servers. LB-CAC determines the optimal \"call admission rates\" and \"signaling paths\" for admitted calls along optimal allocation of CPU and memory resources of the SIP servers through a new linear programming model. This happens by acquiring some critical information of SIP servers. An assessment of the numerical and experimental results demonstrates the efficiency of the proposed method.',\n",
       "  'Title: nesting depth of operators in graph database queries expressiveness vs evaluation complexity\\nAbstract: Designing query languages for graph structured data is an active field of research, where expressiveness and efficient algorithms for query evaluation are conflicting goals. To better handle dynamically changing data, recent work has been done on designing query languages that can compare values stored in the graph database, without hard coding the values in the query. The main idea is to allow variables in the query and bind the variables to values when evaluating the query. For query languages that bind variables only once, query evaluation is usually NP-complete. There are query languages that allow binding inside the scope of Kleene star operators, which can themselves be in the scope of bindings and so on. Uncontrolled nesting of binding and iteration within one another results in query evaluation being PSPACE-complete. #R##N#We define a way to syntactically control the nesting depth of iterated bindings, and study how this affects expressiveness and efficiency of query evaluation. The result is an infinite, syntactically defined hierarchy of expressions. We prove that the corresponding language hierarchy is strict. Given an expression in the hierarchy, we prove that it is undecidable to check if there is a language equivalent expression at lower levels. We prove that evaluating a query based on an expression at level i can be done in $\\\\Sigma_i$ in the polynomial time hierarchy. Satisfiability of quantified Boolean formulas can be reduced to query evaluation; we study the relationship between alternations in Boolean quantifiers and the depth of nesting of iterated bindings.',\n",
       "  'Title: fetishizing food in digital age foodporn around the world\\nAbstract: What food is so good as to be considered pornographic? Worldwide, the popular #foodporn hashtag has been used to share appetizing pictures of peoples\\' favorite culinary experiences. But social scientists ask whether #foodporn promotes an unhealthy relationship with food, as pornography would contribute to an unrealistic view of sexuality. In this study, we examine nearly 10 million Instagram posts by 1.7 million users worldwide. An overwhelming (and uniform across the nations) obsession with chocolate and cake shows the domination of sugary dessert over local cuisines. Yet, we find encouraging traits in the association of emotion and health-related topics with #foodporn, suggesting food can serve as motivation for a healthy lifestyle. Social approval also favors the healthy posts, with users posting with healthy hashtags having an average of 1,000 more followers than those with unhealthy ones. Finally, we perform a demographic analysis which shows nation-wide trends of behavior, such as a strong relationship (r=0.51) between the GDP per capita and the attention to healthiness of their favorite food. Our results expose a new facet of food \"pornography\", revealing potential avenues for utilizing this precarious notion for promoting healthy lifestyles.',\n",
       "  'Title: satisfiability for two variable logic with two successor relations on finite linear orders\\nAbstract: We study the finitary satisfiability problem for first order logic with two variables and two binary relations, corresponding to the induced successor relations of two finite linear orders. We show that the problem is decidable in NEXPTIME.',\n",
       "  'Title: learning over long time lags\\nAbstract: The advantage of recurrent neural networks (RNNs) in learning dependencies between time-series data has distinguished RNNs from other deep learning models. Recently, many advances are proposed in this emerging field. However, there is a lack of comprehensive review on memory models in RNNs in the literature. This paper provides a fundamental review on RNNs and long short term memory (LSTM) model. Then, provides a surveys of recent advances in different memory enhancements and learning techniques for capturing long term dependencies in RNNs.',\n",
       "  'Title: adaptive delay tolerant dstbc in opportunistic relaying cooperative mimo systems\\nAbstract: An adaptive delay-tolerant distributed space-time coding (DSTC) scheme with feedback is proposed for two-hop cooperative multiple-input multiple-output (MIMO) networks using an amplify-and-forward strategy and opportunistic relaying algorithms. Maximum likelihood receivers and adjustable code matrices are considered subject to a power constraint. In the proposed delay-tolerant DSTC scheme, an adjustable code matrix is employed to transform the space-time coded matrices at the relay nodes. Stochastic gradient algorithms are developed with reduced computational complexity to estimate the parameters of the code matrix. Simulation results show that the proposed algorithms obtain significant performance gains and address the delay issue in cooperative MIMO systems as compared to existing DSTC schemes.',\n",
       "  'Title: pagerank pipeline benchmark proposal for a holistic system benchmark for big data platforms\\nAbstract: The rise of big data systems has created a need for benchmarks to measure and compare the capabilities of these systems. Big data benchmarks present unique scalability challenges. The supercomputing community has wrestled with these challenges for decades and developed methodologies for creating rigorous scalable benchmarks (e.g., HPC Challenge). The proposed PageRank pipeline benchmark employs supercomputing benchmarking methodologies to create a scalable benchmark that is reflective of many real-world big data processing systems. The PageRank pipeline benchmark builds on existing prior scalable benchmarks (Graph500, Sort, and PageRank) to create a holistic benchmark with multiple integrated kernels that can be run together or independently. Each kernel is well defined mathematically and can be implemented in any programming environment. The linear algebraic nature of PageRank makes it well suited to being implemented using the GraphBLAS standard. The computations are simple enough that performance predictions can be made based on simple computing hardware models. The surrounding kernels provide the context for each kernel that allows rigorous definition of both the input and the output for each kernel. Furthermore, since the proposed PageRank pipeline benchmark is scalable in both problem size and hardware, it can be used to measure and quantitatively compare a wide range of present day and future systems. Serial implementations in C++, Python, Python with Pandas, Matlab, Octave, and Julia have been implemented and their single threaded performance has been measured.',\n",
       "  'Title: networked constrained cyber physical systems subject to malicious attacks a resilient set theoretic control approach\\nAbstract: In this paper a novel set-theoretic control framework for Networked Constrained Cyber-Physical Systems is presented. By resorting to set-theoretic ideas and the physical watermarking concept, an anomaly detector module and a control remediation strategy are formally derived with the aim to contrast severe cyber attacks affecting the communication channels. The resulting scheme ensures Uniformly Ultimate Boundedness and constraints fulfillment regardless of any admissible attack scenario. Simulation results show the effectiveness of the proposed strategy both against Denial of Service and False Data Injection attacks.',\n",
       "  \"Title: distributed edge caching scheme considering the tradeoff between the diversity and redundancy of cached content\\nAbstract: Caching popular contents at the edge of cellular networks has been proposed to reduce the load, and hence the cost of backhaul links. It is significant to decide which files should be cached and where to cache them. In this paper, we propose a distributed caching scheme considering the tradeoff between the diversity and redundancy of base stations' cached contents. Whether it is better to cache the same or different contents in different base stations? To find out this, we formulate an optimal redundancy caching problem. Our goal is to minimize the total transmission cost of the network, including cost within the radio access network (RAN) and cost incurred by transmission to the core network via backhaul links. The optimal redundancy ratio under given system configuration is obtained with adapted particle swarm optimization (PSO) algorithm. We analyze the impact of important system parameters through Monte-Carlo simulation. Results show that the optimal redundancy ratio is mainly influenced by two parameters, which are the backhaul to RAN unit cost ratio and the steepness of file popularity distribution. The total cost can be reduced by up to 54% at given unit cost ratio of backhaul to RAN when the optimal redundancy ratio is selected. Under typical file request pattern, the reduction amount can be up to 57%.\",\n",
       "  'Title: the case for dynamic key distribution for pki based vanets\\nAbstract: Vehicular Ad hoc Networks (VANETs) are becoming a reality where secure communication is a prerequisite. Public key infrastructure (PKI) can be used to secure VANETs where an onboard tamper proof device (TPD) stores a number of encryption keys which are renewed upon visiting a certificate authority (CA). We previously proposed a dynamic key distribution protocol for PKI-based VANETs [1] to reduce the role of the TPD. A vehicle dynamically requests a key from its nearest road side unit. This request is propagated through network infrastructure to reach a CA cloud and a key is securely returned. A proposed key revocation mechanism reduced the number of messages needed for revocation through Certificate Revocation List (CRL) distribution. In this paper, performance evaluation and security of the proposed dynamic key distribution is investigated analytically and through network simulation. Furthermore, extensive analysis is performed to demonstrate how the proposed protocol can dynamically support efficient and cost-reduced key distribution. Analysis and performance evaluation results clearly make the case for dynamic key distribution for PKI-based VANETS.',\n",
       "  'Title: distributed technology sustained pervasive applications\\nAbstract: Technology-sustained pervasive games, contrary to technology-supported pervasive games, can be understood as computer games interfacing with the physical world. Pervasive games are known to make us ...',\n",
       "  'Title: more than a million ways to be pushed a high fidelity experimental dataset of planar pushing\\nAbstract: Pushing is a motion primitive useful to handle objects that are too large, too heavy, or too cluttered to be grasped. It is at the core of much of robotic manipulation, in particular when physical interaction is involved. It seems reasonable then to wish for robots to understand how pushed objects move. #R##N#In reality, however, robots often rely on approximations which yield models that are computable, but also restricted and inaccurate. Just how close are those models? How reasonable are the assumptions they are based on? To help answer these questions, and to get a better experimental understanding of pushing, we present a comprehensive and high-fidelity dataset of planar pushing experiments. The dataset contains timestamped poses of a circular pusher and a pushed object, as well as forces at the interaction.We vary the push interaction in 6 dimensions: surface material, shape of the pushed object, contact position, pushing direction, pushing speed, and pushing acceleration. An industrial robot automates the data capturing along precisely controlled position-velocity-acceleration trajectories of the pusher, which give dense samples of positions and forces of uniform quality. #R##N#We finish the paper by characterizing the variability of friction, and evaluating the most common assumptions and simplifications made by models of frictional pushing in robotics.',\n",
       "  'Title: on design mining coevolution and surrogate models\\nAbstract: Design mining is the use of computational intelligence techniques to iteratively search and model the attribute space of physical objects evaluated directly through rapid prototyping to meet given objectives. It enables the exploitation of novel materials and processes without formal models or complex simulation. In this article, we focus upon the coevolutionary nature of the design process when it is decomposed into concurrent sub-design-threads due to the overall complexity of the task. Using an abstract, tunable model of coevolution, we consider strategies to sample subthread designs for whole-system testing and how best to construct and use surrogate models within the coevolutionary scenario. Drawing on our findings, we then describe the effective design of an array of six heterogeneous vertical-axis wind turbines.',\n",
       "  \"Title: on the combinatorial complexity of approximating polytopes\\nAbstract: Approximating convex bodies succinctly by convex polytopes is a fundamental problem in discrete geometry. A convex body K of diameter $diam(K)$ is given in Euclidean d-dimensional space, where $d$ is a constant. Given an error parameter eps > 0, the objective is to determine a polytope of minimum combinatorial complexity whose Hausdorff distance from K is at most eps diam(K). By combinatorial complexity we mean the total number of faces of all dimensions of the polytope. A well-known result by Dudley implies that O(1/eps^{(d-1)/2}) facets suffice, and a dual result by Bronshteyn and Ivanov similarly bounds the number of vertices, but neither result bounds the total combinatorial complexity. We show that there exists an approximating polytope whose total combinatorial complexity is O-tilde(1/eps^{(d-1)/2}), where O-tilde conceals a polylogarithmic factor in 1/eps. This is an improvement upon the best known bound, which is roughly O(1/eps^{d-2}).#R##N##R##N#Our result is based on a novel combination of both new and old ideas. First, we employ Macbeath regions, a classical structure from the theory of convexity. The construction of our approximating polytope employs a new stratified placement of these regions. Second, in order to analyze the combinatorial complexity of the approximating polytope, we present a tight analysis of a width-based variant of Barany and Larman's economical cap covering, which may be of independent interest. Finally, we use a deterministic variation of the witness-collector technique (developed recently by Devillers et al.) in the context of our stratified construction.\",\n",
       "  'Title: dmath a scalable linear algebra and math library for heterogeneous gp gpu architectures\\nAbstract: A new scalable parallel math library, dMath, is presented in this paper that demonstrates leading scaling when using intranode, or internode, hybrid-parallelism for deep-learning. dMath provides easy-to-use distributed base primitives and a variety of domain-specific algorithms. These include matrix multiplication, convolutions, and others allowing for rapid development of highly scalable applications, including Deep Neural Networks (DNN), whereas previously one was restricted to libraries that provided effective primitives for only a single GPU, like Nvidia’s cublas & cudnn or DNN primitives from Nervana’s neon framework. Development of HPC software is difficult, labor-intensive work, requiring a unique skill set. dMath allows a wide range of developers to utilize parallel and distributed hardware easily. One contribution of this approach is that data is stored persistently on the GPU hardware, avoiding costly transfers between host and device. Advanced memory management techniques are utilized, including caching of transferred data and memory reuse through pooling. A key contribution of dMath is that it delivers performance, portability, and productivity to its specific domain of support. It enables algorithm and application programmers to quickly solve problems without managing the significant complexity associated with multi-level parallelism. dMath can use intranode GPU-Direct Remote Direct Memory Access (GDR), developed in collaboration with the OpenMPI and MVAPICH groups that has shown to decrease latency and increase bandwidth when compared to previous techniques. Efficient inter-GPU communication is crucial to achieving greater net performance and supporting effective use of the cost-effective, GPU-dense COTS architecture adopted. dMath’s caching approach addresses one of the key drawbacks of GPUs, which is to keep data sets cached and to avoid overheads of the CPU-GPU memory interface wherever possible. Keywords—GP-GPU, CUDA, MPI, deep learning, deep neural network, matrix-matrix multiplication, InfiniBand, scalability',\n",
       "  'Title: on multi point local decoding of reed muller codes\\nAbstract: Reed-Muller codes are among the most important classes of locally correctable codes. Currently local decoding of Reed-Muller codes is based on decoding on lines or quadratic curves to recover one single coordinate. To recover multiple coordinates simultaneously, the naive way is to repeat the local decoding for recovery of a single coordinate. This decoding algorithm might be more expensive, i.e., require higher query complexity. In this paper, we focus on Reed-Muller codes with evaluation polynomials of total degree d . σ √ q for some σ ∈ (0, 1). By introducing a local decoding of Reed-Muller codes via the concept of codex that has been used for arithmetic secret sharing [6, 7], we are able to locally recover arbitrarily large number k of coordinates simultaneously at the cost of querying O(k √ q) coordinates, where q is the code alphabet size. It turns out that our local decoding of ReedMuller codes shows (perhaps surprisingly) that accessing k locations is in fact cheaper than repeating the procedure for accessing a single location for k times. In contrast, by repetition of local decoding for recovery of a single coordinate, one has to query Ω(k √ q log k/ log q) coordinates for k = q √ q) (and query O(kq) coordinates for k = q √ , respectively). Furthermore, our decoding success probability is 1 − ǫ with ǫ = O (',\n",
       "  'Title: faster r cnn features for instance search\\nAbstract: Image representations derived from pre-trained Convolutional Neural Networks (CNNs) have become the new state of the art in computer vision tasks such as instance retrieval. This work explores the suitability for instance retrieval of image- and region-wise representations pooled from an object detection CNN such as Faster R-CNN. We take advantage of the object proposals learned by a Region Proposal Network (RPN) and their associated CNN features to build an instance search pipeline composed of a first filtering stage followed by a spatial reranking. We further investigate the suitability of Faster R-CNN features when the network is fine-tuned for the same objects one wants to retrieve. We assess the performance of our proposed system with the Oxford Buildings 5k, Paris Buildings 6k and a subset of TRECVid Instance Search 2013, achieving competitive results.',\n",
       "  'Title: hirl hierarchical inverse reinforcement learning for long horizon tasks with delayed rewards\\nAbstract: Reinforcement Learning (RL) struggles in problems with delayed rewards, and one approach is to segment the task into sub-tasks with incremental rewards. We propose a framework called Hierarchical Inverse Reinforcement Learning (HIRL), which is a model for learning sub-task structure from demonstrations. HIRL decomposes the task into sub-tasks based on transitions that are consistent across demonstrations. These transitions are defined as changes in local linearity w.r.t to a kernel function. Then, HIRL uses the inferred structure to learn reward functions local to the sub-tasks but also handle any global dependencies such as sequentiality. #R##N#We have evaluated HIRL on several standard RL benchmarks: Parallel Parking with noisy dynamics, Two-Link Pendulum, 2D Noisy Motion Planning, and a Pinball environment. In the parallel parking task, we find that rewards constructed with HIRL converge to a policy with an 80% success rate in 32% fewer time-steps than those constructed with Maximum Entropy Inverse RL (MaxEnt IRL), and with partial state observation, the policies learned with IRL fail to achieve this accuracy while HIRL still converges. We further find that that the rewards learned with HIRL are robust to environment noise where they can tolerate 1 stdev. of random perturbation in the poses in the environment obstacles while maintaining roughly the same convergence rate. We find that HIRL rewards can converge up-to 6x faster than rewards constructed with IRL.',\n",
       "  'Title: kernelized covariance for action recognition\\nAbstract: In this paper we aim at increasing the descriptive power of the covariance matrix, limited in capturing linear mutual dependencies between variables only. We present a rigorous and principled mathematical pipeline to recover the kernel trick for computing the covariance matrix, enhancing it to model more complex, non-linear relationships conveyed by the raw data. To this end, we propose Kernelized-COV, which generalizes the original covariance representation without compromising the efficiency of the computation. In the experiments, we validate the proposed framework against many previous approaches in the literature, scoring on par or superior with respect to the state of the art on benchmark datasets for 3D action recognition.',\n",
       "  'Title: on the gi completeness of a sorting networks isomorphism\\nAbstract: The subitemset isomorphism problem is really important and there are excellent practical solutions described in the literature. However, the computational complexity analysis and classification of the BZ (Bundala and Zavodny) subitemset isomorphism problem is currently an open problem. In this paper we prove that checking whether two sorting networks are BZ isomorphic to each other is GI-Complete; the general GI (Graph Isomorphism) problem is known to be in NP and LWPP, but widely believed to be neither P nor NP-Complete; recent research suggests that the problem is in QP. Moreover, we state the BZ sorting network isomorphism problem as a general isomorphism problem on itemsets --- because every sorting network is represented by Bundala and Zavodny as an itemset. The complexity classification presented in this paper applies sorting networks, as well as the general itemset isomorphism problem. The main consequence of our work is that currently no polynomial-time algorithm exists for solving the BZ sorting network subitemset isomorphism problem; however the CM (Choi and Moon) sorting network isomorphism problem can be efficiently solved in polynomial time.',\n",
       "  'Title: on empirical cumulant generating functions of code lengths for individual sequences\\nAbstract: We consider the problem of lossless compression of individual sequences using finite–state (FS) machines, from the perspective of the best achievable empirical cumulant generating function (CGF) of the code length, i.e., the normalized logarithm of the empirical average of the exponentiated code length. Since the probabilistic CGF is minimized in terms of the Renyi entropy of the source, one of the motivations of this study is to derive an individual–sequence analogue of the Renyi entropy, in the same way that the FS compressibility is the individual–sequence counterpart of the Shannon entropy. We consider the CGF of the code-length both from the perspective of fixed–to–variable (F-V) length coding and the perspective of variable–to–variable (V-V) length coding, where the latter turns out to yield a better result, that coincides with the FS compressibility. We also extend our results to compression with side information, available at both the encoder and decoder. In this case, the V–V version no longer coincides with the FS compressibility, but results in a different complexity measure. Index Terms Individual sequences, compressibility, finite–state machines, cumulant generating function, Renyi entropy, Lempel–Ziv algorithm.',\n",
       "  \"Title: improving image captioning by concept based sentence reranking\\nAbstract: This paper describes our winning entry in the ImageCLEF 2015 image sentence generation task. We improve Google's CNN-LSTM model by introducing concept-based sentence reranking, a data-driven approach which exploits the large amounts of concept-level annotations on Flickr. Different from previous usage of concept detection that is tailored to specific image captioning models, the propose approach reranks predicted sentences in terms of their matches with detected concepts, essentially treating the underlying model as a black box. This property makes the approach applicable to a number of existing solutions. We also experiment with fine tuning on the deep language model, which improves the performance further. Scoring METEOR of 0.1875 on the ImageCLEF 2015 test set, our system outperforms the runner-up (METEOR of 0.1687) with a clear margin.\",\n",
       "  'Title: inference based semantics in data exchange\\nAbstract: Data Exchange is an old problem that was firstly studied from a theoretical point of view only in 2003. Since then many approaches were considered when it came to the language describing the relationship between the source and the target schema. These approaches focus on what it makes a target instance a \"good\" solution for data-exchange. In this paper we propose the inference-based semantics that solves many certain-answer anomalies existing in current data-exchange semantics. To this we introduce a new mapping language between the source and the target schema based on annotated bidirectional dependencies (abd) and, consequently define the semantics for this new language. It is shown that the ABD-semantics can properly represent the inference-based semantics, for any source-to-target mappings. We discovered three dichotomy results under the new semantics for solution-existence, solution-check and UCQ evaluation problems. These results rely on two factors describing the annotation used in the mappings (density and cardinality). Finally we also investigate the certain-answers evaluation problem under ABD-semantics and discover many tractable classes for non-UCQ queries even for a subclass of CQ with negation.',\n",
       "  'Title: bounds on the communication rate needed to achieve sk capacity in the hypergraphical source model\\nAbstract: In the multiterminal source model of Csiszar and Narayan, the communication complexity, R SK , for secret key (SK) generation is the minimum rate of communication required to achieve SK capacity. An obvious upper bound to R SK  is given by R CO , which is the minimum rate of communication required for omniscience. In this paper we derive a better upper bound to R SK  for the hypergraphical source model, which is a special instance of the multiterminal source model. The upper bound is based on the idea of fractional removal of hyperedges. It is further shown that this upper bound can be computed in polynomial time. We conjecture that our upper bound is tight. For the special case of a graphical source model, we also give an explicit lower bound on R SK . This bound, however, is not tight, as demonstrated by a counterexample.',\n",
       "  'Title: total variation reconstruction for compressive sensing using nonlocal lagrangian multiplier\\nAbstract: Total variation has proved its effectiveness in solving inverse problems for compressive sensing. Besides, the nonlocal means filter used as regularization preserves texture better for recovered images, but it is quite complex to implement. In this paper, based on existence of both noise and image information in the Lagrangian multiplier, we propose a simple method in term of implementation called nonlocal Lagrangian multiplier (NLLM) in order to reduce noise and boost useful image information. Experimental results show that the proposed NLLM is superior both in subjective and objective qualities of recovered image over other recovery algorithms.',\n",
       "  'Title: inter technology coexistence in a spectrum commons a case study of wi fi and lte in the 5 ghz unlicensed band\\nAbstract: Spectrum sharing mechanisms need to be carefully designed to enable inter-technology coexistence in the unlicensed bands, as these bands are an instance of a  spectrum commons  where highly heterogeneous technologies and deployments must coexist. Unlike in licensed bands, where multiple technologies could coexist only in a primary–secondary dynamic spectrum access mode, a spectrum commons offers competition opportunities between multiple dominant technologies, such as Wi-Fi and the recently proposed LTE in the 5 GHz unlicensed band. In this paper, we systematically study the performance of different spectrum sharing schemes for inter-technology coexistence in a spectrum commons. Our contributions are threefold. First, we propose a general framework for transparent comparative analysis of spectrum sharing mechanisms in time and frequency, by studying the effect of key constituent parameters. Second, we propose a novel throughput and interference model for inter-technology coexistence, integrating per-device specifics of different distributed MAC sharing mechanisms in a unified network-level perspective. Finally, we present a case study of IEEE 802.11n Wi-Fi and LTE in the 5 GHz unlicensed band, in order to obtain generalizable insight into coexistence in a spectrum commons. Our extensive Monte Carlo simulation results show that LTE/Wi-Fi coexistence in the 5 GHz band can be ensured simply through channel selection schemes, such that time-sharing MAC mechanisms are irrelevant. We also show that, in the general co-channel case, the coexistence performance of MAC sharing mechanisms strongly depends on the  interference coupling  in the network, predominantly determined by building shielding. We thus identify two regimes: (i) low interference coupling, e.g., residential indoor scenarios, where duty cycle mechanisms outperform sensing-based listen-before-talk (LBT) mechanisms and (ii) high interference coupling, e.g., open-plan indoor or outdoor hotspot scenarios, where LBT outperforms duty cycle mechanisms.',\n",
       "  'Title: secure repairable fountain codes\\nAbstract: In this letter, we provide the construction of repairable fountain codes (RFCs) for distributed storage systems that are information-theoretically secure against an eavesdropper that has access to the data stored in a subset of the storage nodes and the data downloaded to repair an additional subset of storage nodes. The security is achieved by adding random symbols to the message, which is then encoded by the concatenation of a Gabidulin code and an RFC. We compare the achievable code rates of the proposed codes with those of secure minimum storage regenerating codes and secure locally repairable codes.',\n",
       "  'Title: the logic of counting query answers\\nAbstract: We consider the problem of counting the number of answers to a first-order formula on a finite structure. We present and study an extension of first-order logic in which algorithms for this counting problem can be naturally and conveniently expressed, in senses that are made precise and that are motivated by the wish to understand tractable cases of the counting problem.',\n",
       "  'Title: secure friend discovery via privacy preserving and decentralized community detection\\nAbstract: The problem of secure friend discovery on a social network has long been proposed and studied. The requirement is that a pair of nodes can make befriending decisions with minimum information exposed to the other party. In this paper, we propose to use community detection to tackle the problem of secure friend discovery. We formulate the first privacy-preserving and decentralized community detection problem as a multiobjective optimization. We design the first protocol to solve this problem, which transforms community detection to a series of Private Set Intersection (PSI) instances using Truncated Random Walk (TRW). Preliminary theoretical results show that our protocol can uncover communities with overwhelming probability and preserve privacy. We also discuss future works, potential extensions and variations.',\n",
       "  \"Title: iris recognition with a database of iris images obtained in visible light using smartphone camera\\nAbstract: This paper delivers a new database of iris images collected in visible light using a mobile phone's camera and presents results of experiments involving existing commercial and open-source iris recognition methods, namely: Iri-Core, VeriEye, MIRLIN and OSIRIS. Several important observations are made. First, we manage to show that after simple preprocessing, such images offer good visibility of iris texture even in heavily-pigmented irides. Second, for all four methods, the enrollment stage is not much affected by the fact that different type of data is used as input. This translates to zero or close-to-zero Failure To Enroll, i.e., cases when templates could not be extracted from the samples. Third, we achieved good matching accuracy, with correct genuine match rate exceeding 94.5% for all four methods, while simultaneously being able to maintain zero false match rate in every case. Correct genuine match rate of over 99.5% was achieved using one of the commercial methods, showing that such images can be used with the existing biometric solutions with minimum additional effort required. Finally, the experiments revealed that incorrect image segmentation is the most prevalent cause of recognition accuracy decrease. To our best knowledge, this is the first database of iris images captured using a mobile device, in which image quality exceeds this of a near-infrared illuminated iris images, as defined in ISO/IEC 19794-6 and 29794-6 documents. This database will be publicly available to all researchers.\",\n",
       "  'Title: residual networks are exponential ensembles of relatively shallow networks\\nAbstract: In this work, we introduce a novel interpretation of residual networks showing they are exponential ensembles. This observation is supported by a large-scale lesion study that demonstrates they behave just like ensembles at test time. Subsequently, we perform an analysis showing these ensembles mostly consist of networks that are each relatively shallow. For example, contrary to our expectations, most of the gradient in a residual network with 110 layers comes from an ensemble of very short networks, i.e., only 10-34 layers deep. This suggests that in addition to describing neural networks in terms of width and depth, there is a third dimension: multiplicity, the size of the implicit ensemble. Ultimately, residual networks do not resolve the vanishing gradient problem by preserving gradient flow throughout the entire depth of the network - rather, they avoid the problem simply by ensembling many short networks together. This insight reveals that depth is still an open research question and invites the exploration of the related notion of multiplicity.',\n",
       "  'Title: i accidentally the whole internet\\nAbstract: Whether as telecommunications or power systems, networks are very important in everyday life. Maintaining these networks properly functional and connected, even under attacks or failures, is of special concern. This topic has been previously studied with a whole network robustness perspective,modeling networks as undirected graphs (such as roads or simply cables). This perspective measures the average behavior of the network after its last node has failed. In this article we propose two alternatives to well-known studies about the robustness of the backbone Internet: to use a supply network model and metrics for its representation (we called it the Go-Index), and to use robustness metrics that can be calculated while disconnections appear. Our research question is: if a smart adversary has a limited number of strikes to attack the Internet, how much will the damage be after each one in terms of network disconnection? Our findings suggest that in order to design robust networks it might be better to have a complete view of the robustness evolution of the network, from both the infrastructure and the users perspective.',\n",
       "  'Title: do users focus on the correct cues to differentiate between phishing and genuine emails\\nAbstract: This paper examines the cues that typically differentiate phishing emails from genuine emails. The research is conducted in two stages. In the first stage, we identify the cues that actually differentiate between phishing and genuine emails. These are the consistency and personalisation of the message, the perceived legitimacy of links and sender, and the presence of spelling or grammatical irregularities. In the second stage, we identify the cues that participants use to differentiate between phishing and genuine emails. This revealed that participants often use cues that are not good indicators of whether an email is phishing or genuine. This includes the presence of legal disclaimers, the quality of visual presentation, and the positive consequences emphasised in the email. This study has implications for education and training and provides a basis for the design and development of targeted and more relevant training and risk communication strategies.',\n",
       "  'Title: spontaneous subtle expression detection and recognition based on facial strain\\nAbstract: Optical strain is an extension of optical flow that is capable of quantifying subtle changes on faces and representing the minute facial motion intensities at the pixel level. This is computationally essential for the relatively new field of spontaneous micro-expression, where subtle expressions can be technically challenging to pinpoint. In this paper, we present a novel method for detecting and recognizing micro-expressions by utilizing facial optical strain magnitudes to construct optical strain features and optical strain weighted features. The two sets of features are then concatenated to form the resultant feature histogram. Experiments were performed on the CASME II and SMIC databases. We demonstrate on both databases, the usefulness of optical strain information and more importantly, that our best approaches are able to outperform the original baseline results for both detection and recognition tasks. A comparison of the proposed method with other existing spatio-temporal feature extraction approaches is also presented. HighlightsThe method proposed is a combination of two optical strain derived features.Optical strain magnitudes were employed to describe fine subtle facial movements.Evaluation was performed in both the detection and recognition tasks.Promising performances were obtained in two micro-expression databases.',\n",
       "  'Title: dialport connecting the spoken dialog research community to real user data\\nAbstract: This paper describes a new spoken dialog portal that connects systems produced by the spoken dialog academic research community and gives them access to real users. We introduce a distributed, multi-modal, multi-agent prototype dialog framework that affords easy integration with various remote resources, ranging from end-to-end dialog systems to external knowledge APIs. To date, the DialPort portal has successfully connected to the multi-domain spoken dialog system at Cambridge University, the NOAA (National Oceanic and Atmospheric Administration) weather API and the Yelp API.',\n",
       "  'Title: low complexity antenna selection for low target rate users in dense cloud radio access networks\\nAbstract: We propose a low complexity antenna selection algorithm for low target rate users in cloud radio access networks. The algorithm consists of two phases: In the first phase, each remote radio head (RRH) determines whether to be included in a candidate set by using a predefined selection threshold. In the second phase, RRHs are randomly selected within the candidate set made in the first phase. To analyze the performance of the proposed algorithm, we model RRHs and users locations by a homogeneous Poisson point process, whereby the signal-to-interference ratio (SIR) complementary cumulative distribution function is derived. By approximating the derived expression, an approximate optimum selection threshold that maximizes the SIR coverage probability is obtained. Using the obtained threshold, we characterize the performance of the algorithm in an asymptotic regime where the RRH density goes to infinity. The obtained threshold is then modified depending on various algorithm options. A distinguishable feature of the proposed algorithm is that the algorithm complexity keeps constant independent to the RRH density, so that a user is able to connect to a network without heavy computation at baseband units.',\n",
       "  'Title: interdiction in practice hardware trojan against a high security usb flash drive\\nAbstract: As part of the revelations about the NSA activities, the notion of interdiction has become known to the public: the interception of deliveries to manipulate hardware in a way that backdoors are introduced. Manipulations can occur on the firmware or at hardware level. With respect to hardware, FPGAs are particular interesting targets as they can be altered by manipulating the corresponding bitstream which configures the device. In this paper, we demonstrate the first successful real-world FPGA hardware Trojan insertion into a commercial product. On the target device, a FIPS-140-2 level 2 certified USB flash drive from Kingston, the user data are encrypted using AES-256 in XTS mode, and the encryption/decryption is processed by an off-the-shelf SRAM-based FPGA. Our investigation required two reverse-engineering steps, related to the proprietary FPGA bitstream and to the firmware of the underlying ARM CPU. In our Trojan insertion scenario, the targeted USB flash drive is intercepted before being delivered to the victim. The physical Trojan insertion requires the manipulation of the SPI flash memory content, which contains the FPGA bitstream as well as the ARM CPU code. The FPGA bitstream manipulation alters the exploited AES-256 algorithm in a way that it turns into a linear function which can be broken with 32 known plaintext–ciphertext pairs. After the manipulated USB flash drive has been used by the victim, the attacker is able to obtain all user data from the ciphertexts. Our work indeed highlights the security risks and especially the practical relevance of bitstream modification attacks that became realistic due to FPGA bitstream manipulations.',\n",
       "  \"Title: computing backup forwarding rules in software defined networks\\nAbstract: The past century of telecommunications has shown that failures in networks are prevalent. Although much has been done to prevent failures, network nodes and links are bound to fail eventually. Failure recovery processes are therefore needed. Failure recovery is mainly influenced by (1) detection of the failure, and (2) circumvention of the detected failure. However, especially in SDNs where controllers recompute network state reactively, this leads to high delays. Hence, next to primary rules, backup rules should be installed in the switches to quickly detour traffic once a failure occurs. In this work, we propose algorithms for computing an all-to-all primary and backup network forwarding configuration that is capable of circumventing link and node failures. Omitting the high delay invoked by controller recomputation through preconfiguration, our proposal's recovery delay is close to the detection time which is significantly below the 50 ms rule of thumb. After initial recovery, we recompute network configuration to guarantee protection from future failures. Our algorithms use packet-labeling to guarantee correct and shortest detour forwarding. The algorithms and labeling technique allow packets to return to the primary path and are able to discriminate between link and node failures. The computational complexity of our solution is comparable to that of all-to-all-shortest paths computations. Our experimental evaluation on both real and generated networks shows that network configuration complexity highly decreases compared to classic disjoint paths computations. Finally, we provide a proof-of-concept OpenFlow controller in which our proposed configuration is implemented, demonstrating that it readily can be applied in production networks.\",\n",
       "  \"Title: bitcoin s security model revisited\\nAbstract: We revisit the fundamental question of Bitcoin's security against double spending attacks. While previous work has bounded the probability that a transaction is reversed, we show that no such guarantee can be effectively given if the attacker can choose when to launch the attack. Other approaches that bound the cost of an attack have erred in considering only limited attack scenarios, and in fact it is easy to show that attacks may not cost the attacker at all. We therefore provide a different interpretation of the results presented in previous papers and correct them in several ways. We provide different notions of the security of transactions that provide guarantees to different classes of defenders: merchants who regularly receive payments, miners, and recipients of large one-time payments. We additionally consider an attack that can be launched against lightweight clients, and show that these are less secure than their full node counterparts and provide the right strategy for defenders in this case as well. Our results, overall, improve the understanding of Bitcoin's security guarantees and provide correct bounds for those wishing to safely accept transactions.\",\n",
       "  'Title: latent bi constraint svm for video based object recognition\\nAbstract: We address the task of recognizing objects from video input. This important problem is relatively unexplored, compared with image-based object recognition. To this end, we make the following contributions. First, we introduce two comprehensive datasets for video-based object recognition. Second, we propose Latent Bi-constraint SVM (LBSVM), a maximum-margin framework for video-based object recognition. LBSVM is based on Structured-Output SVM, but extends it to handle noisy video data and ensure consistency of the output decision throughout time. We apply LBSVM to recognize office objects and museum sculptures, and we demonstrate its benefits over image-based, set-based, and other video-based object recognition.',\n",
       "  \"Title: minimax estimation of kl divergence between discrete distributions\\nAbstract: We consider the problem of estimating the KL divergence between two discrete probability measures $P$ and $Q$ from empirical data in a non-asymptotic and possibly large alphabet setting. We construct minimax rate-optimal estimators for $D(P\\\\|Q)$ when the likelihood ratio is upper bounded by a constant which may depend on the support size, and show that the performance of the optimal estimator with $n$ samples is essentially that of the Maximum Likelihood Estimator (MLE) with $n\\\\ln n$ samples. Our estimator is adaptive in the sense that it does not require the knowledge of the support size or the upper bound on the likelihood ratio. Our approach refines the \\\\emph{Approximation} methodology recently developed for the construction of near minimax estimators of functionals of high-dimensional parameters, such as entropy, R\\\\'enyi entropy, mutual information and $\\\\ell_1$ distance in large alphabet settings, and shows that the \\\\emph{effective sample size enlargement} phenomenon holds significantly more widely than previously established.\",\n",
       "  'Title: weighted residuals for very deep networks\\nAbstract: Deep residual networks have recently shown appealing performance on many challenging computer vision tasks. However, the original residual structure still has some defects making it difficult to converge on very deep networks. In this paper, we introduce a weighted residual network to address the incompatibility between \\\\texttt{ReLU} and element-wise addition and the deep network initialization problem. The weighted residual network is able to learn to combine residuals from different layers effectively and efficiently. The proposed models enjoy a consistent improvement over accuracy and convergence with increasing depths from 100+ layers to 1000+ layers. Besides, the weighted residual networks have little more computation and GPU memory burden than the original residual networks. The networks are optimized by projected stochastic gradient descent. Experiments on CIFAR-10 have shown that our algorithm has a \\\\emph{faster convergence speed} than the original residual networks and reaches a \\\\emph{high accuracy} at 95.3\\\\% with a 1192-layer model.',\n",
       "  'Title: image edge detection based on swarm intelligence using memristive networks\\nAbstract: Recent advancements in the development of memristive devices has opened new opportunities for hardware implementation of non-Boolean computing. To this end, the suitability of memristive devices for swarm intelligence algorithms has enabled researchers to solve a maze in hardware. In this paper, we utilize swarm intelligence of memristive networks to perform image edge detection. First, we propose a hardware-friendly algorithm for image edge detection based on ant colony. Second, we implement the image edge detection algorithm using memristive networks. Furthermore, we explain the impact of various parameters of the memristors on the efficacy of the implementation. Our results show 28% improvement in the energy compared to a low power CMOS hardware implementation based on stochastic circuits. Furthermore, our design occupies up to 5x less area.',\n",
       "  'Title: the crossover process learnability meets protection from inference attacks\\nAbstract: It is usual to consider data protection and learnability as conflicting objectives. This is not always the case: we show how to jointly control causal inference --- seen as the attack --- \\\\textit{and} learnability by a noise-free process that mixes training examples, the Crossover Process (cp). One key point is that the cp~is typically able to alter joint distributions without touching on marginals, nor altering the sufficient statistic for the class. In other words, it saves (and sometimes improves) generalization for supervised learning, but can alter the relationship between covariates --- and therefore fool statistical measures of (nonlinear) independence and causal inference into misleading ad-hoc conclusions. Experiments on a dozen readily available domains validate the theory.',\n",
       "  'Title: fundamentals of modeling finite wireless networks using binomial point process\\nAbstract: Modeling the locations of nodes as a uniform binomial point process (BPP), we present a generic mathematical framework to characterize the performance of an arbitrarily-located reference receiver in a finite wireless network. Different from most of the prior works where the serving transmitter (TX) node is located at the fixed distance from the reference receiver, we consider two general TX-selection policies: i) uniform TX-selection: the serving node is chosen uniformly at random amongst transmitting nodes, and ii) k-closest TX-selection: the serving node is the k-th closest node out of transmitting nodes to the reference receiver. The key intermediate step in our analysis is the derivation of a new set of distance distributions that lead not only to the tractable analysis of coverage probability but also enable the analyses of wide range of classical and currently trending problems in wireless networks. Using this new set of distance distributions, we first investigate the diversity loss due to SIR correlation in a finite network. We then obtain the optimal number of links that can be simultaneously activated to maximize network spectral efficiency. Finally, we evaluate optimal caching probability to maximize the total hit probability in cache-enabled finite networks.',\n",
       "  'Title: double detector for sparse signal detection from one bit compressed sensing measurements\\nAbstract: This letter presents the sparse vector signal detection from one bit compressed sensing measurements, in contrast to the previous works that deal with scalar signal detection. Available results are extended to the vector case and the generalized likelihood ratio test (GLRT) detector and the optimal quantizer design are obtained. A double-detector scheme is introduced, in which a sensor level threshold detector is integrated into network level GLRT to improve the performance. The detection criteria of oracle and clairvoyant detectors are also derived. Simulation results show that with careful design of the threshold detector, the overall detection performance of double-detector scheme would be better than the sign-GLRT proposed in [J. Fang  et\\xa0al.,  “One-bit quantizer design for multisensor GLRT fusion,”  IEEE Signal Process. Lett. , vol. 20, no.\\xa03, pp.\\xa0257–260, Mar. 2013] and close to oracle and clairvoyant detectors. The proposed detector is applied to spectrum sensing and the results are near the well-known energy detector, which uses the real valued data, while the proposed detector only uses the sign of the data.',\n",
       "  'Title: a modified activation function with improved run times for neural networks\\nAbstract: In this paper we present a modified version of the Hyperbolic Tangent Activation Function as a learning unit generator for neural networks. The function uses an integer calibration constant as an approximation to the Euler number, e, based on a quadratic Real Number Formula (RNF) algorithm and an adaptive normalization constraint on the input activations to avoid the vanishing gradient. We demonstrate the effectiveness of the proposed modification using a hypothetical and real world dataset and show that lower run-times can be achieved by learning algorithms using this function leading to improved speed-ups and learning accuracies during training.',\n",
       "  \"Title: bridging nonlinearities and stochastic regularizers with gaussian error linear units\\nAbstract: We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU nonlinearity is the expected transformation of a stochastic regularizer which randomly applies the identity or zero map to a neuron's input. This stochastic regularizer is comparable to nonlinearities aided by dropout, but it removes the need for a traditional nonlinearity. The connection between the GELU and the stochastic regularizer suggests a new probabilistic understanding of nonlinearities. We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all tasks.\",\n",
       "  'Title: there is something beyond the twitter network\\nAbstract: How information spreads through a social network? Can we assume, that the information is spread only through a given social network graph? What is the correct way to compare the models of information flow? These are the basic questions we address in this work. We focus on meticulous comparison of various, well-known models of rumor propagation in the social network. We introduce the model incorporating mass media and effects of absent nodes. In this model the information appears spontaneously in the graph. Using the most conservative metric, we showed that the distribution of cascades sizes generated by this model fits the real data much better than the previously considered models.',\n",
       "  'Title: show me the cup reference with continuous representations\\nAbstract: One of the most basic functions of language is to refer to objects in a shared scene. Modeling reference with continuous representations is challenging because it requires individuation, i.e., tracking and distinguishing an arbitrary number of referents. We introduce a neural network model that, given a definite description and a set of objects represented by natural images, points to the intended object if the expression has a unique referent, or indicates a failure, if it does not. The model, directly trained on reference acts, is competitive with a pipeline manually engineered to perform the same task, both when referents are purely visual, and when they are characterized by a combination of visual and linguistic properties.',\n",
       "  'Title: a note on some inequalities used in channel polarization and polar coding\\nAbstract: We give a unified treatment of some inequalities that are used in the proofs of channel polarization theorems involving a binary-input discrete memoryless channel.',\n",
       "  'Title: energy efficient routing in wireless networks in the presence of jamming\\nAbstract: The effectiveness and the simple implementation of physical layer jammers make them an essential threat for wireless networks. In a multihop wireless network, where jammers can interfere with the transmission of user messages at intermediate nodes along the path, one can employ jamming oblivious routing and then employ physical-layer techniques (e.g., spread spectrum) to suppress jamming. However, whereas these approaches can provide significant gains, the residual jamming can still severely limit system performance. This motivates the consideration of routing approaches that account for the differences in the jamming environment between different paths. First, we take a straightforward approach where an equal outage probability is allocated to each link along a path and develop a minimum energy routing solution. Next, we demonstrate the shortcomings of this approach and then consider the joint problem of outage allocation and routing by employing an approximation to the link outage probability. This yields an efficient and effective routing algorithm that only requires knowledge of the measured jamming at each node. Numerical results demonstrate that the amount of energy saved by the proposed methods with respect to a standard minimum energy routing algorithm, especially for parameters appropriate for terrestrial wireless networks, is substantial.',\n",
       "  'Title: improved lower bounds for online hypercube packing\\nAbstract: Packing a given sequence of items into as few bins as possible in an online fashion is a widely studied problem. We improve lower bounds for packing hypercubes into bins in two or more dimensions, once for general algorithms (in two dimensions) and once for an important subclass, so-called Harmonic-type algorithms (in two or more dimensions). Lastly, we show that two adaptions of the ideas from the best known one-dimensional packing algorithm to square packing also do not help to break the barrier of 2.',\n",
       "  'Title: neural network based next song recommendation\\nAbstract: Recently, the next-item/basket recommendation system, which considers the sequential relation between bought items, has drawn attention of researchers. The utilization of sequential patterns has boosted performance on several kinds of recommendation tasks. Inspired by natural language processing (NLP) techniques, we propose a novel neural network (NN) based next-song recommender, CNN-rec, in this paper. Then, we compare the proposed system with several NN based and classic recommendation systems on the next-song recommendation task. Verification results indicate the proposed system outperforms classic systems and has comparable performance with the state-of-the-art system.',\n",
       "  'Title: person identification based on hand tremor characteristics\\nAbstract: A plethora of biometric measures have been proposed in the past. In this paper we introduce a new potential biometric measure: the human tremor. We present a new method for identifying the user of a handheld device using characteristics of the hand tremor measured with a smartphone built-in inertial sensors (accelerometers and gyroscopes). The main challenge of the proposed method is related to the fact that human normal tremor is very subtle while we aim to address real-life scenarios. To properly address the issue, we have relied on weighted Fourier linear combiner for retrieving only the tremor data from the hand movement and random forest for actual recognition. We have evaluated our method on a database with 10 000 samples from 17 persons reaching an accuracy of 76%.',\n",
       "  \"Title: community detection in political twitter networks using nonnegative matrix factorization methods\\nAbstract: Community detection is a fundamental task in social network analysis. In this paper, first we develop an endorsement filtered user connectivity network by utilizing Heider's structural balance theory and certain Twitter triad patterns. Next, we develop three Nonnegative Matrix Factorization frameworks to investigate the contributions of different types of user connectivity and content information in community detection. We show that user content and endorsement filtered connectivity information are complementary to each other in clustering politically motivated users into pure political communities. Word usage is the strongest indicator of users' political orientation among all content categories. Incorporating user-word matrix and word similarity regularizer provides the missing link in connectivity only methods which suffer from detection of artificially large number of clusters for Twitter networks.\",\n",
       "  'Title: a non parametric learning approach to identify online human trafficking\\nAbstract: Human trafficking is among the most challenging law enforcement problems which demands persistent fight against from all over the globe. In this study, we leverage readily available data from the website \"Backpage\"-- used for classified advertisement-- to discern potential patterns of human trafficking activities which manifest online and identify most likely trafficking related advertisements. Due to the lack of ground truth, we rely on two human analysts --one human trafficking victim survivor and one from law enforcement, for hand-labeling the small portion of the crawled data. We then present a semi-supervised learning approach that is trained on the available labeled and unlabeled data and evaluated on unseen data with further verification of experts.',\n",
       "  'Title: doubly massive mmwave mimo systems using very large antenna arrays at both transmitter and receiver\\nAbstract: One of the key features of next generation wireless communication systems will be the use of frequencies in the range 10-100GHz (aka mmWave band) in densely populated indoor and outdoor scenarios. Due to the reduced wavelength, antenna arrays with a large number of antennas can be packed in very small volumes, making thus it possible to consider, at least in principle, communication links wherein not only the base-station, but also the user device, are equipped with very large antenna arrays. We denote this configuration as a \"doubly-massive\" MIMO wireless link. This paper introduces the concept of doubly massive MIMO systems at mmWave, showing that at mmWave the fundamentals of the massive MIMO regime are completely different from what happens at conventional sub-6 GHz cellular frequencies. It is shown for instance that the multiplexing capabilities of the channel and its rank are no longer ruled by the number of transmit and receive antennas, but rather by the number of scattering clusters in the surrounding environment. The implications of the doubly massive MIMO regime on the transceiver processing, on the system energy efficiency and on the system throughput are also discussed.',\n",
       "  'Title: context discovery for model learning in partially observable environments\\nAbstract: The ability to learn a model is essential for the success of autonomous agents. Unfortunately, learning a model is difficult in partially observable environments, where latent environmental factors influence what the agent observes. In the absence of a supervisory training signal, autonomous agents therefore require a mechanism to autonomously discover these environmental factors, or sensorimotor contexts. #R##N#This paper presents a method to discover sensorimotor contexts in partially observable environments, by constructing a hierarchical transition model. The method is evaluated in a simulation experiment, in which a robot learns that different rooms are characterized by different objects that are found in them.',\n",
       "  'Title: a linear kernel for finding square roots of almost planar graphs\\nAbstract: A graph H is a square root of a graph G if G can be obtained from H by the addition of edges between any two vertices in H that are of distance 2 from each other. The Square Root problem is that of deciding whether a given graph admits a square root. We consider this problem for planar graphs in the context of the \"distance from triviality\" framework. For an integer k, a planar+kv graph (or k-apex graph) is a graph that can be made planar by the removal of at most k vertices. We prove that a generalization of Square Root, in which some edges are prescribed to be either in or out of any solution, has a kernel of size O(k) for planar+kv graphs, when parameterized by k. Our result is based on a new edge reduction rule which, as we shall also show, has a wider applicability for the Square Root problem.',\n",
       "  'Title: on hybrid pilot for channel estimation in massive mimo uplink\\nAbstract: This paper introduces a hybrid pilot-aided channel estimation technique for mitigating the effect of pilot contamination for the uplink of multi-cell multiuser massive MIMO systems. The proposed hybrid pilot is designed such that it enjoys the complementary advantages between time-multiplexed (TM) pilot and time-superimposed (TS) pilot, and thereby, allows superior solution to the conventional pilot schemes. We mathematically characterize the impact of hybrid pilot on the massive MIMO uplink by deriving a closed-form approximation for the uplink achievable rate. In large-number-of-antennas regime, we obtain the asymptotically optimal solution for hybrid pilot by jointly designing the TM pilot and the TS pilot. It is shown that either TM pilot or TS pilot has the advantages for large frame-size and limited frame-size transmission, respectively, while the hybrid pilot scheme can offer a superior performance to that employing either TM pilot or TS pilot. Numerical results demonstrate the effectiveness of the proposed design.',\n",
       "  \"Title: reflections on shannon information in search of a natural information entropy for images\\nAbstract: It is not obvious how to extend Shannon's original information entropy to higher dimensions, and many different approaches have been tried. We replace the English text symbol sequence originally used to illustrate the theory by a discrete, bandlimited signal. Using Shannon's later theory of sampling we derive a new and symmetric version of the second order entropy in 1D. The new theory then naturally extends to 2D and higher dimensions, where by naturally we mean simple, symmetric, isotropic and parsimonious. Simplicity arises from the direct application of Shannon's joint entropy equalities and inequalities to the gradient (del) vector field image embodying the second order relations of the scalar image. Parsimony is guaranteed by halving of the vector data rate using Papoulis' generalized sampling expansion. The new 2D entropy measure, which we dub delentropy, is underpinned by a computable probability density function we call deldensity. The deldensity captures the underlying spatial image structure and pixel co-occurrence. It achieves this because each scalar image pixel value is nonlocally related to the entire gradient vector field. Both deldensity and delentropy are highly tractable and yield many interesting connections and useful inequalities. The new measure explicitly defines a realizable encoding algorithm and a corresponding reconstruction. Initial tests show that delentropy compares favourably with the conventional intensity-based histogram entropy and the compressed data rates of lossless image encoders (GIF, PNG, WEBP, JP2K-LS and JPG-LS) for a selection of images. The symmetric approach may have applications to higher dimensions and problems concerning image complexity measures.\",\n",
       "  'Title: indexing and querying color sets of images\\nAbstract: We aim to study the set of color sets of continuous regions of an image given as a matrix of m rows over n ź m columns where each element in the matrix is an integer from 1 , ź named a color. The set of distinct colors in a region is called fingerprint. We aim to compute, index and query the fingerprints of all rectangular regions named rectangles. The set of all such fingerprints is denoted by F . A rectangle is maximal if it is not contained in a greater rectangle with the same fingerprint. The set of all locations of maximal rectangles is denoted by L . We first explain how to determine all the | L | maximal locations with their fingerprints in expected time O ( n m 2 ź ) using a Monte Carlo algorithm (with polynomially small probability of error) or within deterministic O ( n m 2 ź log ź ( | L | n m 2 + 2 ) ) time. We then show how to build a data structure which occupies O ( n m log ź n + | L | ) space such that a query which asks for all the maximal locations with a given fingerprint f can be answered in time O ( | f | + log ź log ź n + k ) , where k is the number of maximal locations with fingerprint f. If the query asks only for the presence of the fingerprint, then the space usage becomes O ( n m log ź n + | F | ) while the query time becomes O ( | f | + log ź log ź n ) . We eventually consider the special case of squared regions (squares).',\n",
       "  'Title: secret sharing with trusted third parties using piggy bank protocol\\nAbstract: This paper presents a new scheme to distribute secret shares using two trusted third parties to increase security and eliminate the dependency on single trusted third party. This protocol for communication between a device and two trusted third parties uses the piggy bank cryptographic paradigm. We also present a protocol to give law enforcing agencies access to sensitive information present on a cell phone or a device using secret sharing scheme. The ideas for classical systems may also be applied to quantum schemes.',\n",
       "  'Title: testing k monotonicity\\nAbstract: A Boolean $k$-monotone function defined over a finite poset domain ${\\\\cal D}$ alternates between the values $0$ and $1$ at most $k$ times on any ascending chain in ${\\\\cal D}$. Therefore, $k$-monotone functions are natural generalizations of the classical monotone functions, which are the $1$-monotone functions. Motivated by the recent interest in $k$-monotone functions in the context of circuit complexity and learning theory, and by the central role that monotonicity testing plays in the context of property testing, we initiate a systematic study of $k$-monotone functions, in the property testing model. In this model, the goal is to distinguish functions that are $k$-monotone (or are close to being $k$-monotone) from functions that are far from being $k$-monotone. Our results include the following: #R##N#- We demonstrate a separation between testing $k$-monotonicity and testing monotonicity, on the hypercube domain $\\\\{0,1\\\\}^d$, for $k\\\\geq 3$; #R##N#- We demonstrate a separation between testing and learning on $\\\\{0,1\\\\}^d$, for $k=\\\\omega(\\\\log d)$: testing $k$-monotonicity can be performed with $2^{O(\\\\sqrt d \\\\cdot \\\\log d\\\\cdot \\\\log{1/\\\\varepsilon})}$ queries, while learning $k$-monotone functions requires $2^{\\\\Omega(k\\\\cdot \\\\sqrt d\\\\cdot{1/\\\\varepsilon})}$ queries (Blais et al. (RANDOM 2015)). #R##N#- We present a tolerant test for functions $f\\\\colon[n]^d\\\\to \\\\{0,1\\\\}$ with complexity independent of $n$, which makes progress on a problem left open by Berman et al. (STOC 2014). #R##N#Our techniques exploit the testing-by-learning paradigm, use novel applications of Fourier analysis on the grid $[n]^d$, and draw connections to distribution testing techniques.',\n",
       "  'Title: recurrent regression for face recognition\\nAbstract: To address the sequential changes of images including poses, in this paper we propose a recurrent regression neural network(RRNN) framework to unify two classic tasks of cross-pose face recognition on still images and video-based face recognition. To imitate the changes of images, we explicitly construct the potential dependencies of sequential images so as to regularize the final learning model. By performing progressive transforms for sequentially adjacent images, RRNN can adaptively memorize and forget the information that benefits for the final classification. For face recognition of still images, given any one image with any one pose, we recurrently predict the images with its sequential poses to expect to capture some useful information of others poses. For video-based face recognition, the recurrent regression takes one entire sequence rather than one image as its input. We verify RRNN in static face dataset MultiPIE and face video dataset YouTube Celebrities(YTC). The comprehensive experimental results demonstrate the effectiveness of the proposed RRNN method.',\n",
       "  'Title: identifying dogmatism in social media signals and models\\nAbstract: We explore linguistic and behavioral features of dogmatism in social media and construct statistical models that can identify dogmatic comments. Our model is based on a corpus of Reddit posts, collected across a diverse set of conversational topics and annotated via paid crowdsourcing. We operationalize key aspects of dogmatism described by existing psychology theories (such as over-confidence), finding they have predictive power. We also find evidence for new signals of dogmatism, such as the tendency of dogmatic posts to refrain from signaling cognitive processes. When we use our predictive model to analyze millions of other Reddit posts, we find evidence that suggests dogmatism is a deeper personality trait, present for dogmatic users across many different domains, and that users who engage on dogmatic comments tend to show increases in dogmatic posts themselves.',\n",
       "  'Title: a non stochastic learning approach to energy efficient mobility management\\nAbstract: Energy efficient mobility management is an important problem in modern wireless networks with heterogeneous cell sizes and increased nodes densities. We show that optimization-based mobility protocols cannot achieve long-term optimal energy consumption, particularly for ultra-dense networks (UDN). To address the complex dynamics of UDN, we propose a non-stochastic online-learning approach which does not make any assumption on the statistical behavior of the small base station (SBS) activities. In addition, we introduce handover cost to the overall energy consumption, which forces the resulting solution to explicitly minimize frequent handovers. The proposed Batched Randomization with Exponential Weighting (BREW) algorithm relies on batching to explore in bulk, and hence reduces unnecessary handovers. We prove that the regret of BREW is sublinear in time, thus guaranteeing its convergence to the optimal SBS selection. We further study the robustness of the BREW algorithm to delayed or missing feedback. Moreover, we study the setting where SBSs can be dynamically turned on and off. We prove that sublinear regret is impossible with respect to arbitrary SBS on/off, and then develop a novel learning strategy, called ranking expert (RE), that simultaneously takes into account the handover cost and the availability of SBS. To address the high complexity of RE, we propose a contextual ranking expert (CRE) algorithm that only assigns experts in a given context. Rigorous regret bounds are proved for both RE and CRE with respect to the best expert. Simulations show that not only do the proposed mobility algorithms greatly reduce the system energy consumption, but they are also robust to various dynamics which are common in practical ultra-dense wireless networks.',\n",
       "  'Title: pilot contamination attack detection by key confirmation in secure mimo systems\\nAbstract: Many security techniques working at the physical layer need a correct channel state information (CSI) at the transmitter, especially when devices are equipped with multiple antennas. Therefore such techniques are vulnerable to pilot contamination attacks (PCAs) by which an attacker aims at inducing false CSI. In this paper we provide a solution to some PCA methods, by letting two legitimate parties to compare their channel estimates. The comparison is made in order to minimize the information leakage on the channel to a possible attacker. By reasonable assumptions on both the channel knowledge by the attacker and the correlation properties of the attacker and legitimate channels we show the validity of our solution. An accurate analysis of possible attacks and countermeasures is provided, together with a numerical evaluation of the attainable secrecy outage probability when our solution is used in conjunction with beamforming for secret communications.',\n",
       "  'Title: relief r cnn utilizing convolutional feature interrelationship for fast object detection deployment\\nAbstract: R-CNN style models are the state-of-the-art object detection methods, which consist of region proposal generation and deep CNN classification for objects. The proposal generation phase in this paradigm is usually time consuming and not convenient to be deployed on hardwares with limited computational ability. This article shows that the high-level patterns of feature value in deep convolutional feature maps contain plenty of useful spatial information, and propose a simple manually designed approach that can extract the information for fast region proposal generation. The proposed method, dubbed Relief R-CNN($R^2$-CNN), is a new deep learning approach for object detection. By extracting positions of objects from high-level convolutional patterns, $R^2$-CNN generates region proposals and performs deep classification simultaneously based on the same forwarding CNN features, unifies the formerly separated object detection process and speeds up the whole pipeline without additional training. Empirical studies show that $R^2$-CNN could achieve the best trade-off between speed and detection performance among all the comparison algorithms.',\n",
       "  'Title: detection of biasing attacks on distributed estimation networks\\nAbstract: The paper addresses the problem of detecting attacks on distributed estimator networks that aim to intentionally bias process estimates produced by the network. It provides a sufficient condition, in terms of the feasibility of certain linear matrix inequalities, which guarantees distributed input attack detection using an $H_\\\\infty$ approach.',\n",
       "  'Title: psi ψ a private data sharing interface\\nAbstract: We provide an overview of PSI (\"a Private data Sharing Interface\"), a system we are developing to enable researchers in the social sciences and other fields to share and explore privacy-sensitive datasets with the strong privacy protections of differential privacy.',\n",
       "  'Title: the north south divide in the italian higher education system\\nAbstract: This work examines whether the macroeconomic divide between northern and southern Italy is also present at the level of higher education. The analysis confirms that the research performance in the sciences of the professors in the south is on average less than that of the professors in the north, and that this gap does not show noticeable variations at the level of gender or academic rank. For the universities, the gap is still greater. The study analyzes some possible determinants of the gap, and provides some policy recommendations for its reduction.',\n",
       "  'Title: fusing similarity models with markov chains for sparse sequential recommendation\\nAbstract: Predicting personalized sequential behavior is a key task for recommender systems. In order to predict user actions such as the next product to purchase, movie to watch, or place to visit, it is essential to take into account both long-term user preferences and sequential patterns (i.e., short-term dynamics). Matrix Factorization and Markov Chain methods have emerged as two separate but powerful paradigms for modeling the two respectively. Combining these ideas has led to unified methods that accommodate long- and short-term dynamics simultaneously by modeling pairwise user-item and item-item interactions. #R##N#In spite of the success of such methods for tackling dense data, they are challenged by sparsity issues, which are prevalent in real-world datasets. In recent years, similarity-based methods have been proposed for (sequentially-unaware) item recommendation with promising results on sparse datasets. In this paper, we propose to fuse such methods with Markov Chains to make personalized sequential recommendations. We evaluate our method, Fossil, on a variety of large, real-world datasets. We show quantitatively that Fossil outperforms alternative algorithms, especially on sparse datasets, and qualitatively that it captures personalized dynamics and is able to make meaningful recommendations.',\n",
       "  'Title: cnndroid gpu accelerated execution of trained deep convolutional neural networks on android\\nAbstract: Many mobile applications running on smartphones and wearable devices would potentially benefit from the accuracy and scalability of deep CNN-based machine learning algorithms. However, performance and energy consumption limitations make the execution of such computationally intensive algorithms on mobile devices prohibitive. We present a GPU-accelerated library, dubbed CNNdroid [1], for execution of trained deep CNNs on Android-based mobile devices. Empirical evaluations show that CNNdroid achieves up to 60X speedup and 130X energy saving on current mobile devices. The CNNdroid open source library is available for download at https://github.com/ENCP/CNNdroid',\n",
       "  'Title: ap16 ol7 a multilingual database for oriental languages and a language recognition baseline\\nAbstract: We present the AP16-OL7 database which was released as the training and test data for the oriental language recognition (OLR) challenge on APSIPA 2016. Based on the database, a baseline system was constructed on the basis of the i-vector model. We report the baseline results evaluated in various metrics defined by the AP16-OLR evaluation plan and demonstrate that AP16-OL7 is a reasonable data resource for multilingual research.',\n",
       "  'Title: computation of forward stochastic reach sets application to stochastic dynamic obstacle avoidance\\nAbstract: We propose a method to efficiently compute the forward stochastic reach (FSR) set and its probability measure for nonlinear systems with an affine disturbance input, that is stochastic and bounded. This method is applicable to systems with an a priori known controller, or to uncontrolled systems, and often arises in problems in obstacle avoidance in mobile robotics. When used as a constraint in finite horizon controller synthesis, the FSR set, and its probability measure facilitates probabilistic collision avoidance, in contrast to methods which presume the obstacles act in a worst-case fashion and generate hard constraints that cannot be violated. We tailor our approach to accommodate rigid body constraints, and show convexity is assured so long as the rigid body shape of each obstacle is also convex. We extend methods for multi-obstacle avoidance through mixed integer linear programming (with linear robot and obstacle dynamics) to accommodate chance constraints that represent the FSR set probability measure. We demonstrate our method on a rigid-body obstacle avoidance scenario, in which a receding horizon controller is designed to avoid several stochastically moving obstacles while reaching the desired goal. Our approach can provide solutions when approaches that presume a worst-case action from the obstacle fail.',\n",
       "  'Title: distributed averaging cnn elm for big data\\nAbstract: Increasing the scalability of machine learning to handle big volume of data is a challenging task. The scale up approach has some limitations. In this paper, we proposed a scale out approach for CNN-ELM based on MapReduce on classifier level. Map process is the CNN-ELM training for certain partition of data. It involves many CNN-ELM models that can be trained asynchronously. Reduce process is the averaging of all CNN-ELM weights as final training result. This approach can save a lot of training time than single CNN-ELM models trained alone. This approach also increased the scalability of machine learning by combining scale out and scale up approaches. We verified our method in extended MNIST data set and not-MNIST data set experiment. However, it has some drawbacks by additional iteration learning parameters that need to be carefully taken and training data distribution that need to be carefully selected. Further researches to use more complex image data set are required.',\n",
       "  'Title: topic based influence computation in social networks under resource constraints\\nAbstract: As social networks are constantly changing and evolving, methods to analyze dynamic social networks are becoming more important in understanding social trends. However, due to the restrictions imposed by the social network service providers, the resources available to fetch the entire contents of a social network are typically very limited. As a result, analysis of dynamic social network data requires maintaining an approximate copy of the social network for each time period, locally. In this paper, we study the problem of dynamic network and text fetching with limited probing capacities, for identifying and maintaining influential users as the social network evolves. We propose an algorithm to probe the relationships (required for global influence computation) as well as posts (required for topic-based influence computation) of a limited number of users during each probing period, based on the influence trends and activities of the users. We infer the current network based on the newly probed user data and the last known version of the network maintained locally. Additionally, we propose to use link prediction methods to further increase the accuracy of our network inference. We employ PageRank as the metric for influence computation. We illustrate how the proposed solution maintains accurate PageRank scores for computing global influence, and topic-sensitive weighted PageRank scores for topic-based influence. The latter relies on a topic-based network constructed via weights determined by semantic analysis of posts and their sharing statistics. We evaluate the effectiveness of our algorithms by comparing them with the true influence scores of the full and up-to-date version of the network, using data from the micro-blogging service Twitter. Results show that our techniques significantly outperform baseline methods (80% higher accuracy for network fetching and 77% for text fetching) and are superior to state-of-the-art techniques from the literature (21% higher accuracy).',\n",
       "  'Title: code definition analysis for call graph generation\\nAbstract: Enterprise level software is implemented using multi-layer architecture. These layers are often implemented using de-coupled solutions with millions of lines of code. Programmers often have to track and debug a function call from user interface layer to the data access layer while troubleshooting an issue. They have to inspect the code based on search results or use design documents to construct the call graph. This process is time consuming and laborious. The development environment tools are insufficient or confined to analyzing only the code in the loaded solution. This paper proposes a method to construct a call graph of the call across several layers of the code residing in different code bases to help programmers better understand the design and architecture of the software. The signatures of class, methods, and properties were evaluated and then matched against the code files. A graph of matching functions was created. The recursive search stopped when there were no matches or the data layer code was detected. The method resulted in 78.26% accuracy when compared with manual search.',\n",
       "  'Title: learning to reason with adaptive computation\\nAbstract: Multi-hop inference is necessary for machine learning systems to successfully solve tasks such as Recognising Textual Entailment and Machine Reading. In this work, we demonstrate the effectiveness of adaptive computation for learning the number of inference steps required for examples of different complexity and that learning the correct number of inference steps is difficult. We introduce the first model involving Adaptive Computation Time which provides a small performance benefit on top of a similar model without an adaptive component as well as enabling considerable insight into the reasoning process of the model.',\n",
       "  'Title: geometry of polysemy\\nAbstract: Vector representations of words have heralded a transformational approach to classical problems in NLP; the most popular example is word2vec. However, a single vector does not suffice to model the polysemous nature of many (frequent) words, i.e., words with multiple meanings. In this paper, we propose a three-fold approach for unsupervised polysemy modeling: (a) context representations, (b) sense induction and disambiguation and (c) lexeme (as a word and sense pair) representations. A key feature of our work is the finding that a sentence containing a target word is well represented by a low rank subspace, instead of a point in a vector space. We then show that the subspaces associated with a particular sense of the target word tend to intersect over a line (one-dimensional subspace), which we use to disambiguate senses using a clustering algorithm that harnesses the Grassmannian geometry of the representations. The disambiguation algorithm, which we call $K$-Grassmeans, leads to a procedure to label the different senses of the target word in the corpus -- yielding lexeme vector representations, all in an unsupervised manner starting from a large (Wikipedia) corpus in English. Apart from several prototypical target (word,sense) examples and a host of empirical studies to intuit and justify the various geometric representations, we validate our algorithms on standard sense induction and disambiguation datasets and present new state-of-the-art results.',\n",
       "  'Title: towards lifelong self supervision a deep learning direction for robotics\\nAbstract: Despite outstanding success in vision amongst other domains, many of the recent deep learning approaches have evident drawbacks for robots. This manuscript surveys recent work in the literature that pertain to applying deep learning systems to the robotics domain, either as means of estimation or as a tool to resolve motor commands directly from raw percepts. These recent advances are only a piece to the puzzle. We suggest that deep learning as a tool alone is insufficient in building a unified framework to acquire general intelligence. For this reason, we complement our survey with insights from cognitive development and refer to ideas from classical control theory, producing an integrated direction for a lifelong learning architecture.',\n",
       "  'Title: poseidon face from depth for driver pose estimation\\nAbstract: Fast and accurate upper-body and head pose estimation is a key task for automatic monitoring of driver attention, a challenging context characterized by severe illumination changes, occlusions and extreme poses. In this work, we present a new deep learning framework for head localization and pose estimation on depth images. The core of the proposal is a regression neural network, called POSEidon, which is composed of three independent convolutional nets followed by a fusion layer, specially conceived for understanding the pose by depth. In addition, to recover the intrinsic value of face appearance for understanding head position and orientation, we propose a new Face-from-Depth approach for learning image faces from depth. Results in face reconstruction are qualitatively impressive. We test the proposed framework on two public datasets, namely Biwi Kinect Head Pose and ICT-3DHP, and on Pandora, a new challenging dataset mainly inspired by the automotive setup. Results show that our method overcomes all recent state-of-art works, running in real time at more than 30 frames per second.',\n",
       "  'Title: signal to noise in matching markets\\nAbstract: In many matching markets, one side \"applies\" to the other, and these applications are often expensive and time-consuming (e.g. students applying to college). It is tempting to think that making the application process easier should benefit both sides of the market. After all, the applicants could submit more applications, and the recipients would have more applicants to choose from. In this paper, we propose and analyze a simple model to understand settings where both sides of the market suffer from increased number of applications. #R##N#The main insights of the paper are derived from quantifying the signal to noise tradeoffs in random matchings, as applications provide a signal of the applicants\\' preferences. When applications are costly the signal is stronger, as the act of making an application itself is meaningful. Therefore more applications may yield potentially better matches, but fewer applications create stronger signals for the receiving side to learn true preferences. #R##N#We derive analytic characterizations of the expected quality of stable matchings in a simple utility model where applicants have an overall quality, but also synergy with specific possible partners. Our results show how reducing application cost without introducing an effective signaling mechanism might lead to inefficiencies for both sides of the market.',\n",
       "  \"Title: tactics and tallies inferring voter preferences in the 2016 u s presidential primaries using sparse learning\\nAbstract: In this paper, we propose a web-centered framework to infer voter preferences for the 2016 U.S. presidential primaries. Using Twitter data collected from Sept. 2015 to March 2016, we first uncover the tweeting tactics of the candidates and then exploit the variations in the number of 'likes' to infer voters' preference. With sparse learning, we are able to reveal neutral topics as well as positive and negative ones. #R##N#Methodologically, we are able to achieve a higher predictive power with sparse learning. Substantively, we show that for Hillary Clinton the (only) positive issue area is women's rights. We demonstrate that Hillary Clinton's tactic of linking herself to President Obama resonates well with her supporters but the same is not true for Bernie Sanders. In addition, we show that Donald Trump is a major topic for all the other candidates, and that the women's rights issue is equally emphasized in Sanders' campaign as in Clinton's.\",\n",
       "  'Title: distributed algorithm for collision avoidance at road intersections in the presence of communication failures\\nAbstract: Vehicle-to-vehicle (V2V) communication is a crucial component of the future autonomous driving systems since it enables improved awareness of the surrounding environment, even without extensive processing of sensory information. However, V2V communication is prone to failures and delays, so a distributed fault-tolerant approach is required for safe and efficient transportation. In this paper, we focus on the intersection crossing (IC) problem with autonomous vehicles that cooperate via V2V communications, and propose a novel distributed IC algorithm that can handle an unknown number of communication failures. Our analysis shows that both safety and liveness requirements are satisfied in all realistic situations. We also found, based on a real data set, that the crossing delay is only slightly increased even in the presence of highly correlated failures.',\n",
       "  'Title: on the content security policy violations due to the same origin policy\\nAbstract: Modern browsers implement different security policies such as the Content Security Policy (CSP), a mechanism designed to mitigate popular web vulnerabilities, and the Same Origin Policy (SOP), a mechanism that governs interactions between resources of web pages.#R##N##R##N#In this work, we describe how CSP may be violated due to the SOP when a page contains an embedded iframe from the same origin. We analyse 1 million pages from 10,000 top Alexa sites and report that at least 31.1% of current CSP-enabled pages are potentially vulnerable to CSP violations. Further considering real-world situations where those pages are involved in same-origin nested browsing contexts, we found that in at least 23.5% of the cases, CSP violations are possible.#R##N##R##N#During our study, we also identified a divergence among browsers implementations in the enforcement of CSP in srcdoc sandboxed iframes, which actually reveals a problem in Gecko-based browsers CSP implementation. To ameliorate the problematic conflicts of the security mechanisms, we discuss measures to avoid CSP violations.',\n",
       "  'Title: a new backpressure algorithm for joint rate control and routing with vanishing utility optimality gaps and finite queue lengths\\nAbstract: The backpressure algorithm has been widely used as a distributed solution to the problem of joint rate control and routing in multi-hop data networks. By controlling a parameter $V$ in the algorithm, the backpressure algorithm can achieve an arbitrarily small utility optimality gap. However, this in turn brings in a large queue length at each node and hence causes large network delay. This phenomenon is known as the fundamental utility-delay tradeoff. The best known utility-delay tradeoff for general networks is $[O(1/V), O(V)]$ and is attained by a backpressure algorithm based on a drift-plus-penalty technique. This may suggest that to achieve an arbitrarily small utility optimality gap, the existing backpressure algorithms necessarily yield an arbitrarily large queue length. However, this paper proposes a new backpressure algorithm that has a vanishing utility optimality gap, so utility converges to exact optimality as the algorithm keeps running, while queue lengths are bounded throughout by a finite constant. The technique uses backpressure and drift concepts with a new method for convex programming.',\n",
       "  'Title: optimal placement delivery arrays\\nAbstract: In wireless networks, coded caching scheme is an effective technique to reduce network congestion during peak traffic times. A $(K,F,Z,S)$ placement delivery array ($(K,F,Z,S)$PDA in short) can be used to design a coded caching scheme with the delivery rate $S/F$ during the peak traffic times. Clearly in order to minimize delivery rate, we only need to consider a $(K,F,Z,S)$PDA with minimum $S$. For the fixed parameters $K$, $F$ and $Z$, a $(K,F,Z,S)$PDA with the minimum $S$ is called optimal. So it is meaningful to study optimal PDAs. #R##N#There are some known PDAs constructed by combinatorial design theory, hypergraphs and so on. However there is only one class of optimal PDAs (IEEE Trans. Inf. Theory 60:2856-2867, 2014) constructed by Ali and Niesen. We will focus on constructing optimal PDAs. In this paper, two classes of lower bounds on the value of $S$ in a $(K,F,Z,S)$PDA are derived first. Then two classes of recursive constructions are proposed. Consequently (i) optimal PDAs with $Z=1$ and $F-1$ for any positive integers $K$ and $F$ are obtained; (ii) several infinite classes of optimal PDAs for some $K$, $F$ and $Z$ are constructed. Finally more existence of optimal PDAs with $Z=F-2$ are given.',\n",
       "  'Title: factored contextual policy search with bayesian optimization\\nAbstract: Scarce data is a major challenge to scaling robot learning to truly complex tasks, as we need to generalize locally learned policies over different task contexts. Contextual policy search offers data-efficient learning and generalization by explicitly conditioning the policy on a parametric context space. In this paper, we further structure the contextual policy representation. We propose to factor contexts into two components: target contexts that describe the task objectives, e.g. target position for throwing a ball; and environment contexts that characterize the environment, e.g. initial position or mass of the ball. Our key observation is that experience can be directly generalized over target contexts. We show that this can be easily exploited in contextual policy search algorithms. In particular, we apply factorization to a Bayesian optimization approach to contextual policy search both in sampling-based and active learning settings. Our simulation results show faster learning and better generalization in various robotic domains. See our supplementary video: this https URL.',\n",
       "  'Title: optimal inference in crowdsourced classification via belief propagation\\nAbstract: Crowdsourcing systems are popular for solving large-scale labelling tasks with low-paid workers. We study the problem of recovering the true labels from the possibly erroneous crowdsourced labels under the popular Dawid-Skene model. To address this inference problem, several algorithms have recently been proposed, but the best known guarantee is still significantly larger than the fundamental limit. We close this gap by introducing a tighter lower bound on the fundamental limit and proving that Belief Propagation (BP) exactly matches this lower bound. The guaranteed optimality of BP is the strongest in the sense that it is information-theoretically impossible for any other algorithm to correctly label a larger fraction of the tasks. Experimental results suggest that BP is close to optimal for all regimes considered and improves upon competing state-of-the-art algorithms.',\n",
       "  'Title: localization in mobile networks via virtual convex hulls\\nAbstract: In this paper, we develop a \\\\textit{distributed} algorithm to localize an arbitrary number of agents moving in a bounded region of interest. We assume that the network contains \\\\textit{at least one} agent with known location (hereinafter referred to as an anchor), and each agent measures a noisy version of its motion and the distances to the nearby agents. We provide a~\\\\emph{geometric approach}, which allows each agent to: (i) continually update the distances to the locations where it has exchanged information with the other nodes in the past; and (ii) measure the distance between a neighbor and any such locations. Based on this approach, we provide a \\\\emph{linear update} to find the locations of an arbitrary number of mobile agents when they follow some convexity in their deployment and motion. #R##N#Since the agents are mobile, they may not be able to find nearby nodes (agents and/or anchors) to implement a distributed algorithm. To address this issue, we introduce the notion of a \\\\emph{virtual convex hull} with the help of the aforementioned geometric approach. In particular, each agent keeps track of a virtual convex hull of other nodes, which may not physically exist, and updates its location with respect to its neighbors in the virtual hull. We show that the corresponding localization algorithm, in the absence of noise, can be abstracted as a Linear Time-Varying (LTV) system, with non-deterministic system matrices, which asymptotically tracks the true locations of the agents. We provide simulations to verify the analytical results and evaluate the performance of the algorithm in the presence of noise on the motion as well as on the distance measurements.',\n",
       "  'Title: flexonc joint cooperative forwarding and network coding with precise encoding conditions\\nAbstract: In recent years, network coding has emerged as an innovative method that helps a wireless network approach its maximum capacity, by combining multiple unicasts in one broadcast. However, the majority of research conducted in this area is yet to fully utilize the broadcasting nature of wireless networks, and still assumes fixed route between the source and destination that every packet should travel through. This assumption not only limits coding opportunities, but can also cause buffer overflow in some specific intermediate nodes. Although some studies considered scattering of the flows dynamically in the network, they still face some limitations. This paper explains pros and cons of some prominent research in network coding and proposes a Flexible and Opportunistic Network Coding scheme (FlexONC) as a solution to such issues. Furthermore, this research discovers that the conditions used in previous studies to combine packets of different flows are overly optimistic and would affect the network performance adversarially. Therefore, we provide a more accurate set of rules for packet encoding. The experimental results show that FlexONC outperforms previous methods especially in networks with high bit error rate, by better utilizing redundant packets spread in the network.',\n",
       "  'Title: neural networks based eeg speech models\\nAbstract: In this paper, we describe three neural network (NN) based EEG-Speech (NES) models that map the unspoken EEG signals to the corresponding phonemes. Instead of using conventional feature extraction techniques, the proposed NES models rely on graphic learning to project both EEG and speech signals into deep representation feature spaces. This NN based linear projection helps to realize multimodal data fusion (i.e., EEG and acoustic signals). It is convenient to construct the mapping between unspoken EEG signals and phonemes. Specifically, among three NES models, two augmented models (i.e., IANES-B and IANES-G) include spoken EEG signals as either bias or gate information to strengthen the feature learning and translation of unspoken EEG signals. A combined unsupervised and supervised training is implemented stepwise to learn the mapping for all three NES models. To enhance the computational performance, three way factored NN training technique is applied to IANES-G model. Unlike many existing methods, our augmented NES models incorporate spoken-EEG signals that can efficiently suppress the artifacts in unspoken-EEG signals. Experimental results reveal that all three proposed NES models outperform the baseline SVM method, whereas IANES-G demonstrates the best performance on speech recovery and classification task comparatively.',\n",
       "  'Title: fairness aware caching and radio resource allocation for the downlink of multi cell ofdma systems\\nAbstract: The unprecedented growth of internet contents, specially social media, invoking a challenge to the load of cellular networks. In addition, nowadays, the demands of quality of experience (QoE) became a more practical norm in contrast of quality of service (QoS), in which downloading delay is an important factor of it. To satisfy this demand, we propose two resource allocation (RA) algorithms to optimize the place of social media in the cache of the Base stations (BSs) and radio resources (i.e., transmit powers and subcarriers), jointly in a multi-cell Orthogonal Frequency Division Multiple Access (OFDMA) based system. In the first scheme, the total downloading delay of the network is minimized, while in the second scheme, a fairness-aware scheme is proposed in which the maximum downloading delays is minimized. We propose iterative algorithms to solve each problem, where the content placement problem and the joint subcarrier and transmit power allocation problem will be iteratively optimized. We also prove that the proposed approaches converges to a near-optimal solution, as well as the number of iterations increases.',\n",
       "  'Title: distributed infrastructure inspection path planning subject to time constraints\\nAbstract: Within this paper, the problem of 3D structural inspection path planning for distributed infrastructure using aerial robots that are subject to time constraints is addressed. The proposed algorithm handles varying spatial properties of the infrastructure facilities, accounts for their different importance and exploration function and computes an overall inspection path of high inspection reward while respecting the robot endurance or mission time constraints as well as the vehicle dynamics and sensor limitations. To achieve its goal, it employs an iterative, 3-step optimization strategy at each iteration of which it first randomly samples a set of possible structures to visit, subsequently solves the derived traveling salesman problem and computes the travel costs, while finally it samples and assigns inspection times to each structure and evaluates the total inspection reward. For the derivation of the inspection paths per each independent facility, it interfaces a path planner dedicated to the 3D coverage of single structures. The resulting algorithm properties, computational performance and path quality are evaluated using simulation studies as well as experimental test-cases employing a multirotor micro aerial vehicle.',\n",
       "  'Title: robust multigrid for cartesian interior penalty dg formulations of the poisson equation in 3d\\nAbstract: We present a polynomial multigrid method for the nodal interior penalty formulation of the Poisson equation on three-dimensional Cartesian grids. Its key ingredient is a weighted overlapping Schwarz smoother operating on element-centered subdomains. The MG method reaches superior convergence rates corresponding to residual reductions of about two orders of magnitude within a single V(1,1) cycle. It is robust with respect to the mesh size and the ansatz order, at least up to P = 32. Rigorous exploitation of tensor-product factorization yields a computational complexity of O(PN) for N unknowns, whereas numerical experiments indicate even linear runtime scaling. Moreover, by allowing adjustable subdomain overlaps and adding Krylov acceleration, the method proved feasible for anisotropic grids with element aspect ratios up to 48.',\n",
       "  'Title: smpost parts of speech tagger for code mixed indic social media text\\nAbstract: Use of social media has grown dramatically during the last few years. Users follow informal languages in communicating through social media. The language of communication is often mixed in nature, where people transcribe their regional language with English and this technique is found to be extremely popular. Natural language processing (NLP) aims to infer the information from these text where Part-of-Speech (PoS) tagging plays an important role in getting the prosody of the written text. For the task of PoS tagging on Code-Mixed Indian Social Media Text, we develop a supervised system based on Conditional Random Field classifier. In order to tackle the problem effectively, we have focused on extracting rich linguistic features. We participate in three different language pairs, ie. English-Hindi, English-Bengali and English-Telugu on three different social media platforms, Twitter, Facebook & WhatsApp. The proposed system is able to successfully assign coarse as well as fine-grained PoS tag labels for a given a code-mixed sentence. Experiments show that our system is quite generic that shows encouraging performance levels on all the three language pairs in all the domains.',\n",
       "  \"Title: quantifying search bias investigating sources of bias for political searches in social media\\nAbstract: Search systems in online social media sites are frequently used to find information about ongoing events and people. For topics with multiple competing perspectives, such as political events or political candidates, bias in the top ranked results significantly shapes public opinion. However, bias does not emerge from an algorithm alone. It is important to distinguish between the bias that arises from the data that serves as the input to the ranking system and the bias that arises from the ranking system itself. In this paper, we propose a framework to quantify these distinct biases and apply this framework to politics-related queries on Twitter. We found that both the input data and the ranking system contribute significantly to produce varying amounts of bias in the search results and in different ways. We discuss the consequences of these biases and possible mechanisms to signal this bias in social media search systems' interfaces.\",\n",
       "  'Title: the submodular secretary problem under a cardinality constraint and with limited resources\\nAbstract: We study the submodular secretary problem subject to a cardinality constraint, in long-running scenarios, or under resource constraints. In these scenarios the resources consumed by the algorithm should not grow with the input size, and the online selection algorithm should be an anytime algorithm. We propose a $0.1933$-competitive anytime algorithm, which performs only a single evaluation of the marginal contribution for each observed item, and requires a memory of order only $k$ (up to logarithmic factors), where $k$ is the cardinality constraint. The best competitive ratio for this problem known so far under these constraints is $\\\\frac{e-1}{e^2+e} \\\\approx 0.1700$ (Feldman et al., 2011). Our algorithm is based on the observation that information collected during times in which no good items were selected, can be used to improve the subsequent probability of selection success. The improvement is obtained by using an adaptive selection strategy, which is a solution to a stand-alone online selection problem. We develop general tools for analyzing this algorithmic framework, which we believe will be useful also for other online selection problems.',\n",
       "  'Title: statistical anomaly detection via composite hypothesis testing for markov models\\nAbstract: Under Markovian assumptions, we leverage a central limit theorem for the empirical measure in the test statistic of the composite hypothesis Hoeffding test so as to establish weak convergence results for the test statistic, and, thereby, derive a new estimator for the threshold needed by the test. We first show the advantages of our estimator over an existing estimator by conducting extensive numerical experiments. We find that our estimator controls better for false alarms while maintaining satisfactory detection probabilities. We then apply the Hoeffding test with our threshold estimator to detect anomalies in two distinct applications domains: One in communication networks and the other in transportation networks. The former application seeks to enhance cyber security and the latter aims at building smarter transportation systems in cities.',\n",
       "  'Title: sparse representation based multi sensor image fusion a review\\nAbstract: As a result of several successful applications in computer vision and image processing, sparse representation (SR) has attracted significant attention in multi-sensor image fusion. Unlike the traditional multiscale transforms (MSTs) that presume the basis functions, SR learns an over-complete dictionary from a set of training images for image fusion, and it achieves more stable and meaningful representations of the source images. By doing so, the SR-based fusion methods generally outperform the traditional MST-based image fusion methods in both subjective and objective tests. In addition, they are less susceptible to mis-registration among the source images, thus facilitating the practical applications. This survey paper proposes a systematic review of the SR-based multi-sensor image fusion literature, highlighting the pros and cons of each category of approaches. Specifically, we start by performing a theoretical investigation of the entire system from three key algorithmic aspects, (1) sparse representation models; (2) dictionary learning methods; and (3) activity levels and fusion rules. Subsequently, we show how the existing works address these scientific problems and design the appropriate fusion rules for each application, such as multi-focus image fusion and multi-modality (e.g., infrared and visible) image fusion. At last, we carry out some experiments to evaluate the impact of these three algorithmic components on the fusion performance when dealing with different applications. This article is expected to serve as a tutorial and source of reference for researchers preparing to enter the field or who desire to employ the sparse representation theory in other fields.',\n",
       "  'Title: communication efficient algorithms for distributed stochastic principal component analysis\\nAbstract: We study the fundamental problem of Principal Component Analysis in a statistical distributed setting in which each machine out of $m$ stores a sample of $n$ points sampled i.i.d. from a single unknown distribution. We study algorithms for estimating the leading principal component of the population covariance matrix that are both communication-efficient and achieve estimation error of the order of the centralized ERM solution that uses all $mn$ samples. On the negative side, we show that in contrast to results obtained for distributed estimation under convexity assumptions, for the PCA objective, simply averaging the local ERM solutions cannot guarantee error that is consistent with the centralized ERM. We show that this unfortunate phenomena can be remedied by performing a simple correction step which correlates between the individual solutions, and provides an estimator that is consistent with the centralized ERM for sufficiently-large $n$. We also introduce an iterative distributed algorithm that is applicable in any regime of $n$, which is based on distributed matrix-vector products. The algorithm gives significant acceleration in terms of communication rounds over previous distributed algorithms, in a wide regime of parameters.',\n",
       "  'Title: secure virtual network embedding in a multi cloud environment\\nAbstract: Recently-proposed virtualization platforms give cloud users the freedom to specify their network topologies and addressing schemes. These platforms have, however, been targeting a single datacenter of a cloud provider, which is insufficient to support (critical) applications that need to be deployed across multiple trust domains while enforcing diverse security requirements. This paper addresses this problem by presenting a novel solution for a central component of network virtualization -- the online network embedding, which finds efficient mappings of virtual networks requests onto the substrate network. Our solution considers security as a first class citizen, enabling the definition of flexible policies in three central areas: on the communications, where alternative security compromises can be explored (e.g., encryption); on the computations, supporting redundancy if necessary while capitalizing on hardware assisted trusted executions; across multiples clouds, including public and private facilities, with the associated trust levels. We formulate the solution as a Mixed Integer Linear Program (MILP), and evaluate our proposal against the most commonly used alternative. Our analysis gives insight into the trade-offs involved with the inclusion of security and trust into network virtualization, providing evidence that this notion may enhance profits under the appropriate cost models.',\n",
       "  'Title: a time series distance measure for efficient clustering of input output signals by their underlying dynamics\\nAbstract: Starting from a dataset with input/output time series generated by multiple deterministic linear dynamical systems, this letter tackles the problem of automatically clustering these time series. We propose an extension to the so-called Martin cepstral distance, that allows to efficiently cluster these time series, and apply it to simulated electrical circuits data. Traditionally, two ways of handling the problem are used. The first class of methods employs a distance measure on time series (e.g., Euclidean, dynamic time warping) and a clustering technique (e.g.,    $k$   -means,    $k$   -medoids, and hierarchical clustering) to find natural groups in the dataset. It is, however, often not clear whether these distance measures effectively take into account the specific temporal correlations in these time series. The second class of methods uses the input/output data to identify a dynamic system using an identification scheme, and then applies a model norm-based distance (e.g.,    $H_{2}$    and    $H_\\\\infty $   ) to find out which systems are similar. This, however, can be very time consuming for large amounts of long time series data. We show that the new distance measure presented in this letter performs as good as when every input/output pair is modeled explicitly, but remains computationally much less complex. The complexity of calculating this distance between two time series of length    $N$    is    $\\\\mathcal {O}(N\\\\log {N})$   .',\n",
       "  'Title: unsupervised domain adaptation using approximate label matching\\nAbstract: Domain adaptation addresses the problem created when training data is generated by a so-called source distribution, but test data is generated by a significantly different target distribution. In this work, we present approximate label matching (ALM), a new unsupervised domain adaptation technique that creates and leverages a rough labeling on the test samples, then uses these noisy labels to learn a transformation that aligns the source and target samples. We show that the transformation estimated by ALM has favorable properties compared to transformations estimated by other methods, which do not use any kind of target labeling. Our model is regularized by requiring that a classifier trained to discriminate source from transformed target samples cannot distinguish between the two. We experiment with ALM on simulated and real data, and show that it outperforms techniques commonly used in the field.',\n",
       "  'Title: robust phase retrieval via admm with outliers\\nAbstract: An outlier-resistance phase retrieval algorithm based on alternating direction method of multipliers (ADMM) is devised in this letter. Instead of the widely used least squares criterion that is only optimal for Gaussian noise environment, we adopt the least absolute deviation criterion to enhance the robustness against outliers. Considering both intensity- and amplitude-based observation models, the framework of ADMM is developed to solve the resulting non-differentiable optimization problems. It is demonstrated that the core subproblem of ADMM is the proximity operator of the L1-norm, which can be computed efficiently by soft-thresholding in each iteration. Simulation results are provided to validate the accuracy and efficiency of the proposed approach compared to the existing schemes.',\n",
       "  'Title: belief propagation in conditional rbms for structured prediction\\nAbstract: Restricted Boltzmann machines~(RBMs) and conditional RBMs~(CRBMs) are popular models for a wide range of applications. In previous work, learning on such models has been dominated by contrastive divergence~(CD) and its variants. Belief propagation~(BP) algorithms are believed to be slow for structured prediction on conditional RBMs~(e.g., Mnih et al. [2011]), and not as good as CD when applied in learning~(e.g., Larochelle et al. [2012]). In this work, we present a matrix-based implementation of belief propagation algorithms on CRBMs, which is easily scalable to tens of thousands of visible and hidden units. We demonstrate that, in both maximum likelihood and max-margin learning, training conditional RBMs with BP as the inference routine can provide significantly better results than current state-of-the-art CD methods on structured prediction problems. We also include practical guidelines on training CRBMs with BP, and some insights on the interaction of learning and inference algorithms for CRBMs.',\n",
       "  'Title: a consensus based secondary control layer for stable current sharing and voltage balancing in dc microgrids\\nAbstract: In this paper, we propose a secondary consensus-based control layer for current sharing and voltage balancing in DC microGrids (mGs). To this purpose, we assume that Distributed Generation Units (DGUs) are equipped with decentralized primary controllers guaranteeing voltage stability. This goal can be achieved using, for instance, Plug-and-Play (PnP) regulators. We analyze the behavior of the closed-loop mG by approximating local primary control loops with either unitary gains or first-order transfer functions. Besides proving exponential stability, current sharing, and voltage balancing, we describe how to design secondary controllers in a PnP fashion when DGUs are added or removed. Theoretical results are complemented by simulations, using a 7-DGUs mG implemented in Simulink/PLECS, and experiments on a 3-DGUs mG.',\n",
       "  'Title: sce a manifold regularized set covering method for data partitioning\\nAbstract: Cluster analysis plays a very important role in data analysis. In these years, cluster ensemble, as a cluster analysis tool, has drawn much attention for its robustness, stability, and accuracy. Many efforts have been done to combine different initial clustering results into a single clustering solution with better performance. However, they neglect the structure information of the raw data in performing the cluster ensemble. In this paper, we propose a Structural Cluster Ensemble (SCE) algorithm for data partitioning formulated as a set-covering problem. In particular, we construct a Laplacian regularized objective function to capture the structure information among clusters. Moreover, considering the importance of the discriminative information underlying in the initial clustering results, we add a discriminative constraint into our proposed objective function. Finally, we verify the performance of the SCE algorithm on both synthetic and real data sets. The experimental results show the effectiveness of our proposed method SCE algorithm.',\n",
       "  'Title: coupling of crop assignment and vehicle routing for harvest planning in agriculture\\nAbstract: A method for harvest planning based on the coupling of crop assignment with vehicle routing is presented. Given multiple fields (up to hundreds), a path network connecting fields, multiple depots at which a number of harvesters are initially located, the main question addressed is: which crop out of a set of different crops to assign to each field. It must be answered by every farm manager at the beginning of every yearly work-cycle starting with plant seeding and ending with harvesting. Rather than solving a pure assigment problem, we also account for connectivity between fields. In practice, fields are often located distant apart. Traveling costs of machinery and limited harvesting windows demand optimized operation and route planning. The proposed method outputs crop assignment to fields and simultaneously determines an optimized sequence in which to service fields of the same crop during harvest. The described scenario is of particular relevance for larger farms and groups of farms that collaborate and share machinery. We derive integer programming (IP) based exact algorithms. For large numbers of fields, where exact algorithms are not tractable anymore, elements of clustering and the solution of local Traveling Salesman Problems (TSP) are added, thereby rendering the method heuristic, but also large-scale applicable.',\n",
       "  \"Title: why smart appliances may result in a stupid energy grid\\nAbstract: This article discusses unexpected consequences of idealistic conceptions about the modernization of power grids. We will focus our analysis on demand-response policies based on automatic decisions by the so-called smart home appliances. Following the usual design approach, each individual appliance has access to a universal signal (e.g. grid frequency or electricity price) that is believed to indicate the system state. Such information is then used as the basis of the appliances' individual decisions. While each single device has a negligible impact in the system, the aggregate effect of the distributed appliances' reactions is expect to bring improvements in the system efficiency; this effect is the demand-response policy goal. The smartness of such an ideal system, composed by isolated appliances with their individual decisions, but connected in the same physical grid, may worsen the system stability. This first-sight undesirable outcome comes as a consequence of synchronization among agents that are subject to the same signal. We argue that this effect is in fact byproduct of methodological choices, which are many times implicit. To support this claim, we employ a different approach that understands the electricity system as constituted by physical, informational and regulatory (networked and structured) layers that cannot be reduced to only one or two of them, but have to be viewed as an organic whole. By classifying its structure under this lens, more appropriate management tools can be designed by looking at the system totality in action. Two examples are provided to illustrate the strength of this modeling.\",\n",
       "  \"Title: lesionseg semantic segmentation of skin lesions using deep convolutional neural network\\nAbstract: We present a method for skin lesion segmentation for the ISIC 2017 Skin Lesion Segmentation Challenge. Our approach is based on a Fully Convolutional Network architecture which is trained end to end, from scratch, on a limited dataset. Our semantic segmentation architecture utilizes several recent innovations in particularly in the combined use of (i) use of \\\\emph{atrous} convolutions to increase the effective field of view of the network's receptive field without increasing the number of parameters, (ii) the use of network-in-network $1\\\\times1$ convolution layers to increase network capacity without incereasing the number of parameters and (iii) state-of-art super-resolution upsampling of predictions using subpixel CNN layers for accurate and efficient upsampling of predictions. We achieved a IOU score of 0.642 on the validation set provided by the organisers.\",\n",
       "  \"Title: applying deep machine learning for psycho demographic profiling of internet users using o c e a n model of personality\\nAbstract: In the modern era, each Internet user leaves enormous amounts of auxiliary digital residuals (footprints) by using a variety of on-line services. All this data is already collected and stored for many years. In recent works, it was demonstrated that it's possible to apply simple machine learning methods to analyze collected digital footprints and to create psycho-demographic profiles of individuals. However, while these works clearly demonstrated the applicability of machine learning methods for such an analysis, created simple prediction models still lacks accuracy necessary to be successfully applied for practical needs. We have assumed that using advanced deep machine learning methods may considerably increase the accuracy of predictions. We started with simple machine learning methods to estimate basic prediction performance and moved further by applying advanced methods based on shallow and deep neural networks. Then we compared prediction power of studied models and made conclusions about its performance. Finally, we made hypotheses how prediction accuracy can be further improved. As result of this work, we provide full source code used in the experiments for all interested researchers and practitioners in corresponding GitHub repository. We believe that applying deep machine learning for psycho-demographic profiling may have an enormous impact on the society (for good or worse) and provides means for Artificial Intelligence (AI) systems to better understand humans by creating their psychological profiles. Thus AI agents may achieve the human-like ability to participate in conversation (communication) flow by anticipating human opponents' reactions, expectations, and behavior.#R##N#By providing full source code of our research we hope to intensify further research in the area by the wider circle of scholars.\",\n",
       "  'Title: parsimonious data how a single facebook like predicts voting behavior in multiparty systems\\nAbstract: This study shows how liking politicians’ public Facebook posts can be used as an accurate measure for predicting present-day voter intention in a multiparty system. We highlight that a few, but selective digital traces produce prediction accuracies that are on par or even greater than most current approaches based upon bigger and broader datasets. Combining the online and offline, we connect a subsample of surveyed respondents to their public Facebook activity and apply machine learning classifiers to explore the link between their political liking behaviour and actual voting intention. Through this work, we show that even a single selective Facebook like can reveal as much about political voter intention as hundreds of heterogeneous likes. Further, by including the entire political like history of the respondents, our model reaches prediction accuracies above previous multiparty studies (60–70%).#R##N##R##N#The main contribution of this paper is to show how public like-activity on Facebook allows political profiling of individual users in a multiparty system with accuracies above previous studies. Beside increased accuracies, the paper shows how such parsimonious measures allows us to generalize our findings to the entire population of a country and even across national borders, to other political multiparty systems. The approach in this study relies on data that are publicly available, and the simple setup we propose can with some limitations, be generalized to millions of users in other multiparty systems.',\n",
       "  'Title: simplified stochastic feedforward neural networks\\nAbstract: It has been believed that stochastic feedforward neural networks (SFNNs) have several advantages beyond deterministic deep neural networks (DNNs): they have more expressive power allowing multi-modal mappings and regularize better due to their stochastic nature. However, training large-scale SFNN is notoriously harder. In this paper, we aim at developing efficient training methods for SFNN, in particular using known architectures and pre-trained parameters of DNN. To this end, we propose a new intermediate stochastic model, called Simplified-SFNN, which can be built upon any baseline DNNand approximates certain SFNN by simplifying its upper latent units above stochastic ones. The main novelty of our approach is in establishing the connection between three models, i.e., DNN->Simplified-SFNN->SFNN, which naturally leads to an efficient training procedure of the stochastic models utilizing pre-trained parameters of DNN. Using several popular DNNs, we show how they can be effectively transferred to the corresponding stochastic models for both multi-modal and classification tasks on MNIST, TFD, CASIA, CIFAR-10, CIFAR-100 and SVHN datasets. In particular, we train a stochastic model of 28 layers and 36 million parameters, where training such a large-scale stochastic network is significantly challenging without using Simplified-SFNN',\n",
       "  'Title: middleware technologies for cloud of things a survey\\nAbstract: Abstract   The next wave of communication and applications will rely on new services provided by the Internet of Things which is becoming an important aspect in human and machines future. IoT services are a key solution for providing smart environments in homes, buildings, and cities. In the era of massive number of connected things and objects with high growth rate, several challenges have been raised, such as management, aggregation, and storage for big produced data. To address some of these issues, cloud computing emerged to the IoT as Cloud of Things (CoT), which provides virtually unlimited cloud services to enhance the large-scale IoT platforms. There are several factors to be considered in the design and implementation of a CoT platform. One of the most important and challenging problems is the heterogeneity of different objects. This problem can be addressed by deploying a suitable “middleware” which sits between things and applications as a reliable platform for communication among things with different interfaces, operating systems, and architectures. The main aim of this paper is to study the middleware technologies for CoT. Toward this end, we first present the main features and characteristics of middlewares. Next, we study different architecture styles and service domains. Then, we present several middlewares that are suitable for CoT-based platforms and finally, a list of current challenges and issues in the design of CoT-based middlewares is discussed.',\n",
       "  'Title: a robust blind watermarking using convolutional neural network\\nAbstract: This paper introduces a blind watermarking based on a convolutional neural network (CNN). We propose an iterative learning framework to secure robustness of watermarking. One loop of learning process consists of the following three stages: Watermark embedding, attack simulation, and weight update. We have learned a network that can detect a 1-bit message from a image sub-block. Experimental results show that this learned network is an extension of the frequency domain that is widely used in existing watermarking scheme. The proposed scheme achieved robustness against geometric and signal processing attacks with a learning time of one day.',\n",
       "  'Title: on predicting geolocation of tweets using convolutional neural networks\\nAbstract: In many Twitter studies, it is important to know where a tweet came from in order to use the tweet content to study regional user behavior. However, researchers using Twitter to understand user behavior often lack sufficient geo-tagged data. Given the huge volume of Twitter data there is a need for accurate automated geolocating solutions. Herein, we present a new method to predict a Twitter user’s location based on the information in a single tweet. We integrate text and user profile meta-data into a single model using a convolutional neural network. Our experiments demonstrate that our neural model substantially outperforms baseline methods, achieving 52.8% accuracy and 92.1% accuracy on city-level and country-level prediction respectively.',\n",
       "  'Title: a general private information retrieval scheme for mds coded databases with colluding servers\\nAbstract: The problem of private information retrieval gets renewed attentions in recent years due to its information-theoretic reformulation and applications in distributed storage systems. PIR capacity is the maximal number of bits privately retrieved per one bit of downloaded bit. The capacity has been fully solved for some degenerating cases. For a general case where the database is both coded and colluded, the exact capacity remains unknown. We build a general private information retrieval scheme for MDS coded databases with colluding servers. Our scheme achieves the rate $(1+R+R^2+\\\\cdots+R^{M-1})$, where $R=1-\\\\frac{{{N-T}\\\\choose K}}{N\\\\choose K}$. Compared to existing PIR schemes, our scheme performs better for a certain range of parameters and is suitable for any underlying MDS code used in the distributed storage system.',\n",
       "  'Title: extending word problems in deterministic finite automata\\nAbstract: A word $w$ is \\\\emph{extending} a subset of states $S$ of a deterministic finite automaton, if the set of states mapped to $S$ by $w$ (the preimage of $S$ under the action of $w$) is larger than $S$. This notion together with its variations has particular importance in the field of synchronizing automata, where a number of methods and algorithms rely on finding (short) extending words. In this paper we study the complexity of several variants of extending word problems: deciding whether there exists an extending word, an extending word that extends to the whole set of states, a word avoiding a state, and a word that either extends or shrinks the subset. Additionally, we study the complexity of these problems when an upper bound on the length of the word is also given, and we consider the subclasses of strongly connected, synchronizing, binary, and unary automata. We show either hardness or polynomial algorithms for the considered variants.',\n",
       "  'Title: optimal algorithms for hitting topological minors on graphs of bounded treewidth\\nAbstract: For a fixed collection of graphs ${\\\\cal F}$, the ${\\\\cal F}$-M-DELETION problem consists in, given a graph $G$ and an integer $k$, decide whether there exists $S \\\\subseteq V(G)$ with $|S| \\\\leq k$ such that $G \\\\setminus S$ does not contain any of the graphs in ${\\\\cal F}$ as a minor. We are interested in the parameterized complexity of ${\\\\cal F}$-M-DELETION when the parameter is the treewidth of $G$, denoted by $tw$. Our objective is to determine, for a fixed ${\\\\cal F}$, the smallest function $f_{{\\\\cal F}}$ such that ${\\\\cal F}$-M-DELETION can be solved in time $f_{{\\\\cal F}}(tw) \\\\cdot n^{O(1)}$ on $n$-vertex graphs. Using and enhancing the machinery of boundaried graphs and small sets of representatives introduced by Bodlaender et al. [J ACM, 2016], we prove that when all the graphs in ${\\\\cal F}$ are connected and at least one of them is planar, then $f_{{\\\\cal F}}(w) = 2^{O (w \\\\cdot\\\\log w)}$. When ${\\\\cal F}$ is a singleton containing a clique, a cycle, or a path on $i$ vertices, we prove the following asymptotically tight bounds: #R##N#$\\\\bullet$ $f_{\\\\{K_4\\\\}}(w) = 2^{\\\\Theta (w \\\\cdot \\\\log w)}$. #R##N#$\\\\bullet$ $f_{\\\\{C_i\\\\}}(w) = 2^{\\\\Theta (w)}$ for every $i \\\\leq 4$, and $f_{\\\\{C_i\\\\}}(w) = 2^{\\\\Theta (w \\\\cdot\\\\log w)}$ for every $i \\\\geq 5$. #R##N#$\\\\bullet$ $f_{\\\\{P_i\\\\}}(w) = 2^{\\\\Theta (w)}$ for every $i \\\\leq 4$, and $f_{\\\\{P_i\\\\}}(w) = 2^{\\\\Theta (w \\\\cdot \\\\log w)}$ for every $i \\\\geq 6$. #R##N#The lower bounds hold unless the Exponential Time Hypothesis fails, and the superexponential ones are inspired by a reduction of Marcin Pilipczuk [Discrete Appl Math, 2016]. The single-exponential algorithms use, in particular, the rank-based approach introduced by Bodlaender et al. [Inform Comput, 2015]. We also consider the version of the problem where the graphs in ${\\\\cal F}$ are forbidden as topological minors, and prove that essentially the same set of results holds.',\n",
       "  'Title: query adaptive video summarization via quality aware relevance estimation\\nAbstract: Although the problem of automatic video summarization has recently received a lot of attention, the problem of creating a video summary that also highlights elements relevant to a search query has been less studied. We address this problem by posing query-relevant summarization as a video frame subset selection problem, which lets us optimise for summaries which are simultaneously diverse, representative of the entire video, and relevant to a text query. We quantify relevance by measuring the distance between frames and queries in a common textual-visual semantic embedding space induced by a neural network. In addition, we extend the model to capture query-independent properties, such as frame quality. We compare our method against previous state of the art on textual-visual embeddings for thumbnail selection and show that our model outperforms them on relevance prediction. Furthermore, we introduce a new dataset, annotated with diversity and query-specific relevance labels. On this dataset, we train and test our complete model for video summarization and show that it outperforms standard baselines such as Maximal Marginal Relevance.',\n",
       "  'Title: data augmentation for low resource neural machine translation\\nAbstract: The quality of a Neural Machine Translation system depends substantially on the availability of sizable parallel corpora. For low-resource language pairs this is not the case, resulting in poor translation quality. Inspired by work in computer vision, we propose a novel data augmentation approach that targets low-frequency words by generating new sentence pairs containing rare words in new, synthetically created contexts. Experimental results on simulated low-resource settings show that our method improves translation quality by up to 2.9 BLEU points over the baseline and up to 3.2 BLEU over back-translation.',\n",
       "  \"Title: large scale feature selection of risk genetic factors for alzheimer s disease via distributed group lasso regression\\nAbstract: Genome-wide association studies (GWAS) have achieved great success in the genetic study of Alzheimer's disease (AD). Collaborative imaging genetics studies across different research institutions show the effectiveness of detecting genetic risk factors. However, the high dimensionality of GWAS data poses significant challenges in detecting risk SNPs for AD. Selecting relevant features is crucial in predicting the response variable. In this study, we propose a novel Distributed Feature Selection Framework (DFSF) to conduct the large-scale imaging genetics studies across multiple institutions. To speed up the learning process, we propose a family of distributed group Lasso screening rules to identify irrelevant features and remove them from the optimization. Then we select the relevant group features by performing the group Lasso feature selection process in a sequence of parameters. Finally, we employ the stability selection to rank the top risk SNPs that might help detect the early stage of AD. To the best of our knowledge, this is the first distributed feature selection model integrated with group Lasso feature selection as well as detecting the risk genetic factors across multiple research institutions system. Empirical studies are conducted on 809 subjects with 5.9 million SNPs which are distributed across several individual institutions, demonstrating the efficiency and effectiveness of the proposed method.\",\n",
       "  'Title: understanding people flow in transportation hubs\\nAbstract: In this paper, we aim to monitor the flow of people in large public infrastructures. We propose an unsupervised methodology to cluster people flow patterns into the most typical and meaningful configurations. By processing 3-D images from a network of depth cameras, we build a descriptor for the flow pattern. We define a data-irregularity measure that assesses how well each descriptor fits a data model. This allows us to rank flow patterns from highly distinctive (outliers) to very common ones. By discarding outliers, we obtain more reliable key configurations (classes). Synthetic experiments show that the proposed method is superior to standard clustering methods. We applied it in an operational scenario during 14 days in the X-ray screening area of an international airport. Results show that our methodology is able to successfully summarize the representative patterns for such a long observation period, providing relevant information for airport management. Beyond regular flows, our method identifies a set of rare events corresponding to uncommon activities (cleaning, special security, and circulating staff).',\n",
       "  'Title: lexipers an ontology based sentiment lexicon for persian\\nAbstract: Sentiment analysis refers to the use of natural language processing to identify and extract subjective information from textual resources. One approach for sentiment extraction is using a sentiment lexicon. A sentiment lexicon is a set of words associated with the sentiment orientation that they express. In this paper, we describe the process of generating a general purpose sentiment lexicon for Persian. A new graph-based method is introduced for seed selection and expansion based on an ontology. Sentiment lexicon generation is then mapped to a document classification problem. We used the K-nearest neighbors and nearest centroid methods for classification. These classifiers have been evaluated based on a set of hand labeled synsets. The final sentiment lexicon has been generated by the best classifier. The results show an acceptable performance in terms of accuracy and F-measure in the generated sentiment lexicon.',\n",
       "  'Title: cross sentence n ary relation extraction with graph lstms\\nAbstract: Past work in relation extraction has focused on binary relations in single sentences. Recent NLP inroads in high-value domains have sparked interest in the more general setting of extracting n-ary relations that span multiple sentences. In this paper, we explore a general relation extraction framework based on graph long short-term memory networks (graph LSTMs) that can be easily extended to cross-sentence n-ary relation extraction. The graph formulation provides a unified way of exploring different LSTM approaches and incorporating various intra-sentential and inter-sentential dependencies, such as sequential, syntactic, and discourse relations. A robust contextual representation is learned for the entities, which serves as input to the relation classifier. This simplifies handling of relations with arbitrary arity, and enables multi-task learning with related relations. We evaluate this framework in two important precision medicine settings, demonstrating its effectiveness with both conventional supervised learning and distant supervision. Cross-sentence extraction produced larger knowledge bases. and multi-task learning significantly improved extraction accuracy. A thorough analysis of various LSTM approaches yielded useful insight the impact of linguistic analysis on extraction accuracy.',\n",
       "  'Title: mining communication data in a music community a preliminary analysis\\nAbstract: Comments play an important role within online creative communities because they make it possible to foster the production and improvement of authors’ artifacts. We investigate how comment-based communication help shape members’ behavior within online creative communities. In this paper, we report the results of a preliminary study aimed at mining the communication network of a music community for collaborative songwriting, where users collaborate online by first uploading new songs and then by adding new tracks and providing feedback in forms of comments.',\n",
       "  'Title: cross lingual distillation for text classification\\nAbstract: Cross-lingual text classification(CLTC) is the task of classifying documents written in different languages into the same taxonomy of categories. This paper presents a novel approach to CLTC that builds on model distillation, which adapts and extends a framework originally proposed for model compression. Using soft probabilistic predictions for the documents in a label-rich language as the (induced) supervisory labels in a parallel corpus of documents, we train classifiers successfully for new languages in which labeled training data are not available. An adversarial feature adaptation technique is also applied during the model training to reduce distribution mismatch. We conducted experiments on two benchmark CLTC datasets, treating English as the source language and German, French, Japan and Chinese as the unlabeled target languages. The proposed approach had the advantageous or comparable performance of the other state-of-art methods.',\n",
       "  'Title: performance analysis of decoupled cell association in multi tier hybrid networks using real blockage environments\\nAbstract: Millimeter wave (mmWave) links have the potential to offer high data rates and capacity needed in fifth generation (5G) networks, however they have very high penetration and path loss. A solution to this problem is to bring the base station closer to the end-user through heterogeneous networks (HetNets). HetNets could be designed to allow users to connect to different base stations (BSs) in the uplink and downlink. This phenomenon is known as downlink-uplink decoupling (DUDe). This paper explores the effect of DUDe in a three tier HetNet deployed in two different real-world environments. Our simulation results show that DUDe can provide improvements with regard to increasing the system coverage and data rates while the extent of improvement depends on the different environments that the system is deployed in.',\n",
       "  \"Title: completeness theorems for pomset languages and concurrent kleene algebras\\nAbstract: Pomsets constitute one of the most basic models of concurrency. A pomset is a generalisation of a word over an alphabet in that letters may be partially ordered. A term $t$ using the bi-Kleene operations $0,1, +, \\\\cdot\\\\, ,^*, \\\\parallel, ^{(*)}$ defines a language $ \\\\mathopen{[\\\\![ } t \\\\mathclose{]\\\\!] } $ of pomsets in a natural way. #R##N#We prove that every valid universal equality over pomset languages using these operations is a consequence of the equational theory of regular languages (in which parallel multiplication and iteration are undefined) plus that of the commutative-regular languages (in which sequential multiplication and iteration are undefined). We also show that the class of $\\\\textit{rational}$ pomset languages (that is, those languages generated from singleton pomsets using the bi-Kleene operations) is closed under all Boolean operations. #R##N#An $ \\\\textit{ideal}$ of a pomset $p$ is a pomset using the letters of $p$, but having an ordering at least as strict as $p$. A bi-Kleene term $t$ thus defines the set $ \\\\textbf{Id} (\\\\mathopen{[\\\\![ } t \\\\mathclose{]\\\\!] }) $ of ideals of pomsets in $ \\\\mathopen{[\\\\![ } t \\\\mathclose{]\\\\!] } $. We prove that if $t$ does not contain commutative iteration $^{(*)}$ (in our terminology, $t$ is bw-rational) then $\\\\textbf{Id} (\\\\mathopen{[\\\\![ } t \\\\mathclose{]\\\\!] }) \\\\cap \\\\textbf{Pom}_{sp}$, where $ \\\\textbf{Pom}_{sp}$ is the set of pomsets generated from singleton pomsets using sequential and parallel multiplication ($ \\\\cdot$ and $ \\\\parallel$) is defined by a bw-rational term, and if two such terms $t,t'$ define the same ideal language, then $t'=t$ is provable from the Kleene axioms for $0,1, +, \\\\cdot\\\\, ,^*$ plus the commutative idempotent semiring axioms for $0,1, +, \\\\parallel$ plus the exchange law $ (u \\\\parallel v)\\\\cdot ( x \\\\parallel y) \\\\le v \\\\cdot y \\\\parallel u \\\\cdot x $.\",\n",
       "  'Title: an effective algorithm for hyperparameter optimization of neural networks\\nAbstract: A major challenge in designing neural network (NN) systems is to determine the best structure and parameters for the network given the data for the machine learning problem at hand. Examples of parameters are the number of layers and nodes, the learning rates, and the dropout rates. Typically, these parameters are chosen based on heuristic rules and manually fine-tuned, which may be very time-consuming, because evaluating the performance of a single parametrization of the NN may require several hours. This paper addresses the problem of choosing appropriate parameters for the NN by formulating it as a box-constrained mathematical optimization problem, and applying a derivative-free optimization tool that automatically and effectively searches the parameter space. The optimization tool employs a radial basis function model of the objective function (the prediction accuracy of the NN) to accelerate the discovery of configurations yielding high accuracy. Candidate configurations explored by the algorithm are trained to a small number of epochs, and only the most promising candidates receive full training. The performance of the proposed methodology is assessed on benchmark sets and in the context of predicting drug-drug interactions, showing promising results. The optimization tool used in this paper is open-source.',\n",
       "  'Title: an algorithm of frequency estimation for multi channel coprime sampling\\nAbstract: In some applications of frequency estimation, it is challenging to sample at as high as the Nyquist rate due to hardware limitations. An effective solution is to use multiple sub-Nyquist channels with coprime undersampling ratios to jointly sample. In this paper, an algorithm suitable for any number of channels is proposed, which is based on subspace techniques. Numerical simulations show that the proposed algorithm has high accuracy and good robustness.',\n",
       "  'Title: the emergence and evolution of the research fronts in hiv aids research\\nAbstract: In this paper, we have identified and analyzed the emergence, structure and dynamics of the paradigmatic research fronts that established the fundamentals of the biomedical knowledge on HIV/AIDS. A search of papers with the identifiers \"HIV/AIDS\", \"Human Immunodeficiency Virus\", “HIV-1” and \"Acquired Immunodeficiency Syndrome\" in the Web of Science (Thomson Reuters), was carried out. A citation network of those papers was constructed. Then, a sub-network of the papers with the highest number of inter-citations (with a minimal in-degree of 28) was selected to perform a combination of network clustering and text mining to identify the paradigmatic research fronts and analyze their dynamics. Thirteen research fronts were identified in this sub-network. The biggest and oldest front is related to the clinical knowledge on the disease in the patient. Nine of the fronts are related to the study of specific molecular structures and mechanisms and two of these fronts are related to the development of drugs. The rest of the fronts are related to the study of the disease at the cellular level. Interestingly, the emergence of these fronts occurred in successive \"waves\" over the time which suggest a transition in the paradigmatic focus. The emergence and evolution of the biomedical fronts in HIV/AIDS research is explained not just by the partition of the problem in elements and interactions leading to increasingly specialized communities, but also by changes in the technological context of this health problem and the dramatic changes in the epidemiological reality of HIV/AIDS that occurred between 1993 and 1995.',\n",
       "  'Title: mobile edge computing empowers internet of things\\nAbstract: In this paper, we propose a Mobile Edge Internet of Things (MEIoT) architecture by leveraging the fiber-wireless access technology, the cloudlet concept, and the software defined networking framework. The MEIoT architecture brings computing and storage resources close to Internet of Things (IoT) devices in order to speed up IoT data sharing and analytics. Specifically, the IoT devices (belonging to the same user) are associated to a specific proxy Virtual Machine (VM) in the nearby cloudlet. The proxy VM stores and analyzes the IoT data (generated by its IoT devices) in real-time. Moreover, we introduce the semantic and social IoT technology in the context of MEIoT to solve the interoperability and inefficient access control problem in the IoT system. In addition, we propose two dynamic proxy VM migration methods to minimize the end-to-end delay between proxy VMs and their IoT devices and to minimize the total on-grid energy consumption of the cloudlets, respectively. Performance of the proposed methods are validated via extensive simulations.',\n",
       "  'Title: the temporal event graph\\nAbstract: Temporal networks are increasingly being used to model the interactions of complex systems. Most studies require the temporal aggregation of edges (or events) into discrete time steps to perform analysis. In this article we describe a static, lossless, and unique representation of a temporal network, the temporal event graph (TEG). The TEG describes the temporal network in terms of both the inter-event time and two-event temporal motif distributions. By considering these distributions in unison we provide a new method to characterise the behaviour of individuals and collectives in temporal networks as well as providing a natural decomposition of the network. We illustrate the utility of the TEG by providing examples on both synthetic and real temporal networks.',\n",
       "  'Title: deepsf deep convolutional neural network for mapping protein sequences to folds\\nAbstract: Motivation #R##N#Protein fold recognition is an important problem in structural bioinformatics. Almost all traditional fold recognition methods use sequence (homology) comparison to indirectly predict the fold of a tar get protein based on the fold of a template protein with known structure, which cannot explain the relationship between sequence and fold. Only a few methods had been developed to classify protein sequences into a small number of folds due to methodological limitations, which are not generally useful in practice. #R##N#Results #R##N#We develop a deep 1D-convolution neural network (DeepSF) to directly classify any protein se quence into one of 1195 known folds, which is useful for both fold recognition and the study of se quence-structure relationship. Different from traditional sequence alignment (comparison) based methods, our method automatically extracts fold-related features from a protein sequence of any length and map it to the fold space. We train and test our method on the datasets curated from SCOP1.75, yielding a classification accuracy of 80.4%. On the independent testing dataset curated from SCOP2.06, the classification accuracy is 77.0%. We compare our method with a top profile profile alignment method - HHSearch on hard template-based and template-free modeling targets of CASP9-12 in terms of fold recognition accuracy. The accuracy of our method is 14.5%-29.1% higher than HHSearch on template-free modeling targets and 4.5%-16.7% higher on hard template-based modeling targets for top 1, 5, and 10 predicted folds. The hidden features extracted from sequence by our method is robust against sequence mutation, insertion, deletion and truncation, and can be used for other protein pattern recognition problems such as protein clustering, comparison and ranking.',\n",
       "  'Title: adversarial playground a visualization suite for adversarial sample generation\\nAbstract: With growing interest in adversarial machine learning, it is important for machine learning practitioners and users to understand how their models may be attacked. We propose a web-based visualization tool, Adversarial-Playground, to demonstrate the efficacy of common adversarial methods against a deep neural network (DNN) model, built on top of the TensorFlow library. Adversarial-Playground provides users an efficient and effective experience in exploring techniques generating adversarial examples, which are inputs crafted by an adversary to fool a machine learning system. To enable Adversarial-Playground to generate quick and accurate responses for users, we use two primary tactics: (1) We propose a faster variant of the state-of-the-art Jacobian saliency map approach that maintains a comparable evasion rate. (2) Our visualization does not transmit the generated adversarial images to the client, but rather only the matrix describing the sample and the vector representing classification likelihoods. #R##N#The source code along with the data from all of our experiments are available at \\\\url{this https URL}.',\n",
       "  \"Title: securing databases from probabilistic inference\\nAbstract: Databases can leak confidential information when users combine query results with probabilistic data dependencies and prior knowledge. Current research offers mechanisms that either handle a limited class of dependencies or lack tractable enforcement algorithms. We propose a foundation for Database Inference Control based on ProbLog, a probabilistic logic programming language. We leverage this foundation to develop Angerona, a provably secure enforcement mechanism that prevents information leakage in the presence of probabilistic dependencies. We then provide a tractable inference algorithm for a practically relevant fragment of ProbLog. We empirically evaluate Angerona's performance showing that it scales to relevant security-critical problems.\",\n",
       "  'Title: persistent flows and non reciprocal interactions in deterministic networks\\nAbstract: This paper studies deterministic consensus networks with discrete-time dynamics under persistent flows and non-reciprocal agent interactions. An arc describing the interaction strength between two agents is said to be persistent if its weight function has an infinite $l_1$ norm. We discuss two balance conditions on the interactions between agents which generalize the arc-balance and cut-balance conditions in the literature respectively. The proposed conditions require that such a balance should be satisfied over each time window of a fixed length instead of at each time instant. We prove that in both cases global consensus is reached if and only if the persistent graph, which consists of all the persistent arcs, contains a directed spanning tree. The convergence rates of the system to consensus are also provided in terms of the interactions between agents having taken place. The results are obtained under a weak condition without assuming the existence of a positive lower bound of all the nonzero weights of arcs and are compared with the existing results. Illustrative examples are provided to show the critical importance of the nontrivial lower boundedness of the self-confidence of the agents.',\n",
       "  'Title: simultaneous merging multiple grid maps using the robust motion averaging\\nAbstract: Mapping in the GPS-denied environment is an important and challenging task in the field of robotics. In the large environment, mapping can be significantly accelerated by multiple robots exploring different parts of the environment. Accordingly, a key problem is how to integrate these local maps built by different robots into a single global map. In this paper, we propose an approach for simultaneous merging of multiple grid maps by the robust motion averaging. The main idea of this approach is to recover all global motions for map merging from a set of relative motions. Therefore, it firstly adopts the pair-wise map merging method to estimate relative motions for grid map pairs. To obtain as many reliable relative motions as possible, a graph-based sampling scheme is utilized to efficiently remove unreliable relative motions obtained from the pair-wise map merging. Subsequently, the accurate global motions can be recovered from the set of reliable relative motions by the motion averaging. Experimental results carried on real robot data sets demonstrate that the proposed approach can achieve simultaneous merging of multiple grid maps with good performances.',\n",
       "  'Title: joint resource allocation in swipt based multi antenna decode and forward relay networks\\nAbstract: In this paper, we consider relay-assisted simultaneous wireless information and power transfer (SWIPT) for two-hop cooperative transmission, where a half-duplex multi-antenna relay adopts decode-and-forward (DF) relaying strategy for information forwarding. The relay is assumed to be energy-free and needs to harvest energy from the source node. By embedding power splitting (PS) at each relay antenna to coordinate the received energy and information, joint problem of determining PS ratios and power allocation at the multi-antenna relay node is formulated to maximize the end-to-end achievable rate. We show that the multi-antenna relay is equivalent to a virtual single-antenna relay in such a SWIPT system, and the problem is optimally solved with closed-form. To reduce the hardware cost of the PS scheme, we further propose the antenna clustering scheme, where the multiple antennas at the relay are partitioned into two disjoint groups which are exclusively used for information decoding and energy harvesting, respectively. Optimal clustering algorithm is first proposed but with exponential complexity. Then a greedy clustering algorithms is introduced with linear complexity and approaching to the optimal performance. Several valuable insights are provided via theoretical analysis and simulation results.',\n",
       "  'Title: sequential composition in the presence of intermediate termination\\nAbstract: The standard operational semantics of the sequential composition operator gives rise to unbounded branching and forgetfulness when transparent process expressions are put in sequence. Due to transparency, the correspondence between context-free and pushdown processes fails modulo bisimilarity, and it is not clear how to specify an always terminating half counter. We propose a revised operational semantics for the sequential composition operator in the context of intermediate termination. With the revised operational semantics, we eliminate transparency. As a consequence, we establish a correspondence between context-free processes and pushdown processes. Moreover, we prove the reactive Turing powerfulness of TCP with iteration and nesting with the revised operational semantics for sequential composition.',\n",
       "  'Title: on the role of text preprocessing in neural network architectures an evaluation study on text categorization and sentiment analysis\\nAbstract: In this paper we investigate the impact of simple text preprocessing decisions (particularly tokenizing, lemmatizing, lowercasing and multiword grouping) on the performance of a state-of-the-art text classifier based on convolutional neural networks. Despite potentially affecting the final performance of any given model, this aspect has not received a substantial interest in the deep learning literature. We perform an extensive evaluation in standard benchmarks from text categorization and sentiment analysis. Our results show that a simple tokenization of the input text is often enough, but also highlight the importance of being consistent in the preprocessing of the evaluation set and the corpus used for training word embeddings.',\n",
       "  \"Title: on sound relative error bounds for floating point arithmetic\\nAbstract: State-of-the-art static analysis tools for verifying finite-precision code compute worst-case absolute error bounds on numerical errors. These are, however, often not a good estimate of accuracy as they do not take into account the magnitude of the computed values. Relative errors, which compute errors relative to the value's magnitude, are thus preferable. While today's tools do report relative error bounds, these are merely computed via absolute errors and thus not necessarily tight or more informative. Furthermore, whenever the computed value is close to zero on part of the domain, the tools do not report any relative error estimate at all. Surprisingly, the quality of relative error bounds computed by today's tools has not been systematically studied or reported to date. In this paper, we investigate how state-of-the-art static techniques for computing sound absolute error bounds can be used, extended and combined for the computation of relative errors. Our experiments on a standard benchmark set show that computing relative errors directly, as opposed to via absolute errors, is often beneficial and can provide error estimates up to six orders of magnitude tighter, i.e. more accurate. We also show that interval subdivision, another commonly used technique to reduce over-approximations, has less benefit when computing relative errors directly, but it can help to alleviate the effects of the inherent issue of relative error estimates close to zero.\",\n",
       "  'Title: a simple centrality index for scientific social recognition\\nAbstract: We introduce a new centrality index for bipartite network of papers and authors that we call $K$-index. The $K$-index grows with the citation performance of the papers that cite a given researcher and can seen as a measure of scientific social recognition. Indeed, the $K$-index measures the number of hubs, defined in a self-consistent way in the bipartite network, that cites a given author. We show that the $K$-index can be computed by simple inspection of the Web of Science platform and presents several advantages over other centrality indexes, in particular Hirsch $h$-index. The $K$-index is robust to self-citations, is not limited by the total number of papers published by a researcher as occurs for the $h$-index and can distinguish in a consistent way researchers that have the same $h$-index but very different scientific social recognition. The $K$-index easily detects a known case of a researcher with inflated number of papers, citations and $h$-index due to scientific misconduct. Finally, we show that, in a sample of twenty-eight physics Nobel laureates and twenty-eight highly cited non-Nobel-laureate physicists, the $K$-index correlates better to the achievement of the prize than the number of papers, citations, citations per paper, citing articles or the $h$-index. Clustering researchers in a $K$ versus $h$ plot reveals interesting outliers that suggest that these two indexes can present complementary independent information.',\n",
       "  'Title: speaker recognition with cough laugh and wei\\nAbstract: This paper proposes a speaker recognition (SRE) task with trivial speech events, such as cough and laugh. These trivial events are ubiquitous in conversations and less subjected to intentional change, therefore offering valuable particularities to discover the genuine speaker from disguised speech. However, trivial events are often short and idiocratic in spectral patterns, making SRE extremely difficult. Fortunately, we found a very powerful deep feature learning structure that can extract highly speaker-sensitive features. By employing this tool, we studied the SRE performance on three types of trivial events: cough, laugh and \"Wei\" (a short Chinese \"Hello\"). The results show that there is rich speaker information within these trivial events, even for cough that is intuitively less speaker distinguishable. With the deep feature approach, the EER can reach 10%-14% with the three trivial events, despite their extremely short durations (0.2-1.0 seconds).',\n",
       "  'Title: centralized and distributed sparsification for low complexity message passing algorithm in c ran architectures\\nAbstract: Cloud radio access network (C-RAN) is a promising technology for fifth-generation (5G) cellular systems. However the burden imposed by the huge amount of data to be collected (in the uplink) from the radio remote heads (RRHs) and processed at the base band unit (BBU) poses serious challenges. In order to reduce the computation effort of minimum mean square error (MMSE) receiver at the BBU the Gaussian message passing (MP) together with a suitable sparsification of the channel matrix can be used. In this paper we propose two sets of solutions, either centralized or distributed ones. In the centralized solutions, we propose different approaches to sparsify the channel matrix, in order to reduce the complexity of MP. However these approaches still require that all signals reaching the RRH are conveyed to the BBU, therefore the communication requirements among the backbone network devices are unaltered. In the decentralized solutions instead we aim at reducing both the complexity of MP at the BBU and the requirements on the RRHs-BBU communication links by pre-processing the signals at the RRH and convey a reduced set of signals to the BBU.',\n",
       "  \"Title: on signal reconstruction from frog measurements\\nAbstract: Phase retrieval refers to recovering a signal from its Fourier magnitude. This problem arises naturally in many scientific applications, such as ultra-short laser pulse characterization and diffraction imaging. Unfortunately, phase retrieval is ill-posed for almost all one-dimensional signals. In order to characterize a laser pulse and overcome the ill-posedness, it is common to use a technique called Frequency-Resolved Optical Gating (FROG). In FROG, the measured data, referred to as FROG trace, is the Fourier magnitude of the product of the underlying signal with several translated versions of itself. The FROG trace results in a system of phaseless quartic Fourier measurements. In this paper, we prove that it suffices to consider only three translations of the signal to determine almost all bandlimited signals, up to trivial ambiguities. In practice, one usually also has access to the signal's Fourier magnitude. We show that in this case only two translations suffice. Our results significantly improve upon earlier work.\",\n",
       "  \"Title: a note on first order spectra with binary relations\\nAbstract: The spectrum of a first-order sentence is the set of the cardinalities of its finite models. In this paper, we consider the spectra of sentences over binary relations that use at least three variables. We show that for every such sentence $\\\\Phi$, there is a sentence $\\\\Phi'$ that uses the same number of variables, but only one symmetric binary relation, such that its spectrum is linearly proportional to the spectrum of $\\\\Phi$. Moreover, the models of $\\\\Phi'$ are all bipartite graphs. As a corollary, we obtain that to settle Asser's conjecture, i.e., whether the class of spectra is closed under complement, it is sufficient to consider only sentences using only three variables whose models are restricted to undirected bipartite graphs.\",\n",
       "  'Title: toward controlled generation of text\\nAbstract: Generic generation and manipulation of text is challenging and has limited success compared to recent deep generative modeling in visual domain. This paper aims at generating plausible natural language sentences, whose attributes are dynamically controlled by learning disentangled latent representations with designated semantics. We propose a new neural generative model which combines variational auto-encoders and holistic attribute discriminators for effective imposition of semantic structures. With differentiable approximation to discrete text samples, explicit constraints on independent attribute controls, and efficient collaborative learning of generator and discriminators, our model learns highly interpretable representations from even only word annotations, and produces realistic sentences with desired attributes. Quantitative evaluation validates the accuracy of sentence and attribute generation.',\n",
       "  'Title: a pilot study of domain adaptation effect for neural abstractive summarization\\nAbstract: We study the problem of domain adaptation for neural abstractive summarization. We make initial efforts in investigating what information can be transferred to a new domain. Experimental results on news stories and opinion articles indicate that neural summarization model benefits from pre-training based on extractive summaries. We also find that the combination of in-domain and out-of-domain setup yields better summaries when in-domain data is insufficient. Further analysis shows that, the model is capable to select salient content even trained on out-of-domain data, but requires in-domain data to capture the style for a target domain.',\n",
       "  'Title: wireless powered cooperative jamming for secure ofdm system\\nAbstract: This paper studies the secrecy communication in an orthogonal frequency division multiplexing system, where a source sends confidential information to a destination in the presence of a potential eavesdropper. We employ wireless powered cooperative jamming to improve the secrecy rate of this system with the assistance of a cooperative jammer, which works in the harvest-then-jam protocol over two time slots. In the first slot, the source sends dedicated energy signals to power the jammer; in the second slot, the jammer uses the harvested energy to jam the eavesdropper, in order to protect the simultaneous secrecy communication from the source to the destination. In particular, we consider two types of receivers at the destination, namely Type-I and Type-II receivers, which do not have and have the capability of canceling the (a priori known) jamming signals, respectively. For both types of receivers, we maximize the secrecy rate at the destination by jointly optimizing the transmit power allocation at the source and the jammer over sub-carriers, as well as the time allocation between the two time slots. First, we present the globally optimal solution to this problem via the Lagrange dual method, which, however, is of high implementation complexity. Next, to balance the tradeoff between the algorithm complexity and performance, we propose alternative low-complexity solutions based on the minorization maximization and heuristic successive optimization, respectively. Simulation results show that the proposed approaches significantly improve the secrecy rate, as compared to benchmark schemes without joint power and time allocation.',\n",
       "  'Title: digital investigation of pdf files unveiling traces of embedded malware\\nAbstract: Over the last decade, malicious software (or malware, for short) has shown an increasing sophistication and proliferation, fueled by a flourishing underground economy, in response to the increasing complexity of modern defense mechanisms. PDF documents are among the major vectors used to convey malware, thanks to the flexibility of their structure and the ability of embedding different kinds of content, ranging from images to JavaScript code. Despite the numerous efforts made by the research and industrial communities, PDF malware is still one of the major threats on the cyber-security landscape. In this paper, we provide an overview of the current attack techniques used to convey PDF malware, and discuss state-of-the-art PDF malware analysis tools that provide valuable support to digital forensic investigations. We finally discuss limitations and open issues of the current defense mechanisms, and sketch some interesting future research directions.',\n",
       "  'Title: an alon boppana type bound for weighted graphs and lowerbounds for spectral sparsification\\nAbstract: We prove the following Alon-Boppana type theorem for general (not necessarily regular) weighted graphs: if $G$ is an $n$-node weighted undirected graph of average combinatorial degree $d$ (that is, $G$ has $dn/2$ edges) and girth $g> 2d^{1/8}+1$, and if $\\\\lambda_1 \\\\leq \\\\lambda_2 \\\\leq \\\\cdots \\\\lambda_n$ are the eigenvalues of the (non-normalized) Laplacian of $G$, then \\\\[ \\\\frac {\\\\lambda_n}{\\\\lambda_2} \\\\geq 1 + \\\\frac 4{\\\\sqrt d} - O \\\\left( \\\\frac 1{d^{\\\\frac 58} }\\\\right) \\\\] (The Alon-Boppana theorem implies that if $G$ is unweighted and $d$-regular, then $\\\\frac {\\\\lambda_n}{\\\\lambda_2} \\\\geq 1 + \\\\frac 4{\\\\sqrt d} - O\\\\left( \\\\frac 1 d \\\\right)$ if the diameter is at least $d^{1.5}$.) #R##N#Our result implies a lower bound for spectral sparsifiers. A graph $H$ is a spectral $\\\\epsilon$-sparsifier of a graph $G$ if \\\\[ L(G) \\\\preceq L(H) \\\\preceq (1+\\\\epsilon) L(G) \\\\] where $L(G)$ is the Laplacian matrix of $G$ and $L(H)$ is the Laplacian matrix of $H$. Batson, Spielman and Srivastava proved that for every $G$ there is an $\\\\epsilon$-sparsifier $H$ of average degree $d$ where $\\\\epsilon \\\\approx \\\\frac {4\\\\sqrt 2}{\\\\sqrt d}$ and the edges of $H$ are a (weighted) subset of the edges of $G$. Batson, Spielman and Srivastava also show that the bound on $\\\\epsilon$ cannot be reduced below $\\\\approx \\\\frac 2{\\\\sqrt d}$ when $G$ is a clique; our Alon-Boppana-type result implies that $\\\\epsilon$ cannot be reduced below $\\\\approx \\\\frac 4{\\\\sqrt d}$ when $G$ comes from a family of expanders of super-constant degree and super-constant girth. #R##N#The method of Batson, Spielman and Srivastava proves a more general result, about sparsifying sums of rank-one matrices, and their method applies to an \"online\" setting. We show that for the online matrix setting the $4\\\\sqrt 2 / \\\\sqrt d$ bound is tight, up to lower order terms.',\n",
       "  \"Title: a knowledge based analysis of the blockchain protocol\\nAbstract: At the heart of the Bitcoin is a blockchain protocol, a protocol for achieving consensus on a public ledger that records bitcoin transactions. To the extent that a blockchain protocol is used for applications such as contract signing and making certain transactions (such as house sales) public, we need to understand what guarantees the protocol gives us in terms of agents' knowledge. Here, we provide a complete characterization of agent's knowledge when running a blockchain protocol using a variant of common knowledge that takes into account the fact that agents can enter and leave the system, it is not known which agents are in fact following the protocol (some agents may want to deviate if they can gain by doing so), and the fact that the guarantees provided by blockchain protocols are probabilistic. We then consider some scenarios involving contracts and show that this level of knowledge suffices for some scenarios, but not others.\",\n",
       "  'Title: machine learning for anomaly detection and categorization in multi cloud environments\\nAbstract: Cloud computing has been widely adopted by application service providers (ASPs) and enterprises to reduce both capital expenditures (CAPEX) and operational expenditures (OPEX). Applications and services previously running on private data centers are now being migrated to private or public clouds. Since most of the ASPs and enterprises have globally distributed user bases, their services need to be distributed across multiple clouds, spread across the globe which can achieve better performance in terms of latency, scalability and load balancing. The shift has eventually led the research community to study multi-cloud environments. However, the widespread acceptance of such environments has been hampered by major security concerns. Firewalls and traditional rule-based security protection techniques are not sufficient to protect user-data in multi-cloud scenarios. Recently, advances in machine learning techniques have attracted the attention of the research community to build intrusion detection systems (IDS) that can detect anomalies in the network traffic. Most of the research works, however, do not differentiate among different types of attacks. This is, in fact, necessary for appropriate countermeasures and defense against attacks. In this paper, we investigate both detecting and categorizing anomalies rather than just detecting, which is a common trend in the contemporary research works. We have used a popular publicly available dataset to build and test learning models for both detection and categorization of different attacks. To be precise, we have used two supervised machine learning techniques, namely linear regression (LR) and random forest (RF). We show that even if detection is perfect, categorization can be less accurate due to similarities between attacks. Our results demonstrate more than 99% detection accuracy and categorization accuracy of 93.6%, with the inability to categorize some attacks. Further, we argue that such categorization can be applied to multi-cloud environments using the same machine learning techniques.',\n",
       "  'Title: synthesis of positron emission tomography pet images via multi channel generative adversarial networks gans\\nAbstract: Positron emission tomography (PET) image synthesis plays an important role, which can be used to boost the training data for computer aided diagnosis systems. However, existing image synthesis methods have problems in synthesizing the low resolution PET images. To address these limitations, we propose multi-channel generative adversarial networks (M-GAN) based PET image synthesis method. Different to the existing methods which rely on using low-level features, the proposed M-GAN is capable to represent the features in a high-level of semantic based on the adversarial learning concept. In addition, M-GAN enables to take the input from the annotation (label) to synthesize the high uptake regions e.g., tumors and from the computed tomography (CT) images to constrain the appearance consistency and output the synthetic PET images directly. Our results on 50 lung cancer PET-CT studies indicate that our method was much closer to the real PET images when compared with the existing methods.',\n",
       "  \"Title: challenging neural dialogue models with natural data memory networks fail on incremental phenomena\\nAbstract: Natural, spontaneous dialogue proceeds incrementally on a word-by-word basis; and it contains many sorts of disfluency such as mid-utterance/sentence hesitations, interruptions, and self-corrections. But training data for machine learning approaches to dialogue processing is often either cleaned-up or wholly synthetic in order to avoid such phenomena. The question then arises of how well systems trained on such clean data generalise to real spontaneous dialogue, or indeed whether they are trainable at all on naturally occurring dialogue data. To answer this question, we created a new corpus called bAbI+ by systematically adding natural spontaneous incremental dialogue phenomena such as restarts and self-corrections to the Facebook AI Research's bAbI dialogues dataset. We then explore the performance of a state-of-the-art retrieval model, MemN2N, on this more natural dataset. Results show that the semantic accuracy of the MemN2N model drops drastically; and that although it is in principle able to learn to process the constructions in bAbI+, it needs an impractical amount of training data to do so. Finally, we go on to show that an incremental, semantic parser -- DyLan -- shows 100% semantic accuracy on both bAbI and bAbI+, highlighting the generalisation properties of linguistically informed dialogue models.\",\n",
       "  \"Title: towards vision based smart hospitals a system for tracking and monitoring hand hygiene compliance\\nAbstract: One in twenty-five patients admitted to a hospital will suffer from a hospital acquired infection. If we can intelligently track healthcare staff, patients, and visitors, we can better understand the sources of such infections. We envision a smart hospital capable of increasing operational efficiency and improving patient care with less spending. In this paper, we propose a non-intrusive vision-based system for tracking people's activity in hospitals. We evaluate our method for the problem of measuring hand hygiene compliance. Empirically, our method outperforms existing solutions such as proximity-based techniques and covert in-person observational studies. We present intuitive, qualitative results that analyze human movement patterns and conduct spatial analytics which convey our method's interpretability. This work is a first step towards a computer-vision based smart hospital and demonstrates promising results for reducing hospital acquired infections.\",\n",
       "  'Title: an open source c implementation of multi threaded gaussian mixture models k means and expectation maximisation\\nAbstract: Modelling of multivariate densities is a core component in many signal processing, pattern recognition and machine learning applications. The modelling is often done via Gaussian mixture models (GMMs), which use computationally expensive and potentially unstable training algorithms. We provide an overview of a fast and robust implementation of GMMs in the C++ language, employing multi-threaded versions of the Expectation Maximisation (EM) and k-means training algorithms. Multi-threading is achieved through reformulation of the EM and k-means algorithms into a MapReduce-like framework. Furthermore, the implementation uses several techniques to improve numerical stability and modelling accuracy. We demonstrate that the multi-threaded implementation achieves a speedup of an order of magnitude on a recent 16 core machine, and that it can achieve higher modelling accuracy than a previously well-established publically accessible implementation. The multi-threaded implementation is included as a user-friendly class in recent releases of the open source Armadillo C++ linear algebra library. The library is provided under the permissive Apache~2.0 license, allowing unencumbered use in commercial products.',\n",
       "  'Title: a comparative study of the clinical use of motion analysis from kinect skeleton data\\nAbstract: The analysis of human motion as a clinical tool can bring many benefits such as the early detection of disease and the monitoring of recovery, so in turn helping people to lead independent lives. However, it is currently under used. Developments in depth cameras, such as Kinect, have opened up the use of motion analysis in settings such as GP surgeries, care homes and private homes. To provide an insight into the use of Kinect in the healthcare domain, we present a review of the current state of the art. We then propose a method that can represent human motions from time-series data of arbitrary length, as a single vector. Finally, we demonstrate the utility of this method by extracting a set of clinically significant features and using them to detect the age related changes in the motions of a set of 54 individuals, with a high degree of certainty (F1- score between 0.9 - 1.0). Indicating its potential application in the detection of a range of age-related motion impairments.',\n",
       "  'Title: structural patterns of information cascades and their implications for dynamics and semantics\\nAbstract: Information cascades are ubiquitous in both physical society and online social media, taking on large variations in structures, dynamics and semantics. Although the dynamics and semantics of information cascades have been studied, the structural patterns and their correlations with dynamics and semantics are largely unknown. Here we explore a large-scale dataset including $432$ million information cascades with explicit records of spreading traces, spreading behaviors, information content as well as user profiles. We find that the structural complexity of information cascades is far beyond the previous conjectures. We first propose a ten-dimensional metric to quantify the structural characteristics of information cascades, reflecting cascade size, silhouette, direction and activity aspects. We find that bimodal law governs majority of the metrics, information flows in cascades have four directions, and the self-loop number and average activity of cascades follows power law. We then analyze the high-order structural patterns of information cascades. Finally, we evaluate to what extent the structural features of information cascades can explain its dynamic patterns and semantics, and finally uncover some notable implications of structural patterns in information cascades. Our discoveries also provide a foundation for the microscopic mechanisms for information spreading, potentially leading to implications for cascade prediction and outlier detection.',\n",
       "  'Title: nonconvex sparse logistic regression with weakly convex regularization\\nAbstract: In this paper, we propose to fit a sparse logistic regression model by a weakly convex regularized nonconvex optimization problem. The idea is based on the finding that a weakly convex function as an approximation of the l#N# 0#N# pseudo norm is able to better induce sparsity than the commonly used l#N# 1#N# norm. For a class of weakly convex sparsity inducing functions, we prove the nonconvexity of the corresponding problem and study its local optimality conditions and the choice of the regularization parameter. Despite the nonconvexity, a method based on proximal gradient descent is used to solve the general weakly convex sparse logistic regression, and its convergence behavior is studied theoretically. Then, the general framework is applied to a specific weakly convex function, and a local optimality condition and a bound on the logistic loss at a local optimum are provided. The solution method is instantiated in this case as an iterative firm-shrinkage algorithm, and a Nesterov acceleration is used with a convergence guarantee. Its effectiveness is demonstrated in numerical experiments by both randomly generated and real datasets.',\n",
       "  'Title: improved speech reconstruction from silent video\\nAbstract: Speechreading is the task of inferring phonetic information from visually observed articulatory facial movements, and is a notoriously difficult task for humans to perform. In this paper we present an end-to-end model based on a convolutional neural network (CNN) for generating an intelligible and natural-sounding acoustic speech signal from silent video frames of a speaking person. We train our model on speakers from the GRID and TCD-TIMIT datasets, and evaluate the quality and intelligibility of reconstructed speech using common objective measurements. We show that speech predictions from the proposed model attain scores which indicate significantly improved quality over existing models. In addition, we show promising results towards reconstructing speech from an unconstrained dictionary.',\n",
       "  'Title: layouts for plane graphs on constant number of tracks\\nAbstract: A \\\\emph{$k$-track} layout of a graph consists of a vertex $k$ colouring, and a total order of each vertex colour class, such that between each pair of colour classes no two edges cross. A \\\\emph{$k$-queue} layout of a graph consists of a total order of the vertices, and a partition of the edges into $k$ sets such that no two edges that are in the same set are nested with respect to the vertex ordering. The \\\\emph{track number} (\\\\emph{queue number}) of a graph $G$, is the minimum $k$ such that $G$ has a $k$-track ($k$-queue) layout. #R##N#This paper proves that every $n$-vertex plane graph has constant-bound track and queue numbers. The result implies that every plane has a 3D crossing-free straight-line grid drawing in $O(n)$ volume. The proof utilizes a novel graph partition technique.',\n",
       "  'Title: a fast noniterative algorithm for compressive sensing using binary measurement matrices\\nAbstract: In this paper we present a new algorithm for compressive sensing that makes use of binary measurement matrices and achieves exact recovery of sparse vectors, in a single pass, without any iterations. Our algorithm is hundreds of times faster than $\\\\ell_1$-norm minimization, and methods based on expander graphs (which require multiple iterations). Moreover, our method requires the fewest measurements amongst all methods that use binary measurement matrices. The algorithm can accommodate nearly sparse vectors, in which case it recovers the largest components, and can also #R##N#Numerical experiments with randomly generated sparse vectors indicate that the sufficient conditions for our algorithm to work are very close to being necessary. In contrast, the best known sufficient condition for $\\\\ell_1$-norm minimization to recover a sparse vector, namely the Restricted Isometry Property (RIP), is about thirty times away from being necessary. Therefore it would be worthwhile to explore alternate and improved sufficient conditions for $\\\\ell_1$-norm minimization to achieve the recovery of sparse vectors.',\n",
       "  \"Title: activation maximization generative adversarial nets\\nAbstract: Class labels have been empirically shown useful in improving the sample quality of generative adversarial nets (GANs). In this paper, we mathematically study the properties of the current variants of GANs that make use of class label information. With class aware gradient and cross-entropy decomposition, we reveal how class labels and associated losses influence GAN's training. Based on that, we propose Activation Maximization Generative Adversarial Networks (AM-GAN) as an advanced solution. Comprehensive experiments have been conducted to validate our analysis and evaluate the effectiveness of our solution, where AM-GAN outperforms other strong baselines and achieves state-of-the-art Inception Score (8.91) on CIFAR-10. In addition, we demonstrate that, with the Inception ImageNet classifier, Inception Score mainly tracks the diversity of the generator, and there is, however, no reliable evidence that it can reflect the true sample quality. We thus propose a new metric, called AM Score, to provide a more accurate estimation of the sample quality. Our proposed model also outperforms the baseline methods in the new metric.\",\n",
       "  'Title: non uniform wavelet sampling for rf analog to information conversion\\nAbstract: Feature extraction, such as spectral occupancy, interferer energy and type, or direction-of-arrival, from wideband radio-frequency~(RF) signals finds use in a growing number of applications as it enhances RF transceivers with cognitive abilities and enables parameter tuning of traditional RF chains. In power and cost limited applications, e.g., for sensor nodes in the Internet of Things, wideband RF feature extraction with conventional, Nyquist-rate analog-to-digital converters is infeasible. However, the structure of many RF features (such as signal sparsity) enables the use of compressive sensing (CS) techniques that acquire such signals at sub-Nyquist rates. While such CS-based analog-to-information (A2I) converters have the potential to enable low-cost and energy-efficient wideband RF sensing, they suffer from a variety of real-world limitations, such as noise folding, low sensitivity, aliasing, and limited flexibility. #R##N#This paper proposes a novel CS-based A2I architecture called non-uniform wavelet sampling (NUWS). Our solution extracts a carefully-selected subset of wavelet coefficients directly in the RF domain, which mitigates the main issues of existing A2I converter architectures. For multi-band RF signals, we propose a specialized variant called non-uniform wavelet bandpass sampling (NUWBS), which further improves sensitivity and reduces hardware complexity by leveraging the multi-band signal structure. We use simulations to demonstrate that NUWBS approaches the theoretical performance limits of $\\\\ell_1$-norm-based sparse signal recovery. We investigate hardware-design aspects and show ASIC measurement results for the wavelet generation stage, which highlight the efficacy of NUWBS for a broad range of RF feature extraction tasks in cost- and power-limited applications.',\n",
       "  'Title: an fpt algorithm for planar multicuts with sources and sinks on the outer face\\nAbstract: Given a list of k source-sink pairs in an edge-weighted graph G, the minimum multicut problem consists in selecting a set of edges of minimum total weight in G, such that removing these edges leaves no path from each source to its corresponding sink. To the best of our knowledge, no non-trivial FPT result for special cases of this problem, which is APX-hard in general graphs for any fixed k>2, is known with respect to k only. When the graph G is planar, this problem is known to be polynomial-time solvable if k=O(1), but cannot be FPT with respect to k under the Exponential Time Hypothesis. #R##N#In this paper, we show that, if G is planar and in addition all sources and sinks lie on the outer face, then this problem does admit an FPT algorithm when parameterized by k (although it remains APX-hard when k is part of the input, even in stars). To do this, we provide a new characterization of optimal solutions in this case, and then use it to design a \"divide-and-conquer\" approach: namely, some edges that are part of any such solution actually define an optimal solution for a polynomial-time solvable multiterminal variant of the problem on some of the sources and sinks (which can be identified thanks to a reduced enumeration phase). Removing these edges from the graph cuts it into several smaller instances, which can then be solved recursively.',\n",
       "  'Title: passgan a deep learning approach for password guessing\\nAbstract: State-of-the-art password guessing tools, such as HashCat and John the Ripper, enable users to check billions of passwords per second against password hashes. In addition to performing straightforward dictionary attacks, these tools can expand password dictionaries using password generation rules, such as concatenation of words (e.g., \"password123456\") and leet speak (e.g., \"password\" becomes \"p4s5w0rd\"). Although these rules work well in practice, expanding them to model further passwords is a laborious task that requires specialized expertise. To address this issue, in this paper we introduce PassGAN, a novel approach that replaces human-generated password rules with theory-grounded machine learning algorithms. Instead of relying on manual password analysis, PassGAN uses a Generative Adversarial Network (GAN) to autonomously learn the distribution of real passwords from actual password leaks, and to generate high-quality password guesses. Our experiments show that this approach is very promising. When we evaluated PassGAN on two large password datasets, we were able to surpass rule-based and state-of-the-art machine learning password guessing tools. However, in contrast with the other tools, PassGAN achieved this result without any a-priori knowledge on passwords or common password structures. Additionally, when we combined the output of PassGAN with the output of HashCat, we were able to match 51%-73% more passwords than with HashCat alone. This is remarkable, because it shows that PassGAN can autonomously extract a considerable number of password properties that current state-of-the art rules do not encode.',\n",
       "  \"Title: too far to see not really pedestrian detection with scale aware localization policy\\nAbstract: A major bottleneck of pedestrian detection lies on the sharp performance deterioration in the presence of small-size pedestrians that are relatively far from the camera. Motivated by the observation that pedestrians of disparate spatial scales exhibit distinct visual appearances, we propose in this paper an active pedestrian detector that explicitly operates over multiple-layer neuronal representations of the input still image. More specifically, convolutional neural nets, such as ResNet and faster R-CNNs, are exploited to provide a rich and discriminative hierarchy of feature representations, as well as initial pedestrian proposals. Here each pedestrian observation of distinct size could be best characterized in terms of the ResNet feature representation at a certain layer of the hierarchy. Meanwhile, initial pedestrian proposals are attained by the faster R-CNNs techniques, i.e., region proposal network and follow-up region of interesting pooling layer employed right after the specific ResNet convolutional layer of interest, to produce joint predictions on the bounding-box proposals' locations and categories (i.e., pedestrian or not). This is engaged as an input to our active detector, where for each initial pedestrian proposal, a sequence of coordinate transformation actions is carried out to determine its proper x-y 2D location and the layer of feature representation, or eventually terminated as being background. Empirically our approach is demonstrated to produce overall lower detection errors on widely used benchmarks, and it works particularly well with far-scale pedestrians. For example, compared with 60.51% log-average miss rate of the state-of-the-art MS-CNN for far-scale pedestrians (those below 80 pixels in bounding-box height) of the Caltech benchmark, the miss rate of our approach is 41.85%, with a notable reduction of 18.66%.\",\n",
       "  'Title: the satisfiability problem for boolean set theory with a choice correspondence\\nAbstract: Given a set U of alternatives, a choice (correspondence) on U is a contractive map c defined on a family Omega of nonempty subsets of U. Semantically, a choice c associates to each menu A in Omega a nonempty subset c(A) of A comprising all elements of A that are deemed selectable by an agent. A choice on U is total if its domain is the powerset of U minus the empty set, and partial otherwise. According to the theory of revealed preferences, a choice is rationalizable if it can be retrieved from a binary relation on U by taking all maximal elements of each menu. It is well-known that rationalizable choices are characterized by the satisfaction of suitable axioms of consistency, which codify logical rules of selection within menus. For instance, WARP (Weak Axiom of Revealed Preference) characterizes choices rationalizable by a transitive relation. Here we study the satisfiability problem for unquantified formulae of an elementary fragment of set theory involving a choice function symbol c, the Boolean set operators and the singleton, the equality and inclusion predicates, and the propositional connectives. In particular, we consider the cases in which the interpretation of c satisfies any combination of two specific axioms of consistency, whose conjunction is equivalent to WARP. In two cases we prove that the related satisfiability problem is NP-complete, whereas in the remaining cases we obtain NP-completeness under the additional assumption that the number of choice terms is constant.',\n",
       "  'Title: the descriptive complexity of modal mu model checking games\\nAbstract: This paper revisits the well-established relationship between the modal mu calculus and parity games to show that it is even more robust than previously known. It addresses the question of whether the descriptive complexity of modal mu model-checking games, previously known to depend on the syntactic complexity of a formula, depends in fact on its semantic complexity. It shows that up to formulas of semantic co-Buchi complexity, the descriptive complexity of their model-checking games coincides exactly with their semantic complexity. Beyond co-Buchi, the descriptive complexity of the model-checking parity games of a formula is shown to be an upper bound on its semantic complexity; whether it is also a lower bound remains an open question.',\n",
       "  'Title: progress space tradeoffs in single writer memory implementations\\nAbstract: Most algorithms designed for shared-memory distributed systems assume the single-writer multi-reader (SWMR) setting where each process is provided with a unique register readable by all. In a system where computation is performed by a bounded number n of processes coming from a very large (possibly unbounded) set of potential participants, the assumption of a SWMR memory is no longer reasonable. If only a bounded number of multi-writer multi-reader (MWMR) registers are provided, we cannot rely on an a priori assignment of processes to registers. In this setting, simulating SWMR memory, or equivalently, ensuring stable writing (i.e., every written value persists in the memory), is desirable. #R##N#In this paper, we propose a SWMR simulation that adapts the number of MWMR registers used to the desired progress condition. For any given k from 1 to n, we present an algorithm that uses only n+k-1 registers to simulate a k-lock-free SWMR memory. We also give a matching lower bound of n+1 registers required for the case of 2-lock-freedom, which supports our conjectures that the algorithm is space-optimal. Our lower bound holds for the strictly weaker progress condition of 2-obstruction-freedom, which suggests that the space complexity for k-obstruction-free and k-lock-free SWMR simulations might coincide.',\n",
       "  'Title: one bit sphere decoding for uplink massive mimo systems with one bit adcs\\nAbstract: This paper presents a low-complexity near-maximum-likelihood-detection (near-MLD) algorithm called one-bit-sphere-decoding for an uplink massive multiple-input multiple-output (MIMO) system with one-bit analog-to-digital converters (ADCs). The idea of the proposed algorithm is to estimate the transmitted symbol vector sent by uplink users (a codeword vector) by searching over a sphere, which contains a collection of codeword vectors close to the received signal vector at the base station in terms of a weighted Hamming distance. To reduce the computational complexity for the construction of the sphere, the proposed algorithm divides the received signal vector into multiple sub-vectors each with reduced dimension. Then, it generates multiple spheres in parallel, where each sphere is centered at the sub-vector and contains a list of sub-codeword vectors. The detection performance of the proposed algorithm is also analyzed by characterizing the probability that the proposed algorithm performs worse than the MLD. The analysis shows how the dimension of each sphere and the size of the sub-codeword list are related to the performance-complexity tradeoff achieved by the proposed algorithm. Simulation results demonstrate that the proposed algorithm achieves near-MLD performance, while reducing the computational complexity compared to the existing MLD method.',\n",
       "  'Title: an inversion based learning approach for improving impromptu trajectory tracking of robots with non minimum phase dynamics\\nAbstract: This paper presents a learning-based approach for impromptu trajectory tracking of non-minimum phase systems -- systems with unstable inverse dynamics. In the control systems literature, inversion-based feedforward approaches are commonly used for improving the trajectory tracking performance; however, these approaches are not directly applicable to non-minimum phase systems due to the inherent instability. In order to resolve the instability issue, they assume that models of the systems are known and have dealt with the non-minimum phase systems by pre-actuation or inverse approximation techniques. In this work, we extend our deep-neural-network-enhanced impromptu trajectory tracking approach to the challenging case of non-minimum phase systems. Through theoretical discussions, simulations, and experiments, we show the stability and effectiveness of our proposed learning approach. In fact, for a known system, our approach performs equally well or better as a typical model-based approach but does not require a prior model of the system. Interestingly, our approach also shows that including more information in training (as is commonly assumed to be useful) does not lead to better performance but may trigger instability issues and impede the effectiveness of the overall approach.',\n",
       "  'Title: safe robust reachability analysis of hybrid systems\\nAbstract: Abstract   Hybrid systems—more precisely, their mathematical models—can exhibit behaviors, like Zeno behaviors, that are absent in purely discrete or purely continuous systems. First, we observe that, in this context, the usual definition of reachability—namely, the reflexive and transitive closure of a transition relation—can be unsafe, i.e., it may compute a proper subset of the set of states reachable in finite time from a set of initial states. Therefore, we propose safe reachability, which always computes a superset of the set of reachable states.  Second, in safety analysis of hybrid and continuous systems, it is important to ensure that a reachability analysis is also robust w.r.t. small perturbations to the set of initial states and to the system itself, since discrepancies between a system and its mathematical models are unavoidable. We show that, under certain conditions, the best Scott continuous approximation of an analysis A is also its best robust approximation. Finally, we exemplify the gap between the set of reachable states and the supersets computed by safe reachability and its best robust approximation.',\n",
       "  'Title: validity guided synthesis of reactive systems from assume guarantee contracts\\nAbstract: Automated synthesis of reactive systems from specifications has been a topic of research for decades. Recently, a variety of approaches have been proposed to extend synthesis of reactive systems from proposi- tional specifications towards specifications over rich theories. We propose a novel, completely automated approach to program synthesis which reduces the problem to deciding the validity of a set of forall-exists formulas. In spirit of IC3 / PDR, our problem space is recursively refined by blocking out regions of unsafe states, aiming to discover a fixpoint that describes safe reactions. If such a fixpoint is found, we construct a witness that is directly translated into an implementation. We implemented the algorithm on top of the JKind model checker, and exercised it against contracts written using the Lustre specification language. Experimental results show how the new algorithm outperforms JKinds already existing synthesis procedure based on k-induction and addresses soundness issues in the k-inductive approach with respect to unrealizable results.',\n",
       "  'Title: e 2 bows an end to end bag of words model via deep convolutional neural network\\nAbstract: Traditional Bag-of-visual Words (BoWs) model is commonly generated with many steps including local feature extraction, codebook generation, and feature quantization, etc. Those steps are relatively independent with each other and are hard to be jointly optimized. Moreover, the dependency on hand-crafted local feature makes BoWs model not effective in conveying high-level semantics. These issues largely hinder the performance of BoWs model in large-scale image applications. To conquer these issues, we propose an End-to-End BoWs (E$^2$BoWs) model based on Deep Convolutional Neural Network (DCNN). Our model takes an image as input, then identifies and separates the semantic objects in it, and finally outputs the visual words with high semantic discriminative power. Specifically, our model firstly generates Semantic Feature Maps (SFMs) corresponding to different object categories through convolutional layers, then introduces Bag-of-Words Layers (BoWL) to generate visual words for each individual feature map. We also introduce a novel learning algorithm to reinforce the sparsity of the generated E$^2$BoWs model, which further ensures the time and memory efficiency. We evaluate the proposed E$^2$BoWs model on several image search datasets including CIFAR-10, CIFAR-100, MIRFLICKR-25K and NUS-WIDE. Experimental results show that our method achieves promising accuracy and efficiency compared with recent deep learning based retrieval works.',\n",
       "  'Title: throughput and robustness guaranteed beam tracking for mmwave wireless networks\\nAbstract: With the increasing demand of ultra-high-speed wireless communications and the existing low frequency band (e.g., sub-6GHz) becomes more and more crowded, millimeter-wave (mmWave) with large spectra available is considered as the most promising frequency band for future wireless communications. Since the mmWave suffers a serious path-loss, beamforming techniques shall be adopted to concentrate the transmit power and receive region on a narrow beam for achieving long distance communications. However, the mobility of users will bring frequent beam handoff, which will decrease the quality of experience (QoE). Therefore, efficient beam tracking mechanism should be carefully researched. However, the existing beam tracking mechanisms concentrate on system throughput maximization without considering beam handoff and link robustness. This paper proposes a throughput and robustness guaranteed beam tracking mechanism for mobile mmWave communication systems which takes account of both system throughput and handoff probability. Simulation results show that the proposed throughput and robustness guaranteed beam tracking mechanism can provide better performance than the other beam tracking mechanisms.',\n",
       "  'Title: a new balanced subdivision of a simple polygon for time space trade off algorithms\\nAbstract: We are given a read-only memory for input and a write-only stream for output. For a positive integer parameter s, an s-workspace algorithm is an algorithm using only $O(s)$ words of workspace in addition to the memory for input. In this paper, we present an $O(n^2/s)$-time $s$-workspace algorithm for subdividing a simple polygon into $O(\\\\min\\\\{n/s,s\\\\})$ subpolygons of complexity $O(\\\\max\\\\{n/s,s\\\\})$. #R##N#As applications of the subdivision, the previously best known time-space trade-offs for the following three geometric problems are improved immediately: (1) computing the shortest path between two points inside a simple $n$-gon, (2) computing the shortest path tree from a point inside a simple $n$-gon, (3) computing a triangulation of a simple $n$-gon. In addition, we improve the algorithm for the second problem even further.',\n",
       "  'Title: unsupervised contact learning for humanoid estimation and control\\nAbstract: This work presents a method for contact state estimation using fuzzy clustering to learn contact probability for full, six-dimensional humanoid contacts. The data required for training is solely from proprioceptive sensors - endeffector contact wrench sensors and inertial measurement units (IMUs) - and the method is completely unsupervised. The resulting cluster means are used to efficiently compute the probability of contact in each of the six endeffector degrees of freedom (DoFs) independently. This clustering-based contact probability estimator is validated in a kinematics-based base state estimator in a simulation environment with realistic added sensor noise for locomotion over rough, low-friction terrain on which the robot is subject to foot slip and rotation. The proposed base state estimator which utilizes these six DoF contact probability estimates is shown to perform considerably better than that which determines kinematic contact constraints purely based on measured normal force.',\n",
       "  'Title: modular verification of interrupt driven software\\nAbstract: Interrupts have been widely used in safety-critical computer systems to handle outside stimuli and interact with the hardware, but reasoning about interrupt-driven software remains a difficult task. Although a number of static verification techniques have been proposed for interrupt-driven software, they often rely on constructing a monolithic verification model. Furthermore, they do not precisely capture the complete execution semantics of interrupts such as nested invocations of interrupt handlers. To overcome these limitations, we propose an abstract interpretation framework for static verification of interrupt-driven software that first analyzes each interrupt handler in isolation as if it were a sequential program, and then propagates the result to other interrupt handlers. This iterative process continues until results from all interrupt handlers reach a fixed point. Since our method never constructs the global model, it avoids the up-front blowup in model construction that hampers existing, non-modular, verification techniques. We have evaluated our method on 35 interrupt-driven applications with a total of 22,541 lines of code. Our results show the method is able to quickly and more accurately analyze the behavior of interrupts.',\n",
       "  'Title: dynamic reconfiguration of mission parameters in underwater human robot collaboration\\nAbstract: This paper presents a real-time programming and parameter reconfiguration method for autonomous underwater robots in human-robot collaborative tasks. Using a set of intuitive and meaningful hand gestures, we develop a syntactically simple framework that is computationally more efficient than a complex, grammar-based approach. In the proposed framework, a convolutional neural network is trained to provide accurate hand gesture recognition; subsequently, a finite-state machine-based deterministic model performs efficient gesture-to-instruction mapping, and further improves robustness of the interaction scheme. The key aspect of this framework is that it can be easily adopted by divers for communicating simple instructions to underwater robots without using artificial tags such as fiducial markers, or requiring them to memorize a potentially complex set of language rules. Extensive experiments are performed both on field-trial data and through simulation, which demonstrate the robustness, efficiency, and portability of this framework in a number of different scenarios. Finally, a user interaction study is presented that illustrates the gain in usability of our proposed interaction framework compared to the existing methods for underwater domains.',\n",
       "  'Title: replicability analysis for natural language processing testing significance with multiple datasets\\nAbstract: With the ever-growing amounts of textual data from a large variety of languages, domains, and genres, it has become standard to evaluate NLP algorithms on multiple datasets in order to ensure consistent performance across heterogeneous setups. However, such multiple comparisons pose significant challenges to traditional statistical analysis methods in NLP and can lead to erroneous conclusions. In this paper, we propose a Replicability Analysis framework for a statistically sound analysis of multiple comparisons between algorithms for NLP tasks. We discuss the theoretical advantages of this framework over the current, statistically unjustified, practice in the NLP literature, and demonstrate its empirical value across four applications: multi-domain dependency parsing, multilingual POS tagging, cross-domain sentiment classification and word similarity prediction.',\n",
       "  'Title: differential dissipativity theory for dominance analysis\\nAbstract: High-dimensional systems that have a low-dimensional dominant behavior allow for model reduction and simplified analysis. We use differential analysis to formalize this important concept in a nonlinear setting. We show that dominance can be studied through linear dissipation inequalities and an interconnection theory that closely mimics the classical analysis of stability by means of dissipativity theory. In this approach, stability is seen as the limiting situation where the dominant behavior is 0-dimensional. The generalization opens novel tractable avenues to study multistability through 1-dominance and limit cycle oscillations through 2-dominance.',\n",
       "  'Title: theoretical computer science for the working category theorist\\nAbstract: Theoretical computer science discusses foundational issues about computations. It asks and answers questions such as \"What is a computation?\", \"What is computable?\", \"What is efficiently computable?\",\"What is information?\", \"What is random?\", \"What is an algorithm?\", etc. We will present many of the major themes and theorems with the basic language of category theory. Surprisingly, many interesting theorems and concepts of theoretical computer science are easy consequences of functoriality and composition when you look at the right categories and functors connecting them.',\n",
       "  'Title: 4 connected planar graphs are in b 3 epg\\nAbstract: We show that every 4-connected planar graph has a $B_3$-EPG representation, i.e., every vertex is represented by a curve on the grid with at most three bends, and two vertices are adjacent if and only if the corresponding curves share an edge of the grid. Our construction is based on a modification of the representation by touching thickened $L$-shapes proposed by Goncalves et al.',\n",
       "  'Title: rainbow combining improvements in deep reinforcement learning\\nAbstract: The deep reinforcement learning community has made several independent improvements to the DQN algorithm. However, it is unclear which of these extensions are complementary and can be fruitfully combined. This paper examines six extensions to the DQN algorithm and empirically studies their combination. Our experiments show that the combination provides state-of-the-art performance on the Atari 2600 benchmark, both in terms of data efficiency and final performance. We also provide results from a detailed ablation study that shows the contribution of each component to overall performance.',\n",
       "  \"Title: characterizing a spectrum of audience interactivity\\nAbstract: This research overviews and iterates on a Spectrum of Audience Interactivity, a tool to help designers and artists consider the dimensions and range of audience interactivity characteristics. Previous research has introduced a spectrum of audience participation in live performances based on children's co-design sessions. We expand on these findings with a review of literature in theater, games, and theme parks paired with expert interviews in those domains. From this work, we present a framework describing the spheres of audience influence, a new spectrum of audience interactivity, and a set of orthogonal spectrum dimensions that collectively describe audience interactivity. We pair our new framework with design considerations for practitioners; interactivity should be thoughtfully designed, should anticipate audience challenges, and can support multiple levels of the audience interactivity.\",\n",
       "  'Title: the path to path traced movies\\nAbstract: Path tracing is one of several techniques to render photorealistic images by simulating the physics of light propagation within a scene. The roots of path tracing are outside of computer graphics, in the Monte Carlo simulations developed for neutron transport. A great strength of path tracing is that it is conceptually, mathematically, and often-times algorithmically simple and elegant, yet it is very general. Until recently, however, brute-force path tracing techniques were simply too noisy and slow to be practical for movie production rendering. They therefore received little usage outside of academia, except perhaps to generate an occasional reference image to validate the correctness of other faster but less general rendering algorithms. The last ten years have seen a dramatic shift in this balance, and path tracing techniques are now widely used. This shift was partially fueled by steadily increasing computational power and memory, but also by significant improvements in sampling, rendering, and denoising techniques. In this survey, we provide an overview of path tracing and highlight important milestones in its development that have led to it becoming the preferred movie rendering technique today.',\n",
       "  'Title: the quest for scalability and accuracy multi level simulation of the internet of things\\nAbstract: This paper presents a methodology for simulating the Internet of Things (IoT) using multi-level simulation models. With respect to conventional simulators, this approach allows us to tune the level of detail of different parts of the model without compromising the scalability of the simulation. As a use case, we have developed a two-level simulator to study the deployment of smart services over rural territories. The higher level is base on a coarse grained, agent-based adaptive parallel and distributed simulator. When needed, this simulator spawns OMNeT++ model instances to evaluate in more detail the issues concerned with wireless communications in restricted areas of the simulated world. The performance evaluation confirms the viability of multi-level simulations for IoT environments.',\n",
       "  'Title: reconstruction of hidden representation for robust feature extraction\\nAbstract: This article aims to develop a new and robust approach to feature representation. Motivated by the success of Auto-Encoders, we first theoretically analyze and summarize the general properties of all algorithms that are based on traditional Auto-Encoders: (1) The reconstruction error of the input cannot be lower than a lower bound, which can be viewed as a guiding principle for reconstructing the input. Additionally, when the input is corrupted with noises, the reconstruction error of the corrupted input also cannot be lower than a lower bound. (2) The reconstruction of a hidden representation achieving its ideal situation is the necessary condition for the reconstruction of the input to reach the ideal state. (3) Minimizing the Frobenius norm of the Jacobian matrix of the hidden representation has a deficiency and may result in a much worse local optimum value. We believe that minimizing the reconstruction error of the hidden representation is more robust than minimizing the Frobenius norm of the Jacobian matrix of the hidden representation. Based on the above analysis, we propose a new model termed Double Denoising Auto-Encoders (DDAEs), which uses corruption and reconstruction on both the input and the hidden representation. We demonstrate that the proposed model is highly flexible and extensible and has a potentially better capability to learn invariant and robust feature representations. We also show that our model is more robust than Denoising Auto-Encoders (DAEs) for dealing with noises or inessential features. Furthermore, we detail how to train DDAEs with two different pretraining methods by optimizing the objective function in a combined and separate manner, respectively. Comparative experiments illustrate that the proposed model is significantly better for representation learning than the state-of-the-art models.',\n",
       "  'Title: a lambda calculus for transfinite arrays unifying arrays and streams\\nAbstract: Array programming languages allow for concise and generic formulations of numerical algorithms, thereby providing a huge potential for program optimisation such as fusion, parallelisation, etc. One of the restrictions that these languages typically have is that the number of elements in every array has to be finite. This means that implementing streaming algorithms in such languages requires new types of data structures, with operations that are not immediately compatible with existing array operations or compiler optimisations. #R##N#In this paper, we propose a design for a functional language that natively supports infinite arrays. We use ordinal numbers to introduce the notion of infinity in shapes and indices. By doing so, we obtain a calculus that naturally extends existing array calculi and, at the same time, allows for recursive specifications as they are found in stream- and list-based settings. Furthermore, the main language construct that can be thought of as an $n$-fold cons operator gives rise to expressing transfinite recursion in data, something that lists or streams usually do not support. This makes it possible to treat the proposed calculus as a unifying theory of arrays, lists and streams. We give an operational semantics of the proposed language, discuss design choices that we have made, and demonstrate its expressibility with several examples. We also demonstrate that the proposed formalism preserves a number of well-known universal equalities from array/list/stream theories, and discuss implementation-related challenges.',\n",
       "  'Title: libact pool based active learning in python\\nAbstract: libact is a Python package designed to make active learning easier for general users. The package not only implements several popular active learning strategies, but also features the active-learning-by-learning meta-algorithm that assists the users to automatically select the best strategy on the fly. Furthermore, the package provides a unified interface for implementing more strategies, models and application-specific labelers. The package is open-source on Github, and can be easily installed from Python Package Index repository.',\n",
       "  'Title: democratizing online controlled experiments at booking com\\nAbstract: There is an extensive literature about online controlled experiments, both on the statistical methods available to analyze experiment results as well as on the infrastructure built by several large scale Internet companies but also on the organizational challenges of embracing online experiments to inform product development. At Booking.com we have been conducting evidenced based product development using online experiments for more than ten years. Our methods and infrastructure were designed from their inception to reflect Booking.com culture, that is, with democratization and decentralization of experimentation and decision making in mind. #R##N#In this paper we explain how building a central repository of successes and failures to allow for knowledge sharing, having a generic and extensible code library which enforces a loose coupling between experimentation and business logic, monitoring closely and transparently the quality and the reliability of the data gathering pipelines to build trust in the experimentation infrastructure, and putting in place safeguards to enable anyone to have end to end ownership of their experiments have allowed such a large organization as Booking.com to truly and successfully democratize experimentation.',\n",
       "  \"Title: an optimal choice dictionary\\nAbstract: A choice dictionary is a data structure that can be initialized with a parameter $n\\\\in\\\\{1,2,\\\\ldots\\\\}$ and subsequently maintains an initially empty subset $S$ of $\\\\{1,\\\\ldots,n\\\\}$ under insertion, deletion, membership queries and an operation 'choice' that returns an arbitrary element of $S$. The choice dictionary is fundamental in space-efficient computing and has numerous applications. The best previous choice dictionary can be initialized with $n$ and a second parameter $t\\\\in\\\\{1,2,\\\\ldots\\\\}$ in constant time and subsequently executes all operations in $O(t)$ time and occupies $n+O(n({t/w})^t+\\\\log n)$ bits on a word RAM with a word length of $w=\\\\Omega(\\\\log n)$ bits. We describe a new choice dictionary that, following a constant-time initialization, executes all operations in constant time and, in addition to the space needed to store $n$, occupies only $n+1$ bits, which is shown to be optimal if $w=o(n)$. Allowed $\\\\lceil\\\\log_2(n+1)\\\\rceil$ bits of additional space, the new data structure also supports iteration over the set $S$ in constant time per element.\",\n",
       "  \"Title: maximum value matters finding hot topics in scholarly fields\\nAbstract: Finding hot topics in scholarly fields can help researchers to keep up with the latest concepts, trends, and inventions in their field of interest. Due to the rarity of complete large-scale scholarly data, earlier studies target this problem based on manual topic extraction from a limited number of domains, with their focus solely on a single feature such as coauthorship, citation relations, and etc. Given the compromised effectiveness of such predictions, in this paper we use a real scholarly dataset from Microsoft Academic Graph, which provides more than 12000 topics in the field of Computer Science (CS), including 1200 venues, 14.4 million authors, 30 million papers and their citation relations over the period of 1950 till now. Aiming to find the topics that will trend in CS area, we innovatively formalize a hot topic prediction problem where, with joint consideration of both inter- and intra-topical influence, 17 different scientific features are extracted for comprehensive description of topic status. By leveraging all those 17 features, we observe good accuracy of topic scale forecasting after 5 and 10 years with R2 values of 0.9893 and 0.9646, respectively. Interestingly, our prediction suggests that the maximum value matters in finding hot topics in scholarly fields, primarily from three aspects: (1) the maximum value of each factor, such as authors' maximum h-index and largest citation number, provides three times the amount of information than the average value in prediction; (2) the mutual influence between the most correlated topics serve as the most telling factor in long-term topic trend prediction, interpreting that those currently exhibiting the maximum growth rates will drive the correlated topics to be hot in the future; (3) we predict in the next 5 years the top 100 fastest growing (maximum growth rate) topics that will potentially get the major attention in CS area.\",\n",
       "  'Title: maintaining a distributed spanning forest in highly dynamic networks\\nAbstract: Highly dynamic networks are characterized by frequent changes in the availability of communication links. These networks are often partitioned into several components, which split and merge unpredictably. We present a distributed algorithm that maintains a forest of (as few as possible) spanning trees in such a network, with no restriction on the rate of change. Our algorithm is inspired by high-level graph transformations, which we adapt here in a (synchronous) message passing model for dynamic networks. The resulting algorithm has the following properties: First, every decision is purely local---in each round, a node only considers its role and that of its neighbors in the tree, with no further information propagation (in particular, no wave mechanisms). Second, whatever the rate and scale of the changes, the algorithm guarantees that, by the end of every round, the network is covered by a forest of spanning trees in which 1) no cycle occur, 2) every node belongs to exactly one tree, and 3) every tree contains exactly one root (or token). We primarily focus on the correctness of this algorithm, which is established rigorously. While performance is not the main focus, we suggest new complexity metrics for such problems, and report on preliminary experimentation results validating our algorithm in a practical scenario.',\n",
       "  \"Title: toward reducing crop spoilage and increasing small farmer profits in india a simultaneous hardware and software solution\\nAbstract: India's agricultural system has been facing a severe problem of crop wastage. A key contributing factor to this problem is that many small farmers lack access to reliable cold storage that extends crop shelf-life. To avoid having leftover crops that spoil, these farmers often sell their crops at unfavorable low prices. Inevitably, not all crops are sold before spoilage. Even if the farmers have access to cold storage, the farmers may not know how long to hold different crops in cold storage for, which hinges on strategizing over when and where to sell their harvest. In this note, we present progress toward a simultaneous hardware and software solution that aims to help farmers reduce crop spoilage and increase their profits. The hardware is a cost-effective solar-powered refrigerator and control unit. The software refers to a produce price forecasting system, for which we have tested a number of machine learning methods. Note that unlike standard price forecasting tasks such as for stock market data, the produce price data from predominantly rural Indian markets have a large amount of missing values. In developing our two-pronged solution, we are actively working with farmers at two pilot sites in Karnataka and Odisha.\",\n",
       "  \"Title: clickbait detection in tweets using self attentive network\\nAbstract: Clickbait detection in tweets remains an elusive challenge. In this paper, we describe the solution for the Zingel Clickbait Detector at the Clickbait Challenge 2017, which is capable of evaluating each tweet's level of click baiting. We first reformat the regression problem as a multi-classification problem, based on the annotation scheme. To perform multi-classification, we apply a token-level, self-attentive mechanism on the hidden states of bi-directional Gated Recurrent Units (biGRU), which enables the model to generate tweets' task-specific vector representations by attending to important tokens. The self-attentive neural network can be trained end-to-end, without involving any manual feature engineering. Our detector ranked first in the final evaluation of Clickbait Challenge 2017.\",\n",
       "  'Title: deeprank a new deep architecture for relevance ranking in information retrieval\\nAbstract: This paper concerns a deep learning approach to relevance ranking in information retrieval (IR). Existing deep IR models such as DSSM and CDSSM directly apply neural networks to generate ranking scores, without explicit understandings of the relevance. According to the human judgement process, a relevance label is generated by the following three steps: 1) relevant locations are detected; 2) local relevances are determined; 3) local relevances are aggregated to output the relevance label. In this paper we propose a new deep learning architecture, namely DeepRank, to simulate the above human judgment process. Firstly, a detection strategy is designed to extract the relevant contexts. Then, a measure network is applied to determine the local relevances by utilizing a convolutional neural network (CNN) or two-dimensional gated recurrent units (2D-GRU). Finally, an aggregation network with sequential integration and term gating mechanism is used to produce a global relevance score. DeepRank well captures important IR characteristics, including exact/semantic matching signals, proximity heuristics, query term importance, and diverse relevance requirement. Experiments on both benchmark LETOR dataset and a large scale clickthrough data show that DeepRank can significantly outperform learning to ranking methods, and existing deep learning methods.',\n",
       "  'Title: learning from incomplete ratings using nonlinear multi layer semi nonnegative matrix factorization\\nAbstract: Recommender systems problems witness a growing interest for finding better learning algorithms for personalized information. Matrix factorization that estimates the user liking for an item by taking an inner product on the latent features of users and item have been widely studied owing to its better accuracy and scalability. However, it is possible that the mapping between the latent features learned from these and the original features contains rather complex nonlinear hierarchical information, that classical linear matrix factorization can not capture. In this paper, we aim to propose a novel multilayer non-linear approach to a variant of nonnegative matrix factorization (NMF) to learn such factors from the incomplete ratings matrix. Firstly, we construct a user-item matrix with explicit ratings, secondly we learn latent factors for representations of users and items from the designed nonlinear multi-layer approach. Further, the architecture is built with different nonlinearities using adaptive gradient optimizer to better learn the latent factors in this space. We show that by doing so, our model is able to learn low-dimensional representations that are better suited for recommender systems on several benchmark datasets.',\n",
       "  'Title: learning one hidden layer neural networks with landscape design\\nAbstract: We consider the problem of learning a one-hidden-layer neural network: we assume the input $x\\\\in \\\\mathbb{R}^d$ is from Gaussian distribution and the label $y = a^\\\\top \\\\sigma(Bx) + \\\\xi$, where $a$ is a nonnegative vector in $\\\\mathbb{R}^m$ with $m\\\\le d$, $B\\\\in \\\\mathbb{R}^{m\\\\times d}$ is a full-rank weight matrix, and $\\\\xi$ is a noise vector. We first give an analytic formula for the population risk of the standard squared loss and demonstrate that it implicitly attempts to decompose a sequence of low-rank tensors simultaneously. #R##N#Inspired by the formula, we design a non-convex objective function $G(\\\\cdot)$ whose landscape is guaranteed to have the following properties: 1. All local minima of $G$ are also global minima. #R##N#2. All global minima of $G$ correspond to the ground truth parameters. #R##N#3. The value and gradient of $G$ can be estimated using samples. #R##N#With these properties, stochastic gradient descent on $G$ provably converges to the global minimum and learn the ground-truth parameters. We also prove finite sample complexity result and validate the results by simulations.',\n",
       "  'Title: low precision rnns quantizing rnns without losing accuracy\\nAbstract: Similar to convolution neural networks, recurrent neural networks (RNNs) typically suffer from over-parameterization. Quantizing bit-widths of weights and activations results in runtime efficiency on hardware, yet it often comes at the cost of reduced accuracy. This paper proposes a quantization approach that increases model size with bit-width reduction. This approach will allow networks to perform at their baseline accuracy while still maintaining the benefits of reduced precision and overall model size reduction.',\n",
       "  'Title: an fpt algorithm beating 2 approximation for k cut\\nAbstract: In the $k$-Cut problem, we are given an edge-weighted graph $G$ and an integer $k$, and have to remove a set of edges with minimum total weight so that $G$ has at least $k$ connected components. Prior work on this problem gives, for all $h \\\\in [2,k]$, a $(2-h/k)$-approximation algorithm for $k$-cut that runs in time $n^{O(h)}$. Hence to get a $(2 - \\\\varepsilon)$-approximation algorithm for some absolute constant $\\\\varepsilon$, the best runtime using prior techniques is $n^{O(k\\\\varepsilon)}$. Moreover, it was recently shown that getting a $(2 - \\\\varepsilon)$-approximation for general $k$ is NP-hard, assuming the Small Set Expansion Hypothesis. #R##N#If we use the size of the cut as the parameter, an FPT algorithm to find the exact $k$-Cut is known, but solving the $k$-Cut problem exactly is $W[1]$-hard if we parameterize only by the natural parameter of $k$. An immediate question is: \\\\emph{can we approximate $k$-Cut better in FPT-time, using $k$ as the parameter?} #R##N#We answer this question positively. We show that for some absolute constant $\\\\varepsilon > 0$, there exists a $(2 - \\\\varepsilon)$-approximation algorithm that runs in time $2^{O(k^6)} \\\\cdot \\\\widetilde{O} (n^4)$. This is the first FPT algorithm that is parameterized only by $k$ and strictly improves the $2$-approximation.',\n",
       "  \"Title: sea level anomaly prediction using recurrent neural networks\\nAbstract: Sea level change, one of the most dire impacts of anthropogenic global warming, will affect a large amount of the world's population. However, sea level change is not uniform in time and space, and the skill of conventional prediction methods is limited due to the ocean's internal variabi-lity on timescales from weeks to decades. Here we study the potential of neural network methods which have been used successfully in other applications, but rarely been applied for this task. We develop a combination of a convolutional neural network (CNN) and a recurrent neural network (RNN) to ana-lyse both the spatial and the temporal evolution of sea level and to suggest an independent, accurate method to predict interannual sea level anomalies (SLA). We test our method for the northern and equatorial Pacific Ocean, using gridded altimeter-derived SLA data. We show that the used network designs outperform a simple regression and that adding a CNN improves the skill significantly. The predictions are stable over several years.\",\n",
       "  'Title: machine learning based fast power integrity classifier\\nAbstract: In this paper, we proposed a new machine learning based fast power integrity classifier that quickly flags the EM/IR hotspots. We discussed the features to extract to describe the power grid, cell power density, routing impact and controlled collapse chip connection (C4) bumps, etc. The continuous and discontinuous cases are identified and treated using different machine learning models. Nearest neighbors, random forest and neural network models are compared to select the best performance candidates. Experiments are run on open source benchmark, and result is showing promising prediction accuracy.',\n",
       "  'Title: characterizing sparse connectivity patterns in neural networks\\nAbstract: We propose a novel way of reducing the number of parameters in the storage-hungry fully connected layers of a neural network by using pre-defined sparsity, where the majority of connections are absent prior to starting training. Our results indicate that convolutional neural networks can operate without any loss of accuracy at less than half percent classification layer connection density, or less than 5 percent overall network connection density. We also investigate the effects of pre-defining the sparsity of networks with only fully connected layers. Based on our sparsifying technique, we introduce the ‘scatter’ metric to characterize the quality of a particular connection pattern. As proof of concept, we show results on CIFAR, MNIST and a new dataset on classifying Morse code symbols, which highlights some interesting trends and limits of sparse connection patterns.',\n",
       "  'Title: rudin shapiro like polynomials with maximum asymptotic merit factor\\nAbstract: Borwein and Mossinghoff investigated the Rudin-Shapiro-like polynomials, which are infinite families of Littlewood polynomials, that is, polynomials whose coefficients are all in $\\\\{-1,1\\\\}$. Each family of Rudin-Shapiro-like polynomials is obtained from a starting polynomial (which we call the seed) by a recursive construction. These polynomials can be regarded as binary sequences. Borwein and Mossinghoff show that the asymptotic autocorrelation merit factor for any such family is at most $3$, and found the seeds of length $40$ or less that produce the maximum asymptotic merit factor of $3$. The definition of Rudin-Shapiro-like polynomials was generalized by Katz, Lee, and Trunov to include polynomials with arbitrary complex coefficients, with the sole condition that the seed polynomial must have a nonzero constant coefficient. They proved that the maximum asymptotic merit factor is $3$ for this larger class. Here we show that a family of such Rudin-Shapiro-like polynomials achieves asymptotic merit factor $3$ if and only if the seed is the interleaving of a pair of Golay complementary sequences.',\n",
       "  'Title: p4 compatible high level synthesis of low latency 100 gb s streaming packet parsers in fpgas\\nAbstract: Packet parsing is a key step in SDN-aware devices. Packet parsers in SDN networks need to be both reconfigurable and fast, to support the evolving network protocols and the increasing multi-gigabit data rates. The combination of packet processing languages with FPGAs seems to be the perfect match for these requirements. In this work, we develop an open-source FPGA-based configurable architecture for arbitrary packet parsing to be used in SDN networks. We generate low latency and high-speed streaming packet parsers directly from a packet processing program. Our architecture is pipelined and entirely modeled using templated \\\\textttC++ classes. The pipeline layout is derived from a parser graph that corresponds to a P4 code after a series of graph transformation rounds. The RTL code is generated from the  \\\\textttC++  description using Xilinx Vivado HLS and synthesized with Xilinx Vivado. Our architecture achieves a  \\\\SI100 \\\\giga\\\\bit/\\\\second data rate in a Xilinx Virtex-7 FPGA while reducing the latency by 45% and the LUT usage by 40% compared to the state-of-the-art.',\n",
       "  'Title: reflection aware sound source localization\\nAbstract: We present a novel, reflection-aware method for 3D sound localization in indoor environments. Unlike prior approaches, which are mainly based on continuous sound signals from a stationary source, our formulation is designed to localize the position instantaneously from signals within a single frame. We consider direct sound and indirect sound signals that reach the microphones after reflecting off surfaces such as ceilings or walls. We then generate and trace direct and reflected acoustic paths using inverse acoustic ray tracing and utilize these paths with Monte Carlo localization to estimate a 3D sound source position. We have implemented our method on a robot with a cube-shaped microphone array and tested it against different settings with continuous and intermittent sound signals with a stationary or a mobile source. Across different settings, our approach can localize the sound with an average distance error of 0.8m tested in a room of 7m by 7m area with 3m height, including a mobile and non-line-of-sight sound source. We also reveal that the modeling of indirect rays increases the localization accuracy by 40% compared to only using direct acoustic rays.',\n",
       "  'Title: unsupervised learning of object frames by dense equivariant image labelling\\nAbstract: One of the key challenges of visual perception is to extract abstract models of 3D objects and object categories from visual measurements, which are affected by complex nuisance factors such as viewpoint, occlusion, motion, and deformations. Starting from the recent idea of viewpoint factorization, we propose a new approach that, given a large number of images of an object and no other supervision, can extract a dense object-centric coordinate frame. This coordinate frame is invariant to deformations of the images and comes with a dense equivariant labelling neural network that can map image pixels to their corresponding object coordinates. We demonstrate the applicability of this method to simple articulated objects and deformable objects such as human faces, learning embeddings from random synthetic transformations or optical flow correspondences, all without any manual supervision.',\n",
       "  'Title: chaining identity mapping modules for image denoising\\nAbstract: We propose to learn a fully-convolutional network model that consists of a Chain of Identity Mapping Modules (CIMM) for image denoising. The CIMM structure possesses two distinctive features that are important for the noise removal task. Firstly, each residual unit employs identity mappings as the skip connections and receives pre-activated input in order to preserve the gradient magnitude propagated in both the forward and backward directions. Secondly, by utilizing dilated kernels for the convolution layers in the residual branch, in other words within an identity mapping module, each neuron in the last convolution layer can observe the full receptive field of the first layer. After being trained on the BSD400 dataset, the proposed network produces remarkably higher numerical accuracy and better visual image quality than the state-of-the-art when being evaluated on conventional benchmark images and the BSD68 dataset.',\n",
       "  \"Title: deep learning for reliable mobile edge analytics in intelligent transportation systems\\nAbstract: Intelligent transportation systems (ITSs) will be a major component of tomorrow's smart cities. However, realizing the true potential of ITSs requires ultra-low latency and reliable data analytics solutions that can combine, in real-time, a heterogeneous mix of data stemming from the ITS network and its environment. Such data analytics capabilities cannot be provided by conventional cloud-centric data processing techniques whose communication and computing latency can be high. Instead, edge-centric solutions that are tailored to the unique ITS environment must be developed. In this paper, an edge analytics architecture for ITSs is introduced in which data is processed at the vehicle or roadside smart sensor level in order to overcome the ITS latency and reliability challenges. With a higher capability of passengers' mobile devices and intra-vehicle processors, such a distributed edge computing architecture can leverage deep learning techniques for reliable mobile sensing in ITSs. In this context, the ITS mobile edge analytics challenges pertaining to heterogeneous data, autonomous control, vehicular platoon control, and cyber-physical security are investigated. Then, different deep learning solutions for such challenges are proposed. The proposed deep learning solutions will enable ITS edge analytics by endowing the ITS devices with powerful computer vision and signal processing functions. Preliminary results show that the proposed edge analytics architecture, coupled with the power of deep learning algorithms, can provide a reliable, secure, and truly smart transportation environment.\",\n",
       "  'Title: timely throughput optimal scheduling with prediction\\nAbstract: Motivated by the increasing importance of providing delay-guaranteed services in general computing and communication systems, and the recent wide adoption of learning and prediction in network control, in this work, we consider a general stochastic single-server multi-user system and investigate the fundamental benefit of predictive scheduling in improving timely-throughput, being the rate of packets that are delivered to destinations before their deadlines. By adopting an error rate-based prediction model, we first derive a Markov decision process (MDP) solution to optimize the timely-throughput objective subject to an average resource consumption constraint. Based on a packet-level decomposition of the MDP, we explicitly characterize the optimal scheduling policy and rigorously quantify the timely-throughput improvement due to predictive-service, which scales as $\\\\Theta(p\\\\left[C_{1}\\\\frac{(a-a_{\\\\max}q)}{p-q}\\\\rho^{\\\\tau}+C_{2}(1-\\\\frac{1}{p})\\\\right](1-\\\\rho^{D}))$, where $a, a_{\\\\max}, \\\\rho\\\\in(0, 1), C_1>0, C_2\\\\ge0$ are constants, $p$ is the true-positive rate in prediction, $q$ is the false-negative rate, $\\\\tau$ is the packet deadline and $D$ is the prediction window size. We also conduct extensive simulations to validate our theoretical findings. Our results provide novel insights into how prediction and system parameters impact performance and provide useful guidelines for designing predictive low-latency control algorithms.',\n",
       "  'Title: zero shot visual recognition using semantics preserving adversarial embedding networks\\nAbstract: We propose a novel framework called Semantics-Preserving Adversarial Embedding Network (SP-AEN) for zero-shot visual recognition (ZSL), where test images and their classes are both unseen during training. SP-AEN aims to tackle the inherent problem --- semantic loss --- in the prevailing family of embedding-based ZSL, where some semantics would be discarded during training if they are non-discriminative for training classes, but could become critical for recognizing test classes. Specifically, SP-AEN prevents the semantic loss by introducing an independent visual-to-semantic space embedder which disentangles the semantic space into two subspaces for the two arguably conflicting objectives: classification and reconstruction. Through adversarial learning of the two subspaces, SP-AEN can transfer the semantics from the reconstructive subspace to the discriminative one, accomplishing the improved zero-shot recognition of unseen classes. Comparing with prior works, SP-AEN can not only improve classification but also generate photo-realistic images, demonstrating the effectiveness of semantic preservation. On four popular benchmarks: CUB, AWA, SUN and aPY, SP-AEN considerably outperforms other state-of-the-art methods by an absolute performance difference of 12.2\\\\%, 9.3\\\\%, 4.0\\\\%, and 3.6\\\\% in terms of harmonic mean values',\n",
       "  'Title: bridge the gap between group sparse coding and rank minimization via adaptive dictionary learning\\nAbstract: Both sparse coding and rank minimization have led to great successes in various image processing tasks. Though the underlying principles of these two approaches are similar, no theory is available to demonstrate the correspondence. In this paper, starting by designing an adaptive dictionary for each group of image patches, we analyze the sparsity of image patches in each group using the rank minimization approach. Based on this, we prove that the group-based sparse coding is equivalent to the rank minimization problem under our proposed adaptive dictionary. Therefore, the sparsity of the coefficients for each group can be measured by estimating the singular values of this group. Inspired by our theoretical analysis, four nuclear norm like minimization methods including the standard nuclear norm minimization (NNM), weighted nuclear norm minimization (WNNM), Schatten $p$-norm minimization (SNM), and weighted Schatten $p$-norm minimization (WSNM), are employed to analyze the sparsity of the coefficients and WSNM is found to be the closest solution to the singular values of each group. Based on this, WSNM is then translated to a non-convex weighted $\\\\ell_p$-norm minimization problem in group-based sparse coding, and in order to solve this problem, a new algorithm based on the alternating direction method of multipliers (ADMM) framework is developed. Experimental results on two low-level vision tasks: image inpainting and image compressive sensing recovery, demonstrate that the proposed scheme is feasible and outperforms state-of-the-art methods.',\n",
       "  'Title: a novel embedding model for knowledge base completion based on convolutional neural network\\nAbstract: We introduce a novel embedding method for knowledge base completion task. Our approach advances state-of-the-art (SOTA) by employing a convolutional neural network (CNN) for the task which can capture global relationships and transitional characteristics. We represent each triple (head entity, relation, tail entity) as a 3-column matrix which is the input for the convolution layer. Different filters having a same shape of 1x3 are operated over the input matrix to produce different feature maps which are then concatenated into a single feature vector. This vector is used to return a score for the triple via a dot product. The returned score is used to predict whether the triple is valid or not. Experiments show that ConvKB achieves better link prediction results than previous SOTA models on two current benchmark datasets WN18RR and FB15k-237.',\n",
       "  'Title: a database linking piano and orchestral midi scores with application to automatic projective orchestration\\nAbstract: This article introduces the Projective Orchestral Database (POD), a collection of MIDI scores composed of pairs linking piano scores to their corresponding orchestrations. To the best of our knowledge, this is the first database of its kind, which performs piano or orchestral prediction, but more importantly which tries to learn the correlations between piano and orchestral scores. Hence, we also introduce the projective orchestration task, which consists in learning how to perform the automatic orchestration of a piano score. We show how this task can be addressed using learning methods and also provide methodological guidelines in order to properly use this database.',\n",
       "  'Title: millimeter wave interference avoidance via building aware associations\\nAbstract: Signal occlusion by building blockages is a double-edged sword for the performance of millimeter-wave (mmW) communication networks. Buildings may dominantly attenuate the useful signals, especially when mmW base stations (BSs) are sparsely deployed compared to the building density. In the opposite BS deployment, buildings can block the undesired interference. To enjoy only the benefit, we propose a building-aware association scheme that adjusts the directional BS association bias of the user equipments (UEs), based on a given building density and the concentration of UE locations around the buildings. The association of each BS can thereby be biased: (i) toward the UEs located against buildings for avoiding interference to other UEs; or (ii) toward the UEs providing their maximum reference signal received powers (RSRPs). The proposed association scheme is optimized to maximize the downlink average data rate derived by stochastic geometry. Its effectiveness is validated by simulation using real building statistics.',\n",
       "  'Title: a look at the time delays in cvss vulnerability scoring\\nAbstract: Abstract   This empirical paper examines the time delays that occur between the publication of Common Vulnerabilities and Exposures (CVEs) in the National Vulnerability Database (NVD) and the Common Vulnerability Scoring System (CVSS) information attached to published CVEs. According to the empirical results based on regularized regression analysis of over eighty thousand archived vulnerabilities, (i) the CVSS content does not statistically influence the time delays, which, however, (ii) are strongly affected by a decreasing annual trend. In addition to these results, the paper contributes to the empirical research tradition of software vulnerabilities by a couple of insights on misuses of statistical methodology.',\n",
       "  'Title: text text extractor tool for handwritten document transcription and annotation\\nAbstract: This paper presents a framework for semi-automatic transcription of large-scale historical handwritten documents and proposes a simple user-friendly text extractor tool, TexT for transcription. The proposed approach provides a quick and easy transcription of text using computer assisted interactive technique. The algorithm finds multiple occurrences of the marked text on-the-fly using a word spotting system. TexT is also capable of performing on-the-fly annotation of handwritten text with automatic generation of ground truth labels, and dynamic adjustment and correction of user generated bounding box annotations with the word being perfectly encapsulated. The user can view the document and the found words in the original form or with background noise removed for easier visualization of transcription results. The effectiveness of TexT is demonstrated on an archival manuscript collection from well-known publicly available dataset.',\n",
       "  \"Title: automatic analysis of eegs using big data and hybrid deep learning architectures\\nAbstract: Objective: A clinical decision support tool that automatically interprets EEGs can reduce time to diagnosis and enhance real-time applications such as ICU monitoring. Clinicians have indicated that a sensitivity of 95% with a specificity below 5% was the minimum requirement for clinical acceptance. We propose a highperformance classification system based on principles of big data and machine learning. Methods: A hybrid machine learning system that uses hidden Markov models (HMM) for sequential decoding and deep learning networks for postprocessing is proposed. These algorithms were trained and evaluated using the TUH EEG Corpus, which is the world's largest publicly available database of clinical EEG data. Results: Our approach delivers a sensitivity above 90% while maintaining a specificity below 5%. This system detects three events of clinical interest: (1) spike and/or sharp waves, (2) periodic lateralized epileptiform discharges, (3) generalized periodic epileptiform discharges. It also detects three events used to model background noise: (1) artifacts, (2) eye movement (3) background. Conclusions: A hybrid HMM/deep learning system can deliver a low false alarm rate on EEG event detection, making automated analysis a viable option for clinicians. Significance: The TUH EEG Corpus enables application of highly data consumptive machine learning algorithms to EEG analysis. Performance is approaching clinical acceptance for real-time applications.\",\n",
       "  'Title: pkc pc a variant of the mceliece public key cryptosystem based on polar codes\\nAbstract: Polar codes are novel and efficient error correcting codes with low encoding and decoding complexities. These codes have a channel dependent generator matrix which is determined by the code dimension, code length and transmission channel parameters. This paper studies a variant of the McEliece public key cryptosystem based on polar codes, called \"PKC-PC\". Due to the fact that the structure of polar codes\\' generator matrix depends on the parameters of channel, we used an efficient approach to conceal their generator matrix. Then, by the help of the characteristics of polar codes and also introducing an efficient approach, we reduced the public and private key sizes of the PKC-PC and increased its information rate compared to the McEliece cryptosystem. It was shown that polar codes are able to yield an increased security level against conventional attacks and possible vulnerabilities on the code-based public key cryptosystems. Moreover, it is indicated that the security of the PKC-PC is reduced to solve NP-complete problems. Compared to other post-quantum public key schemes, we believe that the PKC-PC is a promising candidate for NIST post-quantum crypto standardization.',\n",
       "  'Title: deep neural networks as 0 1 mixed integer linear programs a feasibility study\\nAbstract: Deep Neural Networks (DNNs) are very popular these days, and are the subject of a very intense investigation. A DNN is made by layers of internal units (or neurons), each of which computes an affine combination of the output of the units in the previous layer, applies a nonlinear operator, and outputs the corresponding value (also known as activation). A commonly-used nonlinear operator is the so-called rectified linear unit (ReLU), whose output is just the maximum between its input value and zero. In this (and other similar cases like max pooling, where the max operation involves more than one input value), one can model the DNN as a 0-1 Mixed Integer Linear Program (0-1 MILP) where the continuous variables correspond to the output values of each unit, and a binary variable is associated with each ReLU to model its yes/no nature. In this paper we discuss the peculiarity of this kind of 0-1 MILP models, and describe an effective bound-tightening technique intended to ease its solution. We also present possible applications of the 0-1 MILP model arising in feature visualization and in the construction of adversarial examples. Preliminary computational results are reported, aimed at investigating (on small DNNs) the computational performance of a state-of-the-art MILP solver when applied to a known test case, namely, hand-written digit recognition.',\n",
       "  'Title: leveraging text and knowledge bases for triple scoring an ensemble approach the bokchoy triple scorer at wsdm cup 2017\\nAbstract: We present our winning solution for the WSDM Cup 2017 triple scoring task. We devise an ensemble of four base scorers, so as to leverage the power of both text and knowledge bases for that task. Then we further refine the outputs of the ensemble by trigger word detection, achieving even better predictive accuracy. The code is available at this https URL',\n",
       "  'Title: social media analysis based on semanticity of streaming and batch data\\nAbstract: Languages shared by people differ in different regions based on their accents, pronunciation and word usages. In this era sharing of language takes place mainly through social media and blogs. Every second swing of such a micro posts exist which induces the need of processing those micro posts, in-order to extract knowledge out of it. Knowledge extraction differs with respect to the application in which the research on cognitive science fed the necessities for the same. This work further moves forward such a research by extracting semantic information of streaming and batch data in applications like Named Entity Recognition and Author Profiling. In the case of Named Entity Recognition context of a single micro post has been utilized and context that lies in the pool of micro posts were utilized to identify the sociolect aspects of the author of those micro posts. In this work Conditional Random Field has been utilized to do the entity recognition and a novel approach has been proposed to find the sociolect aspects of the author (Gender, Age group).',\n",
       "  'Title: network representation learning a survey\\nAbstract: With the widespread use of information technologies, information networks have increasingly become popular to capture complex relationships across various disciplines, such as social networks, citation networks, telecommunication networks, and biological networks. Analyzing these networks sheds light on different aspects of social life such as the structure of society, information diffusion, and different patterns of communication. However, the large scale of information networks often makes network analytic tasks computationally expensive and intractable. Recently, network representation learning has been proposed as a new learning paradigm that embeds network vertices into a low-dimensional vector space, by preserving network topology structure, vertex content, and other side information. This facilitates the original network to be easily handled in the new vector space for further analysis. In this survey, we perform a thorough review of the current literature on network representation learning in the field of data mining and machine learning. We propose a new categorization to analyze and summarize state-of-the-art network representation learning techniques according to the methodology they employ and the network information they preserve. Finally, to facilitate research on this topic, we summarize benchmark datasets and evaluation methodologies, and discuss open issues and future research directions in this field.',\n",
       "  'Title: common tangents of two disjoint polygons in linear time and constant workspace\\nAbstract: We provide a remarkably simple algorithm to compute all (at most four) common tangents of two disjoint simple polygons. Given each polygon as a read-only array of its corners in cyclic order, the algorithm runs in linear time and constant workspace and is the first to achieve the two complexity bounds simultaneously. The set of common tangents provides basic information about the convex hulls of the polygons—whether they are nested, overlapping, or disjoint—and our algorithm thus also decides this relationship.',\n",
       "  \"Title: stabilizing unstable periodic orbits with delayed feedback control in act and wait fashion\\nAbstract: A delayed feedback control framework for stabilizing unstable periodic orbits of linear periodic time-varying systems is proposed. In this framework, act-and-wait approach is utilized for switching a delayed feedback controller on and off alternately at every integer multiples of the period of the system. By analyzing the monodromy matrix of the closed-loop system, we obtain conditions under which the closed-loop system's state converges towards a periodic solution under our proposed control law. We discuss the application of our results in stabilization of unstable periodic orbits of nonlinear systems and present numerical examples to illustrate the efficacy of our approach.\",\n",
       "  'Title: statistical blockage modeling and robustness of beamforming in millimeter wave systems\\nAbstract: There has been a growing interest in the commercialization of millimeter-wave (mmW) technology as a part of the fifth-generation new radio wireless standardization efforts. In this direction, many sets of independent measurements show that the biggest determinants of viability of mmW systems are penetration and blockage of mmW signals through different materials in the scattering environment. With this background, the focus of this paper is on understanding the impact of blockage of mmW signals and reduced spatial coverage due to penetration through the human hand, body, vehicles, and so on. Leveraging measurements with a 28-GHz mmW experimental prototype and electromagnetic simulation studies, we first propose statistical models to capture the impact of the hand, human body, and vehicles. We then study the time scales at which mmW signals are disrupted by blockage (hand and human body). Our results show that these events can be attributed to physical movements, and the time scales corresponding to blockage are, hence, on the order of a few 100 ms or more. Network densification, subarray switching in a user equipment designed with multiple subarrays, fall back mechanisms, etc., can address blockage before it leads to a deleterious impact on the mmW link margin.',\n",
       "  \"Title: frame recurrent video super resolution\\nAbstract: Recent advances in video super-resolution have shown that convolutional neural networks combined with motion compensation are able to merge information from multiple low-resolution (LR) frames to generate high-quality images. Current state-of-the-art methods process a batch of LR frames to generate a single high-resolution (HR) frame and run this scheme in a sliding window fashion over the entire video, effectively treating the problem as a large number of separate multi-frame super-resolution tasks. This approach has two main weaknesses: 1) Each input frame is processed and warped multiple times, increasing the computational cost, and 2) each output frame is estimated independently conditioned on the input frames, limiting the system's ability to produce temporally consistent results. #R##N#In this work, we propose an end-to-end trainable frame-recurrent video super-resolution framework that uses the previously inferred HR estimate to super-resolve the subsequent frame. This naturally encourages temporally consistent results and reduces the computational cost by warping only one image in each step. Furthermore, due to its recurrent nature, the proposed method has the ability to assimilate a large number of previous frames without increased computational demands. Extensive evaluations and comparisons with previous methods validate the strengths of our approach and demonstrate that the proposed framework is able to significantly outperform the current state of the art.\",\n",
       "  'Title: edgeflow open source multi layer data flow processing in edge computing for 5g and beyond\\nAbstract: Edge computing has evolved to be a promising avenue to enhance the system computing capability by offloading processing tasks from the cloud to edge devices. In this paper, we propose a multi- layer edge computing framework called EdgeFlow. In this framework, different nodes ranging from edge devices to cloud data centers are categorized into corresponding layers and cooperate together for data processing. With the help of EdgeFlow, one can balance the trade-off between computing and communication capability so that the tasks are assigned to each layer optimally. At the same time, resources are carefully allocated throughout the whole network to mitigate performance fluctuation. The proposed open-source data flow processing framework is implemented on a platform that can emulate various computing nodes in multiple layers and corresponding network connections. Evaluated on a mobile sensing scenario, EdgeFlow can significantly reduce task finish time and is more tolerant to run-time variation, compared to traditional cloud computing and the pure edge computing approach. Potential applications of EdgeFlow, including network function visualization, Internet of Things, and vehicular networks, are also discussed in the end of this work.',\n",
       "  'Title: robust integral action of port hamiltonian systems\\nAbstract: Interconnection and damping assignment, passivity-based control (IDA-PBC) has proven to be a successful control technique for the stabilisation of many nonlinear systems. In this paper, we propose a method to robustify a system which has been stabilised using IDA-PBC with respect to constant, matched disturbances via the addition of integral action. The proposed controller extends previous work on the topic by being robust against the damping of the system, a quantity which may not be known in many applications.',\n",
       "  'Title: relaxed conditions for secrecy in a role based specification\\nAbstract: In this paper, we look at the property of secrecy through the growth of the protocol. Intuitively, an increasing protocol preserves the secret. For that, we need functions to estimate the security of messages. Here, we give relaxed conditions on the functions and on the protocol and we prove that an increasing protocol is correct when analyzed with functions that meet these conditions.',\n",
       "  'Title: a systematic review of productivity factors in software development\\nAbstract: Analysing and improving productivity has been one of the main goals of software engineering research since its beginnings. A plethora of studies has been conducted on various factors that resulted in several models for analysis and prediction of productivity. However, productivity is still an issue in current software development and not all factors and their relationships are known. This paper reviews the large body of available literature in order to distill a list of the main factors influencing productivity investigated so far. The measure for importance here is the number of articles a factor is mentioned in. Special consideration is given to soft or human-related factors in software engineering that are often not analysed with equal detail as more technical factors. The resulting list can be used to guide further analysis and as basis for building productivity models.',\n",
       "  'Title: deep learning for malicious flow detection\\nAbstract: Cyber security has grown up to be a hot issue in recent years. How to identify potential malware becomes a challenging task. To tackle this challenge, we adopt deep learning approaches and perform flow detection on real data. However, real data often encounters an issue of imbalanced data distribution which will lead to a gradient dilution issue. When training a neural network, this problem will not only result in a bias toward the majority class but show the inability to learn from the minority classes. In this paper, we propose an end-to-end trainable Tree-Shaped Deep Neural Network (TSDNN) which classifies the data in a layer-wise manner. To better learn from the minority classes, we propose a Quantity Dependent Backpropagation (QDBP) algorithm which incorporates the knowledge of the disparity between classes. We evaluate our method on an imbalanced data set. Experimental result demonstrates that our approach outperforms the state-of-the-art methods and justifies that the proposed method is able to overcome the difficulty of imbalanced learning. We also conduct a partial flow experiment which shows the feasibility of real-time detection and a zero-shot learning experiment which justifies the generalization capability of deep learning in cyber security.',\n",
       "  'Title: learning multiple levels of representations with kernel machines\\nAbstract: We propose a connectionist-inspired kernel machine model with three key advantages over traditional kernel machines. First, it is capable of learning distributed and hierarchical representations. Second, its performance is highly robust to the choice of kernel function. Third, the solution space is not limited to the span of images of training data in reproducing kernel Hilbert space (RKHS). Together with the architecture, we propose a greedy learning algorithm that allows the proposed multilayer network to be trained layer-wise without backpropagation by optimizing the geometric properties of images in RKHS. With a single fixed generic kernel for each layer and two layers in total, our model compares favorably with state-of-the-art multiple kernel learning algorithms using significantly more kernels and popular deep architectures on widely used classification benchmarks.',\n",
       "  \"Title: a spatial mapping algorithm with applications in deep learning based structure classification\\nAbstract: Convolutional Neural Network (CNN)-based machine learning systems have made breakthroughs in feature extraction and image recognition tasks in two dimensions (2D). Although there is significant ongoing work to apply CNN technology to domains involving complex 3D data, the success of such efforts has been constrained, in part, by limitations in data representation techniques. Most current approaches rely upon low-resolution 3D models, strategic limitation of scope in the 3D space, or the application of lossy projection techniques to allow for the use of 2D CNNs. To address this issue, we present a mapping algorithm that converts 3D structures to 2D and 1D data grids by mapping a traversal of a 3D space-filling curve to the traversal of corresponding 2D and 1D curves. We explore the performance of 2D and 1D CNNs trained on data encoded with our method versus comparable volumetric CNNs operating upon raw 3D data from a popular benchmarking dataset. Our experiments demonstrate that both 2D and 1D representations of 3D data generated via our method preserve a significant proportion of the 3D data's features in forms learnable by CNNs. Furthermore, we demonstrate that our method of encoding 3D data into lower-dimensional representations allows for decreased CNN training time cost, increased original 3D model rendering resolutions, and supports increased numbers of data channels when compared to purely volumetric approaches. This demonstration is accomplished in the context of a structural biology classification task wherein we train 3D, 2D, and 1D CNNs on examples of two homologous branches within the Ras protein family. The essential contribution of this paper is the introduction of a dimensionality-reduction method that may ease the application of powerful deep learning tools to domains characterized by complex structural data.\",\n",
       "  'Title: measuring the intrinsic dimension of objective landscapes\\nAbstract: Many recently trained neural networks employ large numbers of parameters to achieve good performance. One may intuitively use the number of parameters required as a rough gauge of the difficulty of a problem. But how accurate are such notions? How many parameters are really needed? In this paper we attempt to answer this question by training networks not in their native parameter space, but instead in a smaller, randomly oriented subspace. We slowly increase the dimension of this subspace, note at which dimension solutions first appear, and define this to be the intrinsic dimension of the objective landscape. The approach is simple to implement, computationally tractable, and produces several suggestive conclusions. Many problems have smaller intrinsic dimensions than one might suspect, and the intrinsic dimension for a given dataset varies little across a family of models with vastly different sizes. This latter result has the profound implication that once a parameter space is large enough to solve a problem, extra parameters serve directly to increase the dimensionality of the solution manifold. Intrinsic dimension allows some quantitative comparison of problem difficulty across supervised, reinforcement, and other types of learning where we conclude, for example, that solving the inverted pendulum problem is 100 times easier than classifying digits from MNIST, and playing Atari Pong from pixels is about as hard as classifying CIFAR-10. In addition to providing new cartography of the objective landscapes wandered by parameterized models, the method is a simple technique for constructively obtaining an upper bound on the minimum description length of a solution. A byproduct of this construction is a simple approach for compressing networks, in some cases by more than 100 times.',\n",
       "  'Title: improving active learning in systematic reviews\\nAbstract: Systematic reviews are essential to summarizing the results of different clinical and social science studies. The first step in a systematic review task is to identify all the studies relevant to the review. The task of identifying relevant studies for a given systematic review is usually performed manually, and as a result, involves substantial amounts of expensive human resource. Lately, there have been some attempts to reduce this manual effort using active learning. In this work, we build upon some such existing techniques, and validate by experimenting on a larger and comprehensive dataset than has been attempted until now. Our experiments provide insights on the use of different feature extraction models for different disciplines. More importantly, we identify that a naive active learning based screening process is biased in favour of selecting similar documents. We aimed to improve the performance of the screening process using a novel active learning algorithm with success. Additionally, we propose a mechanism to choose the best feature extraction method for a given review.',\n",
       "  'Title: maximum total correntropy diffusion adaptation over networks with noisy links\\nAbstract: Distributed estimation over networks draws much attraction in recent years. In many situations, due to imperfect information communication among nodes, the performance of traditional diffusion adaptive algorithms such as the diffusion LMS (DLMS) may degrade. To deal with this problem, several modified DLMS algorithms have been proposed. However, these DLMS based algorithms still suffer from biased estimation and are not robust to impulsive link noise. In this paper, we focus on improving the performance of diffusion adaptation with noisy links from two aspects: accuracy and robustness. A new algorithm called diffusion maximum total correntropy (DMTC) is proposed. The new algorithm is theoretically unbiased in Gaussian noise, and can efficiently handle the link noises in the presence of large outliers. The adaptive combination rule is applied to further improve the performance. The stability analysis of the proposed algorithm is given. Simulation results show that the DMTC algorithm can achieve good performance in both Gaussian and non-Gaussian noise environments.',\n",
       "  'Title: holoface augmenting human to human interactions on hololens\\nAbstract: We present HoloFace, an open-source framework for face alignment, head pose estimation and facial attribute retrieval for Microsoft HoloLens. HoloFace implements two state-of-the-art face alignment methods which can be used interchangeably: one running locally and one running on a remote backend. Head pose estimation is accomplished by fitting a deformable 3D model to the landmarks localized using face alignment. The head pose provides both the rotation of the head and a position in the world space. The parameters of the fitted 3D face model provide estimates of facial attributes such as mouth opening or smile. Together the above information can be used to augment the faces of people seen by the HoloLens user, and thus their interaction. Potential usage scenarios include facial recognition, emotion recognition, eye gaze tracking and many others. We demonstrate the capabilities of our framework by augmenting the faces of people seen through the HoloLens with various objects and animations.',\n",
       "  'Title: intelliav building an effective on device android malware detector\\nAbstract: The importance of employing machine learning for malware detection has become explicit to the security community. Several anti-malware vendors have claimed and advertised the application of machine learning in their products in which the inference phase is performed on servers and high-performance machines, but the feasibility of such approaches on mobile devices with limited computational resources has not yet been assessed by the research community, vendors still being skeptical. In this paper, we aim to show the practicality of devising a learning-based anti-malware on Android mobile devices, first. Furthermore, we aim to demonstrate the significance of such a tool to cease new and evasive malware that can not easily be caught by signature-based or offline learning-based security tools. To this end, we first propose the extraction of a set of lightweight yet powerful features from Android applications. Then, we embed these features in a vector space to build an effective as well as efficient model. Hence, the model can perform the inference on the device for detecting potentially harmful applications. We show that without resorting to any signatures and relying only on a training phase involving a reasonable set of samples, the proposed system, named IntelliAV, provides more satisfying performances than the popular major anti-malware products. Moreover, we evaluate the robustness of IntelliAV against common obfuscation techniques where most of the anti-malware solutions get affected.',\n",
       "  'Title: what is gab a bastion of free speech or an alt right echo chamber\\nAbstract: Over the past few years, a number of new \"fringe\" communities, like 4chan or certain subreddits, have gained traction on the Web at a rapid pace. However, more often than not, little is known about how they evolve or what kind of activities they attract, despite recent research has shown that they influence how false information reaches mainstream communities. This motivates the need to monitor these communities and analyze their impact on the Web\\'s information ecosystem. In August 2016, a new social network called Gab was created as an alternative to Twitter. It positions itself as putting \"people and free speech first\", welcoming users banned or suspended from other social networks. In this paper, we provide, to the best of our knowledge, the first characterization of Gab. We collect and analyze 22M posts produced by 336K users between August 2016 and January 2018, finding that Gab is predominantly used for the dissemination and discussion of news and world events, and that it attracts alt-right users, conspiracy theorists, and other trolls. We also measure the prevalence of hate speech on the platform, finding it to be much higher than Twitter, but lower than 4chan\\'s Politically Incorrect board.',\n",
       "  'Title: ppfnet global context aware local features for robust 3d point matching\\nAbstract: We present PPFNet - Point Pair Feature NETwork for deeply learning a globally informed 3D local feature descriptor to find correspondences in unorganized point clouds. PPFNet learns local descriptors on pure geometry and is highly aware of the global context, an important cue in deep learning. Our 3D representation is computed as a collection of point-pair-features combined with the points and normals within a local vicinity. Our permutation invariant network design is inspired by PointNet and sets PPFNet to be ordering-free. As opposed to voxelization, our method is able to consume raw point clouds to exploit the full sparsity. PPFNet uses a novel \\\\textit{N-tuple} loss and architecture injecting the global information naturally into the local descriptor. It shows that context awareness also boosts the local feature representation. Qualitative and quantitative evaluations of our network suggest increased recall, improved robustness and invariance as well as a vital step in the 3D descriptor extraction performance.',\n",
       "  'Title: molecular structure extraction from documents using deep learning\\nAbstract: Chemical structure extraction from documents remains a hard problem due to both false positive identification of structures during segmentation and errors in the predicted structures. Current approaches rely on handcrafted rules and subroutines that perform reasonably well generally, but still routinely encounter situations where recognition rates are not yet satisfactory and systematic improvement is challenging. Complications impacting performance of current approaches include the diversity in visual styles used by various software to render structures, the frequent use of ad hoc annotations, and other challenges related to image quality, including resolution and noise. We here present end-to-end deep learning solutions for both segmenting molecular structures from documents and for predicting chemical structures from these segmented images. This deep learning-based approach does not require any handcrafted features, is learned directly from data, and is robust against variations in image quality and style. Using the deep-learning approach described herein we show that it is possible to perform well on both segmentation and prediction of low resolution images containing moderately sized molecules found in journal articles and patents.',\n",
       "  'Title: oei operation execution integrity for embedded devices\\nAbstract: We formulate a new security property, called \"Operation Execution Integrity\" or OEI, tailored for embedded devices. Inspired by the operation-oriented design of embedded programs and considering the limited hardware capabilities of embedded devices, OEI attestation enables selective and practical verification of both control-flow integrity and critical-variable integrity for an operation being executed. This attestation allows remote verifiers to detect control-flow hijacks as well as data-only attacks, including data-oriented programming, on an embedded device\\'s capability needed for securing IoT but unachievable using existing methods. We design and build a system, called OAT, to realize and evaluate the idea of OEI attestation on ARM-based bare-metal devices. OAT features a highly efficient measurement collection mechanism, a control-flow measurement scheme designed for determinate verifiability, and a method for lightweight variable-integrity checking. When tested against real-world embedded programs on a development board, OAT incurred only a mild runtime overhead (2.7%).',\n",
       "  'Title: cgans with projection discriminator\\nAbstract: We propose a novel, projection based way to incorporate the conditional information into the discriminator of GANs that respects the role of the conditional information in the underlining probabilistic model. This approach is in contrast with most frameworks of conditional GANs used in application today, which use the conditional information by concatenating the (embedded) conditional vector to the feature vectors. With this modification, we were able to significantly improve the quality of the class conditional image generation on ILSVRC2012 (ImageNet) 1000-class image dataset from the current state-of-the-art result, and we achieved this with a single pair of a discriminator and a generator. We were also able to extend the application to super-resolution and succeeded in producing highly discriminative super-resolution images. This new structure also enabled high quality category transformation based on parametric functional transformation of conditional batch normalization layers in the generator.',\n",
       "  'Title: socialml machine learning for social media video creators\\nAbstract: In the recent years, social media have become one of the main places where creative content is being published and consumed by billions of users. Contrary to traditional media, social media allow the publishers to receive almost instantaneous feedback regarding their creative work at an unprecedented scale. This is a perfect use case for machine learning methods that can use these massive amounts of data to provide content creators with inspirational ideas and constructive criticism of their work. In this work, we present a comprehensive overview of machine learning-empowered tools we developed for video creators at Group Nine Media - one of the major social media companies that creates short-form videos with over three billion views per month. Our main contribution is a set of tools that allow the creators to leverage massive amounts of data to improve their creation process, evaluate their videos before the publication and improve content quality. These applications include an interactive conversational bot that allows access to material archives, a Web-based application for automatic selection of optimal video thumbnail, as well as deep learning methods for optimizing headline and predicting video popularity. Our A/B tests show that deployment of our tools leads to significant increase of average video view count by 12.9%. Our additional contribution is a set of considerations collected during the deployment of those tools that can hel',\n",
       "  'Title: augmented cyclegan learning many to many mappings from unpaired data\\nAbstract: Learning inter-domain mappings from unpaired data can improve performance in structured prediction tasks, such as image segmentation, by reducing the need for paired data. CycleGAN was recently proposed for this problem, but critically assumes the underlying inter-domain mapping is approximately deterministic and one-to-one. This assumption renders the model ineffective for tasks requiring flexible, many-to-many mappings. We propose a new model, called Augmented CycleGAN, which learns many-to-many mappings between domains. We examine Augmented CycleGAN qualitatively and quantitatively on several image datasets.',\n",
       "  'Title: ccp conflicts check protocol for bitcoin block security\\nAbstract: In this work, we present our early stage results on a Conflicts Check Protocol (CCP) that enables preventing potential attacks on bitcoin system. Based on the observation and discovery of a common symptom that many attacks may generate, CCP refines the current bitcoin systems by proposing a novel arbitration mechanism that is capable to determine the approval or abandon of certain transactions involved in confliction. This work examines the security issue of bitcoin from a new perspective, which may extend to a larger scope of attack analysis and prevention',\n",
       "  'Title: classification of breast cancer histology images using transfer learning\\nAbstract: Breast cancer is one of the leading causes of mortality in women. Early detection and treatment are imperative for improving survival rates, which have steadily increased in recent years as a result of more sophisticated computer-aided-diagnosis (CAD) systems. A critical component of breast cancer diagnosis relies on histopathology, a laborious and highly subjective process. Consequently, CAD systems are essential to reduce inter-rater variability and supplement the analyses conducted by specialists. In this paper, a transfer-learning based approach is proposed, for the task of breast histology image classification into four tissue sub-types, namely, normal, benign, \\\\textit{in situ} carcinoma and invasive carcinoma. The histology images, provided as part of the BACH 2018 grand challenge, were first normalized to correct for color variations resulting from inconsistencies during slide preparation. Subsequently, image patches were extracted and used to fine-tune Google`s Inception-V3 and ResNet50 convolutional neural networks (CNNs), both pre-trained on the ImageNet database, enabling them to learn domain-specific features, necessary to classify the histology images. The ResNet50 network (based on residual learning) achieved a test classification accuracy of 97.50% for four classes, outperforming the Inception-V3 network which achieved an accuracy of 91.25%.',\n",
       "  'Title: tool demonstration fsolidm for designing secure ethereum smart contracts\\nAbstract: Blockchain-based distributed computing platforms enable the trusted execution of computation - defined in the form of smart contracts - without trusted agents. Smart contracts are envisioned to have a variety of applications, ranging from financial to IoT asset tracking. Unfortunately, the development of smart contracts has proven to be extremely error prone. In practice, contracts are riddled with security vulnerabilities comprising a critical issue since bugs are by design non-fixable and contracts may handle financial assets of significant value. To facilitate the development of secure smart contracts, we have created the FSolidM framework, which allows developers to define contracts as finite state machines (FSMs) with rigorous and clear semantics. FSolidM provides an easy-to-use graphical editor for specifying FSMs, a code generator for creating Ethereum smart contracts, and a set of plugins that developers may add to their FSMs to enhance security and functionality.',\n",
       "  'Title: real world repetition estimation by div grad and curl\\nAbstract: We consider the problem of estimating repetition in video, such as performing push-ups, cutting a melon or playing violin. Existing work shows good results under the assumption of static and stationary periodicity. As realistic video is rarely perfectly static and stationary, the often preferred Fourier-based measurements is inapt. Instead, we adopt the wavelet transform to better handle non-static and non-stationary video dynamics. From the flow field and its differentials, we derive three fundamental motion types and three motion continuities of intrinsic periodicity in 3D. On top of this, the 2D perception of 3D periodicity considers two extreme viewpoints. What follows are 18 fundamental cases of recurrent perception in 2D. In practice, to deal with the variety of repetitive appearance, our theory implies measuring time-varying flow and its differentials (gradient, divergence and curl) over segmented foreground motion. For experiments, we introduce the new QUVA Repetition dataset, reflecting reality by including non-static and non-stationary videos. On the task of counting repetitions in video, we obtain favorable results compared to a deep learning alternative.',\n",
       "  \"Title: caching in combination networks a novel delivery by leveraging the network topology\\nAbstract: Maddah-Ali and Niesen (MAN) in 2014 surprisingly showed that it is possible to serve an arbitrarily large number of cache-equipped users with a constant number of transmissions by using coded caching in shared-link broadcast networks. This paper studies the tradeoff between the user's cache size and the file download time for combination networks, where users with caches communicate with the servers through intermediate relays. Motivated by the so-called separation approach, it is assumed that placement and multicast message generation are done according to the MAN original scheme and regardless of the network topology. The main contribution of this paper is the design of a novel two-phase delivery scheme that, accounting to the network topology, outperforms schemes available in the literature. The key idea is to create additional (compared to MAN) multicasting opportunities: in the first phase coded messages are sent with the goal of increasing the amount of `side information' at the users, which is then leveraged during the second phase. The download time with the novel scheme is shown to be proportional to 1=H (with H being the number or relays) and to be order optimal under the constraint of uncoded placement for some parameter regimes.\",\n",
       "  'Title: size based termination of higher order rewriting\\nAbstract: We provide a general and modular criterion for the termination of simply-typed $\\\\lambda$ -calculus extended with function symbols defined by user-defined rewrite rules. Following a work of Hughes, Pareto and Sabry for functions defined with a fixpoint operator and pattern-matching, several criteria use typing rules for bounding the height of arguments in function calls. In this paper, we extend this approach to rewriting-based function definitions and more general user-defined notions of size.',\n",
       "  'Title: exploiting friction in torque controlled humanoid robots\\nAbstract: A common architecture for torque controlled humanoid robots consists in two nested loops. The outer loop generates desired joint/motor torques, and the inner loop stabilises these desired values. In doing so, the inner loop usually compensates for joint friction phenomena, thus removing their inherent stabilising property that may be also beneficial for high level control objectives. This paper shows how to exploit friction for joint and task space control of humanoid robots. Experiments are carried out using the humanoid robot iCub.',\n",
       "  \"Title: the role of the task topic in web search of different task types\\nAbstract: When users are looking for information on the Web, they show different behavior for different task types, e.g., for fact finding vs. information gathering tasks. For example, related work in this area has investigated how this behavior can be measured and applied to distinguish between easy and difficult tasks. In this work, we look at the searcher's behavior in the domain of journalism for four different task types, and additionally, for two different topics in each task type. Search behavior is measured with a number of session variables and correlated to subjective measures such as task difficulty, task success and the usefulness of documents. We acknowledge prior results in this area that task difficulty is correlated to user effort and that easy and difficult tasks are distinguishable by session variables. However, in this work, we emphasize the role of the task topic - in and of itself - over parameters such as the search results and read content pages, dwell times, session variables and subjective measures such as task difficulty or task success. With this knowledge researchers should give more attention to the task topic as an important influence factor for user behavior.\",\n",
       "  'Title: byzantine stochastic gradient descent\\nAbstract: This paper studies the problem of distributed stochastic optimization in an adversarial setting where, out of the $m$ machines which allegedly compute stochastic gradients every iteration, an $\\\\alpha$-fraction are Byzantine, and can behave arbitrarily and adversarially. Our main result is a variant of stochastic gradient descent (SGD) which finds $\\\\varepsilon$-approximate minimizers of convex functions in $T = \\\\tilde{O}\\\\big( \\\\frac{1}{\\\\varepsilon^2 m} + \\\\frac{\\\\alpha^2}{\\\\varepsilon^2} \\\\big)$ iterations. In contrast, traditional mini-batch SGD needs $T = O\\\\big( \\\\frac{1}{\\\\varepsilon^2 m} \\\\big)$ iterations, but cannot tolerate Byzantine failures. Further, we provide a lower bound showing that, up to logarithmic factors, our algorithm is information-theoretically optimal both in terms of sampling complexity and time complexity.',\n",
       "  \"Title: real time deep learning method for abandoned luggage detection in video\\nAbstract: Recent terrorist attacks in major cities around the world have brought many casualties among innocent citizens. One potential threat is represented by abandoned luggage items (that could contain bombs or biological warfare) in public areas. In this paper, we describe an approach for real-time automatic detection of abandoned luggage in video captured by surveillance cameras. The approach is comprised of two stages: (i) static object detection based on background subtraction and motion estimation and (ii) abandoned luggage recognition based on a cascade of convolutional neural networks (CNN). To train our neural networks we provide two types of examples: images collected from the Internet and realistic examples generated by imposing various suitcases and bags over the scene's background. We present empirical results demonstrating that our approach yields better performance than a strong CNN baseline method.\",\n",
       "  'Title: improved scaling law for activity detection in massive mimo systems\\nAbstract: In this paper, we study the problem of activity detection (AD) in a massive MIMO setup, where the Base Station (BS) has $M \\\\gg 1$ antennas. We consider a flat fading channel model where the $M$-dim channel vector of each user remains almost constant over a coherence block (CB) containing $D_c$ signal dimensions. We study a setting in which the number of potential users $K_c$ assigned to a specific CB is much larger than the dimension of the CB $D_c$ ($K_c \\\\gg D_c$) but at each time slot only $A_c \\\\ll K_c$ of them are active. Most of the previous results, based on compressed sensing, require that $A_c\\\\le D_c$, which is a bottleneck in massive deployment scenarios such as Internet-of-Things (IoT) and Device-to-Device (D2D) communication. In this paper, we propose a novel scheme for AD and show that it overcomes this limitation when the number of antennas $M$ is sufficiently large. We also derive a scaling law on the parameters $(M, D_c, K_c, A_c)$ and also Signal-to-Noise Ratio (SNR) under which our proposed AD scheme succeeds. Our analysis indicates that with a CB of dimension $D_c$, and a sufficient number of BS antennas $M=O(A_c)$, one can identify the activity of $A_c=O(D_c^2/\\\\log (\\\\frac{K_c}{A_c}))$ active users, which is much larger than the previous bound $A_c=O(D_c)$ obtained via traditional compressed sensing techniques. In particular, in our proposed scheme one needs to pay only a negligible logarithmic penalty $O(\\\\log (\\\\frac{K_c}{A_c}))$ for increasing the number of potential users $K_c$, which makes it perfect for AD in IoT setups. We propose very low-complexity algorithms for AD and provide numerical simulations to illustrate the validity of our results.',\n",
       "  'Title: expert finding in heterogeneous bibliographic networks with locally trained embeddings\\nAbstract: Expert finding is an important task in both industry and academia. It is challenging to rank candidates with appropriate expertise for various queries. In addition, different types of objects interact with one another, which naturally forms heterogeneous information networks. We study the task of expert finding in heterogeneous bibliographical networks based on two aspects: textual content analysis and authority ranking. Regarding the textual content analysis, we propose a new method for query expansion via locally-trained embedding learning with concept hierarchy as guidance, which is particularly tailored for specific queries with narrow semantic meanings. Compared with global embedding learning, locally-trained embedding learning projects the terms into a latent semantic space constrained on relevant topics, therefore it preserves more precise and subtle information for specific queries. Considering the candidate ranking, the heterogeneous information network structure, while being largely ignored in the previous studies of expert finding, provides additional information. Specifically, different types of interactions among objects play different roles. We propose a ranking algorithm to estimate the authority of objects in the network, treating each strongly-typed edge type individually. To demonstrate the effectiveness of the proposed framework, we apply the proposed method to a large-scale bibliographical dataset with over two million entries and one million researcher candidates. The experiment results show that the proposed framework outperforms existing methods for both general and specific queries.',\n",
       "  'Title: aspiration based perturbed learning automata\\nAbstract: This paper introduces a novel payoff-based learning scheme for distributed optimization in repeatedly-played strategic-form games. Standard reinforcement-based learning exhibits several limitations with respect to their asymptotic stability. For example, in two-player coordination games, payoff-dominant (or efficient) Nash equilibria may not be stochastically stable. In this work, we present an extension of perturbed learning automata, namely aspiration-based perturbed learning automata (APLA) that overcomes these limitations. We provide a stochastic stability analysis of APLA in multi-player coordination games. We further show that payoff-dominant Nash equilibria are the only stochastically stable states.',\n",
       "  'Title: gerrymandering and compactness implementation flexibility and abuse\\nAbstract: The shape of an electoral district may suggest whether it was drawn with political motivations, or gerrymandered. For this reason, quantifying the shape of districts, in particular their compactness, is a key task in politics and civil rights. A growing body of literature suggests and analyzes compactness measures mathematically, but little consideration has been given to how these scores should be calculated in practice. Here, we consider the effects of a number of decisions that must be made in interpreting and implementing a set of popular compactness scores. We show that the choices made in quantifying compactness may themselves become political tools, with seemingly innocuous decisions leading to disparate scores. We show that when the full range of implementation flexibility is used, it can be abused to make clearly gerrymandered districts appear quantitatively reasonable. This complicates using compactness as a legislative or judicial standard to counteract unfair redistricting practices. This paper accompanies the release of packages in C++, Python, and R which correctly, efficiently, and reproducibly calculate a variety of compactness scores.',\n",
       "  'Title: distributed transactions dissecting the nightmare\\nAbstract: Many distributed storage systems are transactional and a lot of work has been devoted to optimizing their performance, especially the performance of read-only transactions that are considered the most frequent in practice. Yet, the results obtained so far are rather disappointing, and some of the design decisions seem contrived. This paper contributes to explaining this state of affairs by proving intrinsic limitations of transactional storage systems, even those that need not ensure strong consistency but only causality. #R##N#We first consider general storage systems where some transactions are read-only and some also involve write operations. We show that even read-only transactions cannot be \"fast\": their operations cannot be executed within one round-trip message exchange between a client seeking an object and the server storing it. We then consider systems (as sometimes implemented today) where all transactions are read-only, i.e., updates are performed as individual operations outside transactions. In this case, read-only transactions can indeed be \"fast\", but we prove that they need to be \"visible\". They induce inherent updates on the servers, which in turn impact their overall performance.',\n",
       "  'Title: learning spectral spatial temporal features via a recurrent convolutional neural network for change detection in multispectral imagery\\nAbstract: Change detection is one of the central problems in earth observation and was extensively investigated over recent decades. In this paper, we propose a novel recurrent convolutional neural network (ReCNN) architecture, which is trained to learn a joint spectral–spatial–temporal feature representation in a unified framework for change detection in multispectral images. To this end, we bring together a convolutional neural network and a recurrent neural network into one end-to-end network. The former is able to generate rich spectral-spatial feature representations, while the latter effectively analyzes temporal dependence in bitemporal images. In comparison with previous approaches to change detection, the proposed network architecture possesses three distinctive properties: 1) it is end-to-end trainable, in contrast to most existing methods whose components are separately trained or computed; 2) it naturally harnesses spatial information that has been proven to be beneficial to change detection task; and 3) it is capable of adaptively learning the temporal dependence between multitemporal images, unlike most of the algorithms that use fairly simple operation like image differencing or stacking. As far as we know, this is the first time that a recurrent convolutional network architecture has been proposed for multitemporal remote sensing image analysis. The proposed network is validated on real multispectral data sets. Both visual and quantitative analyses of the experimental results demonstrate competitive performance in the proposed mode.',\n",
       "  'Title: age of information in a network of preemptive servers\\nAbstract: A source submits status updates to a network for delivery to a destination monitor. Updates follow a route through a series of network nodes. Each node is a last-come-first-served queue supporting preemption in service. We characterize the average age of information at the input and output of each node in the route induced by the updates passing through. For Poisson arrivals to a line network of preemptive memoryless servers, we show that average age accumulates through successive network nodes.',\n",
       "  'Title: quantification of lung abnormalities in cystic fibrosis using deep networks\\nAbstract: Cystic fibrosis is a genetic disease which may appear in early life with structural abnormalities in lung tissues. We propose to detect these abnormalities using a texture classification approach. Our method is a cascade of two convolutional neural networks. The first network detects the presence of abnormal tissues. The second network identifies the type of the structural abnormalities: bronchiectasis, atelectasis or mucus plugging.We also propose a network computing pixel-wise heatmaps of abnormality presence learning only from the patch-wise annotations. Our database consists of CT scans of 194 subjects. We use 154 subjects to train our algorithms and the 40 remaining ones as a test set. We compare our method with random forest and a single neural network approach. The first network reaches an accuracy of 0,94 for disease detection, 0,18 higher than the random forest classifier and 0,37 higher than the single neural network. Our cascade approach yields a final class-averaged F1-score of 0,33, outperforming the baseline method and the single network by 0,10 and 0,12.',\n",
       "  'Title: intrinsically motivated reinforcement learning for human robot interaction in the real world\\nAbstract: Abstract   For a natural social human–robot interaction, it is essential for a robot to learn the human-like social skills. However, learning such skills is notoriously hard due to the limited availability of direct instructions from people to teach a robot. In this paper, we propose an intrinsically motivated reinforcement learning framework in which an agent gets the intrinsic motivation-based rewards through the action-conditional predictive model. By using the proposed method, the robot learned the social skills from the human–robot interaction experiences gathered in the real uncontrolled environments. The results indicate that the robot not only acquired human-like social skills but also took more human-like decisions, on a test dataset, than a robot which received direct rewards for the task achievement.',\n",
       "  'Title: cross lingual and multilingual speech emotion recognition on english and french\\nAbstract: Research on multilingual speech emotion recognition faces the problem that most available speech corpora differ from each other in important ways, such as annotation methods or interaction scenarios. These inconsistencies complicate building a multilingual system. We present results for cross-lingual and multilingual emotion recognition on English and French speech data with similar characteristics in terms of interaction (human-human conversations). Further, we explore the possibility of fine-tuning a pre-trained cross-lingual model with only a small number of samples from the target language, which is of great interest for low-resource languages. To gain more insights in what is learned by the deployed convolutional neural network, we perform an analysis on the attention mechanism inside the network.',\n",
       "  'Title: qmix monotonic value function factorisation for deep multi agent reinforcement learning\\nAbstract: In many real-world settings, a team of agents must coordinate their behaviour while acting in a decentralised way. At the same time, it is often possible to train the agents in a centralised fashion in a simulated or laboratory setting, where global state information is available and communication constraints are lifted. Learning joint action-values conditioned on extra state information is an attractive way to exploit centralised learning, but the best strategy for then extracting decentralised policies is unclear. Our solution is QMIX, a novel value-based method that can train decentralised policies in a centralised end-to-end fashion. QMIX employs a network that estimates joint action-values as a complex non-linear combination of per-agent values that condition only on local observations. We structurally enforce that the joint-action value is monotonic in the per-agent values, which allows tractable maximisation of the joint action-value in off-policy learning, and guarantees consistency between the centralised and decentralised policies. We evaluate QMIX on a challenging set of StarCraft II micromanagement tasks, and show that QMIX significantly outperforms existing value-based multi-agent reinforcement learning methods.',\n",
       "  'Title: communication in open source projects end of the e mail era\\nAbstract: Communication is essential in software engineering. Especially in distributed open-source teams, communication needs to be supported by channels including mailing lists, forums, issue trackers, and chat systems. Yet, we do not have a clear understanding of which communication channels stakeholders in open-source projects use. In this study, we fill the knowledge gap by investigating a statistically representative sample of 400 GitHub projects. We discover the used communication channels by regular expressions on project data. We show that (1) half of the GitHub projects use observable communication channels; (2) GitHub Issues, e-mail addresses, and the modern chat system Gitter are the most common channels; (3) mailing lists are only in place five and have a lower market share than all modern chat systems combined.',\n",
       "  'Title: stochastic variational inference with gradient linearization\\nAbstract: Variational inference has experienced a recent surge in popularity owing to stochastic approaches, which have yielded practical tools for a wide range of model classes. A key benefit is that stochastic variational inference obviates the tedious process of deriving analytical expressions for closed-form variable updates. Instead, one simply needs to derive the gradient of the log-posterior, which is often much easier. Yet for certain model classes, the log-posterior itself is difficult to optimize using standard gradient techniques. One such example are random field models, where optimization based on gradient linearization has proven popular, since it speeds up convergence significantly and can avoid poor local optima. In this paper we propose stochastic variational inference with gradient linearization (SVIGL). It is similarly convenient as standard stochastic variational inference - all that is required is a local linearization of the energy gradient. Its benefit over stochastic variational inference with conventional gradient methods is a clear improvement in convergence speed, while yielding comparable or even better variational approximations in terms of KL divergence. We demonstrate the benefits of SVIGL in three applications: Optical flow estimation, Poisson-Gaussian denoising, and 3D surface reconstruction.',\n",
       "  'Title: pir array codes with optimal virtual server rate\\nAbstract: There has been much recent interest in Private information Retrieval (PIR) in models where a database is stored across several servers using coding techniques from distributed storage, rather than being simply replicated. In particular, a recent breakthrough result of Fazelli, Vardy and Yaakobi introduces the notion of a PIR code and a PIR array code, and uses this notion to produce efficient PIR protocols. #R##N#In this paper we are interested in designing PIR array codes. We consider the case when we have $m$ servers, with each server storing a fraction $(1/s)$ of the bits of the database; here $s$ is a fixed rational number with $s > 1$. A PIR array code with the $k$-PIR property enables a $k$-server PIR protocol (with $k\\\\leq m$) to be emulated on $m$ servers, with the overall storage requirements of the protocol being reduced. The communication complexity of a PIR protocol reduces as $k$ grows, so the virtual server rate, defined to be $k/m$, is an important parameter. We study the maximum virtual server rate of a PIR array code with the $k$-PIR property. We present upper bounds on the achievable virtual server rate, some constructions, and ideas how to obtain PIR array codes with the highest possible virtual server rate. In particular, we present constructions that asymptotically meet our upper bounds, and the exact largest virtual server rate is obtained when $1 < s \\\\leq 2$. #R##N#A $k$-PIR code (and similarly a $k$-PIR array code) is also a locally repairable code with symbol availability $k-1$. Such a code ensures $k$ parallel reads for each information symbol. So the virtual server rate is very closely related to the symbol availability of the code when used as a locally repairable code. The results of this paper are discussed also in this context, where subspace codes also have an important role.',\n",
       "  'Title: deep communicating agents for abstractive summarization\\nAbstract: We present deep communicating agents in an encoder-decoder architecture to address the challenges of representing a long document for abstractive summarization. With deep communicating agents, the task of encoding a long text is divided across multiple collaborating agents, each in charge of a subsection of the input text. These encoders are connected to a single decoder, trained end-to-end using reinforcement learning to generate a focused and coherent summary. Empirical results demonstrate that multiple communicating encoders lead to a higher quality summary compared to several strong baselines, including those based on a single encoder or multiple non-communicating encoders.',\n",
       "  'Title: privacy preserving sensory data recovery\\nAbstract: In recent years, a large scale of various wireless sensor networks have been deployed for basic scientific works. Massive data loss is so common that there is a great demand for data recovery. While data recovery methods fulfil the requirement of accuracy, the potential privacy leakage caused by them concerns us a lot. Thus the major challenge of sensory data recovery is the issue of effective privacy preservation. Existing algorithms can either accomplish accurate data recovery or solve privacy issue, yet no single design is able to address these two problems simultaneously. Therefore in this paper, we propose a novel approach Privacy-Preserving Compressive Sensing with Multi-Attribute Assistance (PPCS-MAA). It applies PPCS scheme to sensory data recovery, which can effectively encrypts sensory data without decreasing accuracy, because it maintains the homomorphic obfuscation property for compressive sensing. In addition, multiple environmental attributes from sensory datasets usually have strong correlation so that we design a MultiAttribute Assistance (MAA) component to leverage this feature for better recovery accuracy. Combining PPCS with MAA, the novel recovery scheme can provide reliable privacy with high accuracy. Firstly, based on two real datasets, IntelLab and GreenOrbs, we reveal the inherited low-rank features as the ground truth and find such multi-attribute correlation. Secondly, we develop a PPCS-MAA algorithm to preserve privacy and optimize the recovery accuracy. Thirdly, the results of real data-driven simulations show that the algorithm outperforms the existing solutions.',\n",
       "  'Title: restoration of pansharpened images by conditional filtering in the pca domain\\nAbstract: Pansharpening techniques aim at fusing a low-spatial resolution multispectral (MS) image with a higher spatial resolution panchromatic (PAN) image to produce an MS image at high spatial resolution. Despite significant progress in the field, spectral and spatial distortions might still compromise the quality of the results. We introduce a restoration strategy to mitigate artifacts of fused products. After applying the principal component analysis transform to a pansharpened image, the chromatic components are filtered conditionally to the geometry of PAN. The structural component is then replaced by the locally histogram-matched PAN for spatial enhancement. Experimental results illustrate the efficiency of the proposed restoration chain.',\n",
       "  'Title: simple dynamic algorithms for maximal independent set and other problems\\nAbstract: Most graphs in real life keep changing with time. These changes can be in the form of insertion or deletion of edges or vertices. Such rapidly changing graphs motivate us to study dynamic graph algorithms. However, three important graph problems that are perhaps not sufficiently addressed in the literature include independent sets, maximum matching (exact) and maximum flows. #R##N#Maximal Independent Set (MIS) is one of the most prominently studied problems in the distributed setting. Recently, the first dynamic MIS algorithm for distributed networks was given by Censor-Hillel et al. [PODC16], requiring expected $O(1)$ amortized rounds with $O(\\\\Delta)$ messages per update, where $\\\\Delta$ is the maximum degree of a vertex in the graph. They suggested an open problem to maintain MIS in fully dynamic centralized setting more efficiently. Assadi et al. [STOC18] presented a deterministic centralized fully dynamic MIS algorithm requiring $O(\\\\min\\\\{\\\\Delta,m^{3/4}\\\\})$ amortized time per update. This result is quite complex involving an exhaustive case analysis. We report a surprisingly simple deterministic centralized algorithm which improves the amortized update time to $O(\\\\min\\\\{\\\\Delta,m^{2/3}\\\\})$. #R##N#Additionally, we present some other minor results related to dynamic MIS, Maximum Flow, and Maximum Matching. A common trait of all our results is that despite improving state of the art upper bounds or matching state of the art lower bounds, they are surprisingly simple and are analysed using simple amortization arguments. Further, they use no complicated data structures or black box algorithms for their implementation.',\n",
       "  'Title: attribute centered loss for soft biometrics guided face sketch photo recognition\\nAbstract: Face sketches are able to capture the spatial topology of a face while lacking some facial attributes such as race, skin, or hair color. Existing sketch-photo recognition approaches have mostly ignored the importance of facial attributes. In this paper, we propose a new loss function, called attribute-centered loss, to train a Deep Coupled Convolutional Neural Network (DCCNN) for the facial attribute guided sketch to photo matching. Specifically, an attribute-centered loss is proposed which learns several distinct centers, in a shared embedding space, for photos and sketches with different combinations of attributes. The DCCNN simultaneously is trained to map photos and pairs of testified attributes and corresponding forensic sketches around their associated centers, while preserving the spatial topology information. Importantly, the centers learn to keep a relative distance from each other, related to their number of contradictory attributes. Extensive experiments are performed on composite (E-PRIP) and semi-forensic (IIIT-D Semi-forensic) databases. The proposed method significantly outperforms the state-of-the-art.',\n",
       "  'Title: deepim deep iterative matching for 6d pose estimation\\nAbstract: Estimating 6D poses of objects from images is an important problem in various applications such as robot manipulation and virtual reality. While direct regression of images to object poses has limited accuracy, matching rendered images of an object against the input image can produce accurate results. In this work, we propose a novel deep neural network for 6D pose matching named DeepIM. Given an initial pose estimation, our network is able to iteratively refine the pose by matching the rendered image against the observed image. The network is trained to predict a relative pose transformation using a disentangled representation of 3D location and 3D orientation and an iterative training process. Experiments on two commonly used benchmarks for 6D pose estimation demonstrate that DeepIM achieves large improvements over state-of-the-art methods. We furthermore show that DeepIM is able to match previously unseen objects.',\n",
       "  \"Title: movie pirates of the caribbean exploring illegal streaming cyberlockers\\nAbstract: Online video piracy (OVP) is a contentious topic, with strong proponents on both sides of the argument. Recently, a number of illegal websites, called streaming cyberlockers, have begun to dominate OVP. These websites specialise in distributing pirated content, underpinned by third party indexing services offering easy-to-access directories of content. This paper performs the first exploration of this new ecosystem. It characterises the content, as well the streaming cyberlockers' individual attributes. We find a remarkably centralised system with just a few networks, countries and cyberlockers underpinning most provisioning. We also investigate the actions of copyright enforcers. We find they tend to target small subsets of the ecosystem, although they appear quite successful. 84% of copyright notices see content removed.\",\n",
       "  'Title: spatially coupled turbo like codes a new trade off between waterfall and error floor\\nAbstract: Spatially coupled turbo-like codes (SC-TCs) have been shown to have excellent decoding thresholds due to the threshold saturation effect. Furthermore, even for moderate block lengths, simulation results demonstrate very good bit error rate performance (BER) in the waterfall region. In this paper, we discuss the effect of spatial coupling on the performance of TCs in the finite block-length regime. We investigate the effect of coupling on the error-floor performance of SC-TCs by establishing conditions under which spatial coupling either preserves or improves the minimum distance of TCs. This allows us to investigate the error-floor performance of SC-TCs by performing a weight enumerator function (WEF) analysis of the corresponding uncoupled ensembles. While uncoupled TC ensembles with close-to-capacity performance exhibit a high error floor, our results show that SC-TCs can simultaneously approach capacity and achieve very low error floor.',\n",
       "  'Title: question type guided attention in visual question answering\\nAbstract: Visual Question Answering (VQA) requires integration of feature maps with drastically different structures and focus of the correct regions. Image descriptors have structures at multiple spatial scales, while lexical inputs inherently follow a temporal sequence and naturally cluster into semantically different question types. A lot of previous works use complex models to extract feature representations but neglect to use high-level information summary such as question types in learning. In this work, we propose Question Type-guided Attention (QTA). It utilizes the information of question type to dynamically balance between bottom-up and top-down visual features, respectively extracted from ResNet and Faster R-CNN networks. We experiment with multiple VQA architectures with extensive input ablation studies over the TDIUC dataset and show that QTA systematically improves the performance by more than 5% across multiple question type categories such as \"Activity Recognition\", \"Utility\" and \"Counting\" on TDIUC dataset. By adding QTA on the state-of-art model MCB, we achieve 3% improvement for overall accuracy. Finally, we propose a multi-task extension to predict question types which generalizes QTA to applications that lack of question type, with minimal performance loss.',\n",
       "  \"Title: adversarial training versus weight decay\\nAbstract: Performance-critical machine learning models should be robust to input perturbations not seen during training. Adversarial training is a method for improving a model's robustness to some perturbations by including them in the training process, but this tends to exacerbate other vulnerabilities of the model. The adversarial training framework has the effect of translating the data with respect to the cost function, while weight decay has a scaling effect. Although weight decay could be considered a crude regularization technique, it appears superior to adversarial training as it remains stable over a broader range of regimes and reduces all generalization errors. Equipped with these abstractions, we provide key baseline results and methodology for characterizing robustness. The two approaches can be combined to yield one small model that demonstrates good robustness to several white-box attacks associated with different metrics.\",\n",
       "  'Title: enhancing cybersecurity skills by creating serious games\\nAbstract: Adversary thinking is an essential skill for cybersecurity experts, enabling them to understand cyber attacks and set up effective defenses. While this skill is commonly exercised by Capture the Flag games and hands-on activities, we complement these approaches with a key innovation: undergraduate students learn methods of network attack and defense by creating educational games in a cyber range. In this paper, we present the design of two courses, instruction and assessment techniques, as well as our observations over the last three semesters. The students report they had a unique opportunity to deeply understand the topic and practice their soft skills, as they presented their results at a faculty open day event. Their peers, who played the created games, rated the quality and educational value of the games overwhelmingly positively. Moreover, the open day raised awareness about cybersecurity and research and development in this field at our faculty. We believe that sharing our teaching experience will be valuable for instructors planning to introduce active learning of cybersecurity and adversary thinking.',\n",
       "  'Title: netadapt platform aware neural network adaptation for mobile applications\\nAbstract: This work proposes an algorithm, called NetAdapt, that automatically adapts a pre-trained deep neural network to a mobile platform given a resource budget. While many existing algorithms simplify networks based on the number of MACs or weights, optimizing those indirect metrics may not necessarily reduce the direct metrics, such as latency and energy consumption. To solve this problem, NetAdapt incorporates direct metrics into its adaptation algorithm. These direct metrics are evaluated using empirical measurements, so that detailed knowledge of the platform and toolchain is not required. NetAdapt automatically and progressively simplifies a pre-trained network until the resource budget is met while maximizing the accuracy. Experiment results show that NetAdapt achieves better accuracy versus latency trade-offs on both mobile CPU and mobile GPU, compared with the state-of-the-art automated network simplification algorithms. For image classification on the ImageNet dataset, NetAdapt achieves up to a 1.7$\\\\times$ speedup in measured inference latency with equal or higher accuracy on MobileNets (V1&V2).',\n",
       "  'Title: peeking the impact of points of interests on didi\\nAbstract: Recently, the online car-hailing service, Didi, has emerged as a leader in the sharing economy. Used by passengers and drivers extensive, it becomes increasingly important for the car-hailing service providers to minimize the waiting time of passengers and optimize the vehicle utilization, thus to improve the overall user experience. Therefore, the supply-demand estimation is an indispensable ingredient of an efficient online car-hailing service. To improve the accuracy of the estimation results, we analyze the implicit relationships between the points of Interest (POI) and the supply-demand gap in this paper. The different categories of POIs have positive or negative effects on the estimation, we propose a POI selection scheme and incorporate it into XGBoost [1] to achieve more accurate estimation results. Our experiment demonstrates our method provides more accurate estimation results and more stable estimation results than the existing methods.',\n",
       "  'Title: quantum b uchi automata\\nAbstract: This paper defines a notion of quantum B\\\\\"uchi automaton (QBA for short) with two different acceptance conditions for {\\\\omega}-words: non-disturbing and disturbing. Several pumping lemmas are established for QBAs. The relationship between the {\\\\omega}-languages accepted by QBAs and those accepted by classical B\\\\\"uchi automata are clarified with the help of the pumping lemmas. The closure properties of the languages accepted by QBAs are studied in the probable, almost sure and threshold semantics. The decidability of the emptiness problem for the languages accepted by QBAs is proved using the Tarski-Seidenberg elimination.',\n",
       "  'Title: crisismmd multimodal twitter datasets from natural disasters\\nAbstract: During natural and man-made disasters, people use social media platforms such as Twitter to post textual and multime- dia content to report updates about injured or dead people, infrastructure damage, and missing or found people among other information types. Studies have revealed that this on- line information, if processed timely and effectively, is ex- tremely useful for humanitarian organizations to gain situational awareness and plan relief operations. In addition to the analysis of textual content, recent studies have shown that imagery content on social media can boost disaster response significantly. Despite extensive research that mainly focuses on textual content to extract useful information, limited work has focused on the use of imagery content or the combination of both content types. One of the reasons is the lack of labeled imagery data in this domain. Therefore, in this paper, we aim to tackle this limitation by releasing a large multi-modal dataset collected from Twitter during different natural disasters. We provide three types of annotations, which are useful to address a number of crisis response and management tasks for different humanitarian organizations.',\n",
       "  'Title: low rank approximation in the presence of outliers\\nAbstract: We consider the problem of principal component analysis (PCA) in the presence of outliers. Given a matrix $A$ ($d \\\\times n$) and parameters $k, m$, the goal is to remove a set of at most $m$ columns of $A$ (known as outliers), so as to minimize the rank-$k$ approximation error of the remaining matrix. While much of the work on this problem has focused on recovery of the rank-$k$ subspace under assumptions on the inliers and outliers, we focus on the approximation problem above. Our main result shows that sampling-based methods developed in the outlier-free case give non-trivial guarantees even in the presence of outliers. Using this insight, we develop a simple algorithm that has bi-criteria guarantees. Further, unlike similar formulations for clustering, we show that bi-criteria guarantees are unavoidable for the problem, under appropriate complexity assumptions.',\n",
       "  'Title: lightweight probabilistic deep networks\\nAbstract: Even though probabilistic treatments of neural networks have a long history, they have not found widespread use in practice. Sampling approaches are often too slow already for simple networks. The size of the inputs and the depth of typical CNN architectures in computer vision only compound this problem. Uncertainty in neural networks has thus been largely ignored in practice, despite the fact that it may provide important information about the reliability of predictions and the inner workings of the network. In this paper, we introduce two lightweight approaches to making supervised learning with probabilistic deep networks practical: First, we suggest probabilistic output layers for classification and regression that require only minimal changes to existing networks. Second, we employ assumed density filtering and show that activation uncertainties can be propagated in a practical fashion through the entire network, again with minor changes. Both probabilistic networks retain the predictive power of the deterministic counterpart, but yield uncertainties that correlate well with the empirical error induced by their predictions. Moreover, the robustness to adversarial examples is significantly increased.',\n",
       "  \"Title: inceptext a new inception text module with deformable psroi pooling for multi oriented scene text detection\\nAbstract: Incidental scene text detection, especially for multi-oriented text regions, is one of the most challenging tasks in many computer vision applications. Different from the common object detection task, scene text often suffers from a large variance of aspect ratio, scale, and orientation. To solve this problem, we propose a novel end-to-end scene text detector IncepText from an instance-aware segmentation perspective. We design a novel Inception-Text module and introduce deformable PSROI pooling to deal with multi-oriented text detection. Extensive experiments on ICDAR2015, RCTW-17, and MSRA-TD500 datasets demonstrate our method's superiority in terms of both effectiveness and efficiency. Our proposed method achieves 1st place result on ICDAR2015 challenge and the state-of-the-art performance on other datasets. Moreover, we have released our implementation as an OCR product which is available for public access.\",\n",
       "  'Title: understanding uav cellular communications from existing networks to massive mimo\\nAbstract: The purpose of this article is to bestow the reader with a timely study of UAV cellular communications, bridging the gap between the 3GPP standardization status quo and the more forward-looking research. Special emphasis is placed on the downlink command and control (CC (ii) over a 10 MHz bandwidth, and for UAV heights of up to 300 m, massive MIMO networks can support 100 kbps CC (iii) supporting UAV C&C channels can considerably affect the performance of ground users on account of severe pilot contamination, unless suitable power control policies are in place.',\n",
       "  'Title: automation of road intersections using consensus based auction algorithms\\nAbstract: This paper investigates a consensus-based auction algorithm in the context of decentralized traffic control. In particular, we study the automation of a road intersection, where a set of vehicles is required to cross without collisions. The crossing order will be negotiated in a decentralized fashion. An on-board model predictive controller (MPC) will compute an optimal trajectory which avoids collisions with higher priority vehicles, thus retaining convex safety constraints. Simulations are then performed in a time-variant traffic environment.',\n",
       "  'Title: analogical reasoning on chinese morphological and semantic relations\\nAbstract: Analogical reasoning is effective in capturing linguistic regularities. This paper proposes an analogical reasoning task on Chinese. After delving into Chinese lexical knowledge, we sketch 68 implicit morphological relations and 28 explicit semantic relations. A big and balanced dataset CA8 is then built for this task, including 17813 questions. Furthermore, we systematically explore the influences of vector representations, context features, and corpora on analogical reasoning. With the experiments, CA8 is proved to be a reliable benchmark for evaluating Chinese word embeddings.',\n",
       "  \"Title: target apps selection towards a unified search framework for mobile devices\\nAbstract: With the recent growth of conversational systems and intelligent assistants such as Apple Siri and Google Assistant, mobile devices are becoming even more pervasive in our lives. As a consequence, users are getting engaged with the mobile apps and frequently search for an information need in their apps. However, users cannot search within their apps through their intelligent assistants. This requires a unified mobile search framework that identifies the target app(s) for the user's query, submits the query to the app(s), and presents the results to the user. In this paper, we take the first step forward towards developing unified mobile search. In more detail, we introduce and study the task of target apps selection, which has various potential real-world applications. To this aim, we analyze attributes of search queries as well as user behaviors, while searching with different mobile apps. The analyses are done based on thousands of queries that we collected through crowdsourcing. We finally study the performance of state-of-the-art retrieval models for this task and propose two simple yet effective neural models that significantly outperform the baselines. Our neural approaches are based on learning high-dimensional representations for mobile apps. Our analyses and experiments suggest specific future directions in this research area.\",\n",
       "  'Title: symmetric rendezvous with advice how to rendezvous in a disk\\nAbstract: In the classic Symmetric Rendezvous problem on a Line (SRL), two robots at known distance 2 but unknown direction execute the same randomized algorithm trying to minimize the expected rendezvous time. A long standing conjecture is that the best possible rendezvous time is 4.25 with known upper and lower bounds being very close to that value. We introduce and study a geometric variation of SRL that we call Symmetric Rendezvous in a Disk (SRD) where two robots at distance 2 have a common reference point at distance $\\\\rho$. We show that even when $\\\\rho$ is not too small, the two robots can meet in expected time that is less than $4.25$. Part of our contribution is that we demonstrate how to adjust known, even simple and provably non-optimal, algorithms for SRL, effectively improving their performance in the presence of a reference point. Special to our algorithms for SRD is that, unlike in SRL, for every fixed $\\\\rho$ the worst case distance traveled, i.e. energy that is used, in our algorithms is finite. In particular, we show that the energy of our algorithms is $O\\\\left(\\\\rho^2\\\\right)$, while we also explore time-energy tradeoffs, concluding that one may be efficient both with respect to time and energy, with only a minor compromise on the optimal termination time.',\n",
       "  'Title: comparing phonemes and visemes with dnn based lipreading\\nAbstract: There is debate if phoneme or viseme units are the most effective for a lipreading#R##N#system. Some studies use phoneme units even though phonemes describe unique short#R##N#sounds; other studies tried to improve lipreading accuracy by focusing on visemes with#R##N#varying results. We compare the performance of a lipreading system by modeling visual#R##N#speech using either 13 viseme or 38 phoneme units. We report the accuracy of our#R##N#system at both word and unit levels. The evaluation task is large vocabulary continuous#R##N#speech using the TCD-TIMIT corpus. We complete our visual speech modeling via#R##N#hybrid DNN-HMMs and our visual speech decoder is aWeighted Finite-State Transducer#R##N#(WFST). We use DCT and Eigenlips as a representation of mouth ROI image. The#R##N#phoneme lipreading system word accuracy outperforms the viseme based system word#R##N#accuracy. However, the phoneme system achieved lower accuracy at the unit level which#R##N#shows the importance of the dictionary for decoding classification outputs into words.',\n",
       "  'Title: advanced local motion patterns for macro and micro facial expression recognition\\nAbstract: In this paper, we develop a new method that recognizes facial expressions, on the basis of an innovative local motion patterns feature, with three main contributions. The first one is the analysis of the face skin temporal elasticity and face deformations during expression. The second one is a unified approach for both macro and micro expression recognition. And, the third one is the step forward towards in-the-wild expression recognition, dealing with challenges such as various intensity and various expression activation patterns, illumination variation and small head pose variations. Our method outperforms state-of-the-art methods for micro expression recognition and positions itself among top-rank state-of-the-art methods for macro expression recognition.',\n",
       "  \"Title: the wisdom of the network how adaptive networks promote collective intelligence\\nAbstract: Social networks continuously change as people create new ties and break existing ones. It is widely noted that our social embedding exerts strong influence on what information we receive, and how we form beliefs and make decisions. However, most studies overlook the dynamic nature of social networks, and its role in fostering adaptive collective intelligence. It remains unknown (1) how network structures adapt to the performances of individuals, and (2) whether this adaptation promotes the accuracy of individual and collective decisions. Here, we answer these questions through a series of behavioral experiments and simulations. Our results reveal that groups of people embedded in dynamic social networks can adapt to biased and non-stationary information environments. As a result, individual and collective accuracy is substantially improved over static networks and unconnected groups. Moreover, we show that groups in dynamic networks far outperform their best-performing member, and that even the best member's judgment substantially benefits from group engagement. Thereby, our findings substantiate the role of dynamic social networks as adaptive mechanisms for refining individual and collective judgments.\",\n",
       "  'Title: distributed multiple access with multiple transmission options at the link layer\\nAbstract: This paper investigates the problem of distributed medium access control in a wireless multiple access network with an unknown finite number of homogeneous transmitters. An enhanced physical link layer interface is considered where each link layer user can be equipped with multiple transmission options. Assume that each user is backlogged with a saturated message queue. With a generally-modeled channel, a distributed medium access control framework is suggested to adapt the transmission scheme of each user to maximize an arbitrarily chosen symmetric network utility. The proposed framework suggests that the receiver should measure the success probability of a carefully designed virtual packet, and feed such information back to the transmitters. Upon receiving the measured probability, each transmitter should obtain a user number estimate by comparing the probability with its theoretical value, and then adapt its transmission scheme accordingly. Conditions under which the proposed algorithm should converge to a designed unique equilibrium are characterized. Simulation results are provided to demonstrate the optimality and the convergence properties of the proposed algorithm.',\n",
       "  'Title: multicast networks solvable over every finite field\\nAbstract: In this work, it is revealed that an acyclic multicast network that is scalar linearly solvable over Galois Field of two elements, GF(2), is solvable over all higher finite fields. An algorithm which, given a GF(2) solution for an acyclic multicast network, computes the solution over any arbitrary finite field is presented. The concept of multicast matroid is introduced in this paper. Gammoids and their base-orderability along with the regularity of a binary multicast matroid are used to prove the results.',\n",
       "  \"Title: robustness of sentence length measures in written texts\\nAbstract: Hidden structural patterns in written texts have been subject of considerable research in the last decades. In particular, mapping a text into a time series of sentence lengths is a natural way to investigate text structure. Typically, sentence length has been quantified by using measures based on the number of words and the number of characters, but other variations are possible. To quantify the robustness of different sentence length measures, we analyzed a database containing about five hundred books in English. For each book, we extracted six distinct measures of sentence length, including number of words and number of characters (taking into account lemmatization and stop words removal). We compared these six measures for each book by using i) Pearson's coefficient to investigate linear correlations; ii) Kolmogorov--Smirnov test to compare distributions; and iii) detrended fluctuation analysis (DFA) to quantify auto-correlations. We have found that all six measures exhibit very similar behavior, suggesting that sentence length is a robust measure related to text structure.\",\n",
       "  \"Title: scalable methods for 8 bit training of neural networks\\nAbstract: Quantized Neural Networks (QNNs) are often used to improve network efficiency during the inference phase, i.e. after the network has been trained. Extensive research in the field suggests many different quantization schemes. Still, the number of bits required, as well as the best quantization scheme, are yet unknown. Our theoretical analysis suggests that most of the training process is robust to substantial precision reduction, and points to only a few specific operations that require higher precision. Armed with this knowledge, we quantize the model parameters, activations and layer gradients to 8-bit, leaving at a higher precision only the final step in the computation of the weight gradients. Additionally, as QNNs require batch-normalization to be trained at high precision, we introduce Range Batch-Normalization (BN) which has significantly higher tolerance to quantization noise and improved computational complexity. Our simulations show that Range BN is equivalent to the traditional batch norm if a precise scale adjustment, which can be approximated analytically, is applied. To the best of the authors' knowledge, this work is the first to quantize the weights, activations, as well as a substantial volume of the gradients stream, in all layers (including batch normalization) to 8-bit while showing state-of-the-art results over the ImageNet-1K dataset.\",\n",
       "  'Title: learning attentional communication for multi agent cooperation\\nAbstract: Communication could potentially be an effective way for multi-agent cooperation. However, information sharing among all agents or in predefined communication architectures that existing methods adopt can be problematic. When there is a large number of agents, agents cannot differentiate valuable information that helps cooperative decision making from globally shared information. Therefore, communication barely helps, and could even impair the learning of multi-agent cooperation. Predefined communication architectures, on the other hand, restrict communication among agents and thus restrain potential cooperation. To tackle these difficulties, in this paper, we propose an attentional communication model that learns when communication is needed and how to integrate shared information for cooperative decision making. Our model leads to efficient and effective communication for large-scale multi-agent cooperation. Empirically, we show the strength of our model in a variety of cooperative scenarios, where agents are able to develop more coordinated and sophisticated strategies than existing methods.',\n",
       "  'Title: generating high quality surface realizations using data augmentation and factored sequence models\\nAbstract: This work presents a new state of the art in reconstruction of surface realizations from obfuscated text. We identify the lack of sufficient training data as the major obstacle to training high-performing models, and solve this issue by generating large amounts of synthetic training data. We also propose preprocessing techniques which make the structure contained in the input features more accessible to sequence models. Our models were ranked first on all evaluation metrics in the English portion of the 2018 Surface Realization shared task.',\n",
       "  'Title: a simple stochastic variance reduced algorithm with fast convergence rates\\nAbstract: Recent years have witnessed exciting progress in the study of stochastic variance reduced gradient methods (e.g., SVRG, SAGA), their accelerated variants (e.g, Katyusha) and their extensions in many different settings (e.g., online, sparse, asynchronous, distributed). Among them, accelerated methods enjoy improved convergence rates but have complex coupling structures, which makes them hard to be extended to more settings (e.g., sparse and asynchronous) due to the existence of perturbation. In this paper, we introduce a simple stochastic variance reduced algorithm (MiG), which enjoys the best-known convergence rates for both strongly convex and non-strongly convex problems. Moreover, we also present its efficient sparse and asynchronous variants, and theoretically analyze its convergence rates in these settings. Finally, extensive experiments for various machine learning problems such as logistic regression are given to illustrate the practical improvement in both serial and asynchronous settings.',\n",
       "  'Title: wisenetmd motion detection using dynamic background region analysis\\nAbstract: Motion detection algorithms that can be applied to surveillance cameras such as CCTV (Closed Circuit Television) have been studied extensively. Motion detection algorithm is mostly based on background subtraction. One main issue in this technique is that false positives of dynamic backgrounds such as wind shaking trees and flowing rivers might occur. In this paper, we proposed a method to search for dynamic background region by analyzing the video and removing false positives by re-checking false positives. The proposed method was evaluated based on CDnet 2012/2014 dataset obtained at \"changedetection.net\" site. We also compared its processing speed with other algorithms.',\n",
       "  'Title: hyperbolic attention networks\\nAbstract: We introduce hyperbolic attention networks to endow neural networks with enough capacity to match the complexity of data with hierarchical and power-law structure. A few recent approaches have successfully demonstrated the benefits of imposing hyperbolic geometry on the parameters of shallow networks. We extend this line of work by imposing hyperbolic geometry on the activations of neural networks. This allows us to exploit hyperbolic geometry to reason about embeddings produced by deep networks. We achieve this by re-expressing the ubiquitous mechanism of soft attention in terms of operations defined for hyperboloid and Klein models. Our method shows improvements in terms of generalization on neural machine translation, learning on graphs and visual question answering tasks while keeping the neural representations compact.',\n",
       "  'Title: improving the gaussian mechanism for differential privacy analytical calibration and optimal denoising\\nAbstract: The Gaussian mechanism is an essential building block used in multitude of differentially private data analysis algorithms. In this paper we revisit the Gaussian mechanism and show that the original analysis has several important limitations. Our analysis reveals that the variance formula for the original mechanism is far from tight in the high privacy regime ($\\\\varepsilon \\\\to 0$) and it cannot be extended to the low privacy regime ($\\\\varepsilon \\\\to \\\\infty$). We address these limitations by developing an optimal Gaussian mechanism whose variance is calibrated directly using the Gaussian cumulative density function instead of a tail bound approximation. We also propose to equip the Gaussian mechanism with a post-processing step based on adaptive estimation techniques by leveraging that the distribution of the perturbation is known. Our experiments show that analytical calibration removes at least a third of the variance of the noise compared to the classical Gaussian mechanism, and that denoising dramatically improves the accuracy of the Gaussian mechanism in the high-dimensional regime.',\n",
       "  'Title: online multi object tracking with historical appearance matching and scene adaptive detection filtering\\nAbstract: In this paper, we propose the methods to handle temporal errors during multi-object tracking. Temporal error occurs when objects are occluded or noisy detections appear near the object. In those situations, tracking may fail and various errors like drift or ID-switching occur. It is hard to overcome temporal errors only by using motion and shape information. So, we propose the historical appearance matching method and joint-input siamese network which was trained by 2-step process. It can prevent tracking failures although objects are temporally occluded or last matching information is unreliable. We also provide useful technique to remove noisy detections effectively according to scene condition. Tracking performance, especially identity consistency, is highly improved by attaching our methods.',\n",
       "  \"Title: algorithms and analysis for the sparql constructs\\nAbstract: As Resource Description Framework (RDF) is becoming a popular data modelling standard, the challenges of efficient processing of Basic Graph Pattern (BGP) SPARQL queries (a.k.a. SQL inner-joins) have been a focus of the research community over the past several years. In our recently published work we brought community's attention to another equally important component of SPARQL, i.e., OPTIONAL pattern queries (a.k.a. SQL left-outer-joins). We proposed novel optimization techniques -- first of a kind -- and showed experimentally that our techniques perform better for the low-selectivity queries, and give at par performance for the highly selective queries, compared to the state-of-the-art methods. #R##N#BGPs and OPTIONALs (BGP-OPT) make the basic building blocks of the SPARQL query language. Thus, in this paper, treating our BGP-OPT query optimization techniques as the primitives, we extend them to handle other broader components of SPARQL such as such as UNION, FILTER, and DISTINCT. We mainly focus on the procedural (algorithmic) aspects of these extensions. We also make several important observations about the structural aspects of complex SPARQL queries with any intermix of these clauses, and relax some of the constraints regarding the cyclic properties of the queries proposed earlier. We do so without affecting the correctness of the results, thus providing more flexibility in using the BGP-OPT optimization techniques.\",\n",
       "  \"Title: annotating electronic medical records for question answering\\nAbstract: Our research is in the relatively unexplored area of question answering technologies for patient-specific questions over their electronic health records. A large dataset of human expert curated question and answer pairs is an important pre-requisite for developing, training and evaluating any question answering system that is powered by machine learning. In this paper, we describe a process for creating such a dataset of questions and answers. Our methodology is replicable, can be conducted by medical students as annotators, and results in high inter-annotator agreement (0.71 Cohen's kappa). Over the course of 11 months, 11 medical students followed our annotation methodology, resulting in a question answering dataset of 5696 questions over 71 patient records, of which 1747 questions have corresponding answers generated by the medical students.\",\n",
       "  'Title: a preliminary exploration of floating point grammatical evolution\\nAbstract: Current GP frameworks are highly effective on a range of real and simulated benchmarks. However, due to the high dimensionality of the genotypes for GP, the task of visualising the fitness landscape for GP search can be difficult. This paper describes a new framework: Floating Point Grammatical Evolution (FP-GE) which uses a single floating point genotype to encode an individual program. This encoding permits easier visualisation of the fitness landscape arbitrary problems by providing a way to map fitness against a single dimension. The new framework also makes it trivially easy to apply continuous search algorithms, such as Differential Evolution, to the search problem. In this work, the FP-GE framework is tested against several regression problems, visualising the search landscape for these and comparing different search meta-heuristics.',\n",
       "  \"Title: relay a new ir for machine learning frameworks\\nAbstract: Machine learning powers diverse services in industry including search, translation, recommendation systems, and security. The scale and importance of these models require that they be efficient, expressive, and portable across an array of heterogeneous hardware devices. These constraints are often at odds; in order to better accommodate them we propose a new high-level intermediate representation (IR) called Relay. Relay is being designed as a purely-functional, statically-typed language with the goal of balancing efficient compilation, expressiveness, and portability. We discuss the goals of Relay and highlight its important design constraints. Our prototype is part of the open source NNVM compiler framework, which powers Amazon's deep learning framework MxNet.\",\n",
       "  'Title: transformationally identical and invariant convolutional neural networks through symmetric element operators\\nAbstract: Mathematically speaking, a transformationally invariant operator, such as a transformationally identical (TI) matrix kernel (i.e., K= T{K}), commutes with the transformation (T{.}) itself when they operate on the first operand matrix. We found that by consistently applying the same type of TI kernels in a convolutional neural networks (CNN) system, the commutative property holds throughout all layers of convolution processes with and without involving an activation function and/or a 1D convolution across channels within a layer. We further found that any CNN possessing the same TI kernel property for all convolution layers followed by a flatten layer with weight sharing among their transformation corresponding elements would output the same result for all transformation versions of the original input vector. In short, CNN[ Vi ] = CNN[ T{Vi} ] providing every K = T{K} in CNN, where Vi denotes input vector and CNN[.] represents the whole CNN process as a function of input vector that produces an output vector. With such a transformationally identical CNN (TI-CNN) system, each transformation, that is not associated with a predefined TI used in data augmentation, would inherently include all of its corresponding transformation versions of the input vector for the training. Hence the use of same TI property for every kernel in the CNN would serve as an orientation or a translation independent training guide in conjunction with the error-backpropagation during the training. This TI kernel property is desirable for applications requiring a highly consistent output result from corresponding transformation versions of an input. Several C programming routines are provided to facilitate interested parties of using the TI-CNN technique which is expected to produce a better generalization performance than its ordinary CNN counterpart.',\n",
       "  'Title: dialogwae multimodal response generation with conditional wasserstein auto encoder\\nAbstract: Variational autoencoders (VAEs) have shown a promise in data-driven conversation modeling. However, most VAE conversation models match the approximate posterior distribution over the latent variables to a simple prior such as standard normal distribution, thereby restricting the generated responses to a relatively simple (e.g., single-modal) scope. In this paper, we propose DialogWAE, a conditional Wasserstein autoencoder (WAE) specially designed for dialogue modeling. Unlike VAEs that impose a simple distribution over the latent variables, DialogWAE models the distribution of data by training a GAN within the latent variable space. Specifically, our model samples from the prior and posterior distributions over the latent variables by transforming context-dependent random noise using neural networks and minimizes the Wasserstein distance between the two distributions. We further develop a Gaussian mixture prior network to enrich the latent space. Experiments on two widely-used datasets show that DialogWAE outperforms the state-of-the-art approaches in generating more coherent, informative and diverse responses.',\n",
       "  'Title: shape robust text detection with progressive scale expansion network\\nAbstract: Scene text detection has witnessed rapid progress especially with the recent development of convolutional neural networks. However, there still exists two challenges which prevent the algorithm into industry applications. On the one hand, most of the state-of-art algorithms require quadrangle bounding box which is in-accurate to locate the texts with arbitrary shape. On the other hand, two text instances which are close to each other may lead to a false detection which covers both instances. Traditionally, the segmentation-based approach can relieve the first problem but usually fail to solve the second challenge. To address these two challenges, in this paper, we propose a novel Progressive Scale Expansion Network (PSENet), which can precisely detect text instances with arbitrary shapes. More specifically, PSENet generates the different scale of kernels for each text instance, and gradually expands the minimal scale kernel to the text instance with the complete shape. Due to the fact that there are large geometrical margins among the minimal scale kernels, our method is effective to split the close text instances, making it easier to use segmentation-based methods to detect arbitrary-shaped text instances. Extensive experiments on CTW1500, Total-Text, ICDAR 2015 and ICDAR 2017 MLT validate the effectiveness of PSENet. Notably, on CTW1500, a dataset full of long curve texts, PSENet achieves a F-measure of 74.3% at 27 FPS, and our best F-measure (82.2%) outperforms state-of-art algorithms by 6.6%. The code will be released in the future.',\n",
       "  'Title: large margin classification in hyperbolic space\\nAbstract: Representing data in hyperbolic space can effectively capture latent hierarchical relationships. With the goal of enabling accurate classification of points in hyperbolic space while respecting their hyperbolic geometry, we introduce hyperbolic SVM, a hyperbolic formulation of support vector machine classifiers, and elucidate through new theoretical work its connection to the Euclidean counterpart. We demonstrate the performance improvement of hyperbolic SVM for multi-class prediction tasks on real-world complex networks as well as simulated datasets. Our work allows analytic pipelines that take the inherent hyperbolic geometry of the data into account in an end-to-end fashion without resorting to ill-fitting tools developed for Euclidean space.',\n",
       "  'Title: end to end named entity extraction from speech\\nAbstract: Named entity recognition (NER) is among SLU tasks that usually extract semantic information from textual documents. Until now, NER from speech is made through a pipeline process that consists in processing first an automatic speech recognition (ASR) on the audio and then processing a NER on the ASR outputs. Such approach has some disadvantages (error propagation, metric to tune ASR systems sub-optimal in regards to the final task, reduced space search at the ASR output level...) and it is known that more integrated approaches outperform sequential ones, when they can be applied. In this paper, we present a first study of end-to-end approach that directly extracts named entities from speech, though a unique neural architecture. On a such way, a joint optimization is able for both ASR and NER. Experiments are carried on French data easily accessible, composed of data distributed in several evaluation campaign. Experimental results show that this end-to-end approach provides better results (F-measure=0.69 on test data) than a classical pipeline approach to detect named entity categories (F-measure=0.65).',\n",
       "  'Title: ten years of research on intelligent educational games for learning spelling and mathematics\\nAbstract: In this article, we present our findings from ten years of research on intelligent educational games. We discuss the architecture of our training environments for learning spelling and mathematics, and specifically focus on the representation of the content and the controller that enables personalized trainings. We first show the multi-modal representation that reroutes information through multiple perceptual cues and discuss the game structure. We then present the data-driven student model that is used for a personalized, adaptive presentation of the content. We further leverage machine learning for analytics and visualization tools targeted at teachers and experts. A large data set consisting of training sessions of more than 20,000 children allows statistical interpretations and insights into the nature of learning.',\n",
       "  'Title: improving surgical training phantoms by hyperrealism deep unpaired image to image translation from real surgeries\\nAbstract: Current ‘dry lab’ surgical phantom simulators are a valuable tool for surgeons which allows them to improve their dexterity and skill with surgical instruments. These phantoms mimic the haptic and shape of organs of interest, but lack a realistic visual appearance. In this work, we present an innovative application in which representations learned from real intraoperative endoscopic sequences are transferred to a surgical phantom scenario. The term hyperrealism is introduced in this field, which we regard as a novel subform of surgical augmented reality for approaches that involve real-time object transfigurations. For related tasks in the computer vision community, unpaired cycle-consistent Generative Adversarial Networks (GANs) have shown excellent results on still RGB images. Though, application of this approach to continuous video frames can result in flickering, which turned out to be especially prominent for this application. Therefore, we propose an extension of cycle-consistent GANs, named tempCycleGAN, to improve temporal consistency. The novel method is evaluated on captures of a silicone phantom for training endoscopic reconstructive mitral valve procedures. Synthesized videos show highly realistic results with regard to (1) replacement of the silicone appearance of the phantom valve by intraoperative tissue texture, while (2) explicitly keeping crucial features in the scene, such as instruments, sutures and prostheses. Compared to the original CycleGAN approach, tempCycleGAN efficiently removes flickering between frames. The overall approach is expected to change the future design of surgical training simulators since the generated sequences clearly demonstrate the feasibility to enable a considerably more realistic training experience for minimally-invasive procedures.',\n",
       "  'Title: deep lip reading a comparison of models and an online application\\nAbstract: The goal of this paper is to develop state-of-the-art models for lip reading -- visual speech recognition. We develop three architectures and compare their accuracy and training times: (i) a recurrent model using LSTMs; (ii) a fully convolutional model; and (iii) the recently proposed transformer model. The recurrent and fully convolutional models are trained with a Connectionist Temporal Classification loss and use an explicit language model for decoding, the transformer is a sequence-to-sequence model. Our best performing model improves the state-of-the-art word error rate on the challenging BBC-Oxford Lip Reading Sentences 2 (LRS2) benchmark dataset by over 20 percent. #R##N#As a further contribution we investigate the fully convolutional model when used for online (real time) lip reading of continuous speech, and show that it achieves high performance with low latency.',\n",
       "  'Title: learning 6 dof grasping and pick place using attention focus\\nAbstract: We address a class of manipulation problems where the robot perceives the scene with a depth sensor and can move its end effector in a space with six degrees of freedom -- 3D position and orientation. Our approach is to formulate the problem as a Markov decision process (MDP) with abstract yet generally applicable state and action representations. Finding a good solution to the MDP requires adding constraints on the allowed actions. We develop a specific set of constraints called hierarchical $\\\\text{SE}(3)$ sampling (HSE3S) which causes the robot to learn a sequence of gazes to focus attention on the task-relevant parts of the scene. We demonstrate the effectiveness of our approach on three challenging pick-place tasks (with novel objects in clutter and nontrivial places) both in simulation and on a real robot, even though all training is done in simulation.',\n",
       "  'Title: right for the right reason training agnostic networks\\nAbstract: We consider the problem of a neural network being requested to classify images (or other inputs) without making implicit use of a “protected concept”, that is a concept that should not play any role in the decision of the network. Typically these concepts include information such as gender or race, or other contextual information such as image backgrounds that might be implicitly reflected in unknown correlations with other variables, making it insufficient to simply remove them from the input features. In other words, making accurate predictions is not good enough if those predictions rely on information that should not be used: predictive performance is not the only important metric for learning systems. We apply a method developed in the context of domain adaptation to address this problem of “being right for the right reason”, where we request a classifier to make a decision in a way that is entirely ‘agnostic’ to a given protected concept (e.g. gender, race, background etc.), even if this could be implicitly reflected in other attributes via unknown correlations. After defining the concept of an ‘agnostic model’, we demonstrate how the Domain-Adversarial Neural Network can remove unwanted information from a model using a gradient reversal layer.',\n",
       "  'Title: latent heterogeneous multilayer community detection\\nAbstract: We propose a method for simultaneously detecting shared and unshared communities in heterogeneous multilayer weighted and undirected networks. The multilayer network is assumed to follow a generative probabilistic model that takes into account the similarities and dissimilarities between the communities. We make use of a variational Bayes approach for jointly inferring the shared and unshared hidden communities from multilayer network observations. We show the robustness of our approach compared to state-of-the art algorithms in detecting disparate (shared and private) communities on synthetic data as well as on real genome-wide fibroblast proliferation dataset.',\n",
       "  'Title: probabilistic natural language generation with wasserstein autoencoders\\nAbstract: Probabilistic generation of natural language sentences is an important task in NLP. Existing models such as variational autoencoders (VAE) for sequence generation are extremely difficult to train due to the issues associated with the Kullback-Leibler (KL) loss collapsing to zero. One has to implement various heuristics such as KL weight annealing and word dropout in a carefully engineered manner to successfully train a text VAE. In this paper, we propose the use of Wasserstein autoencoders (WAE) for probabilistic natural language sentence generation. We show that sequence-to-sequence WAEs are more robust towards hyperparameters and can be trained in a straightforward manner without the need for any weight annealing. Empirical evidence shows that the latent space learned by WAEs exhibits properties of continuity and smoothness as in VAEs, while simultaneously achieving much higher BLEU scores for sentence reconstruction.',\n",
       "  'Title: calorinet from silhouettes to calorie estimation in private environments\\nAbstract: We propose a novel deep fusion architecture, CaloriNet, for the online estimation of energy expenditure for free living monitoring in private environments, where RGB data is discarded and replaced by silhouettes. Our fused convolutional neural network architecture is trainable end-to-end, to estimate calorie expenditure, using temporal foreground silhouettes alongside accelerometer data. The network is trained and cross-validated on a publicly available dataset, SPHERE_RGBD + Inertial_calorie. Results show state-of-the-art minimum error on the estimation of energy expenditure (calories per minute), outperforming alternative, standard and single-modal techniques.',\n",
       "  'Title: binary ensemble neural network more bits per network or more networks per bit\\nAbstract: Binary neural networks (BNN) have been studied extensively since they run dramatically faster at lower memory and power consumption than floating-point networks, thanks to the efficiency of bit operations. However, contemporary BNNs whose weights and activations are both single bits suffer from severe accuracy degradation. To understand why, we investigate the representation ability, speed and bias/variance of BNNs through extensive experiments. We conclude that the error of BNNs is predominantly caused by the intrinsic instability (training time) and non-robustness (train & test time). Inspired by this investigation, we propose the Binary Ensemble Neural Network (BENN) which leverages ensemble methods to improve the performance of BNNs with limited efficiency cost. While ensemble techniques have been broadly believed to be only marginally helpful for strong classifiers such as deep neural networks, our analyses and experiments show that they are naturally a perfect fit to boost BNNs. We find that our BENN, which is faster and much more robust than state-of-the-art binary networks, can even surpass the accuracy of the full-precision floating number network with the same architecture.',\n",
       "  'Title: stability of kalman filtering with a random measurement equation application to sensor scheduling with intermittent observations\\nAbstract: Studying the stability of the Kalman filter whose measurements are randomly lost has been an active research topic for over a decade. In this paper we extend the existing results to a far more general setting in which the measurement equation, i.e., the measurement matrix and the measurement error covariance, are random. Our result also generalizes existing ones in the sense that it does not require the system matrix to be diagonalizable. For this general setting, we state a necessary and a sufficient condition for stability, and address its numerical computation. An important application of our generalization is a networking setting with multiple sensors which transmit their measurement to the estimator using a sensor scheduling protocol over a lossy network. We demonstrate how our result is used for assessing the stability of a Kalman filter in this multi-sensor setting.',\n",
       "  'Title: solving atari games using fractals and entropy\\nAbstract: In this paper, we introduce a novel MCTS based approach that is derived from the laws of the thermodynamics. The algorithm coined Fractal Monte Carlo (FMC), allows us to create an agent that takes intelligent actions in both continuous and discrete environments while providing control over every aspect of the agent behavior. Results show that FMC is several orders of magnitude more efficient than similar techniques, such as MCTS, in the Atari games tested.',\n",
       "  'Title: reachability in timed automata with diagonal constraints\\nAbstract: We consider the reachability problem for timed automata having diagonal constraints (like x - y < 5) as guards in transitions. The best algorithms for timed automata proceed by enumerating reachable sets of its configurations, stored in the form of a data structure called \"zones\". Simulation relations between zones are essential to ensure termination and efficiency. The algorithm employs a simulation test \"is-Z-simulated-by-Z\\' ?\" which ascertains that zone Z does not reach more states than zone Z\\', and hence further enumeration from Z is not necessary. No effective simulations are known for timed automata containing diagonal constraints as guards. In this paper, we propose a simulation relation LU-d for timed automata with diagonal constraints. On the negative side, we show that deciding Z-is-not-LU-d-simulated-by-Z\\' is NP-complete. On the positive side, we identify a witness for non-simulation and propose an algorithm to decide the existence of such a witness using an SMT solver. The shape of the witness reveals that the simulation test is likely to be efficient in practice.',\n",
       "  \"Title: octen online compression based tensor decomposition\\nAbstract: Tensor decompositions are powerful tools for large data analytics as they jointly model multiple aspects of data into one framework and enable the discovery of the latent structures and higher-order correlations within the data. One of the most widely studied and used decompositions, especially in data mining and machine learning, is the Canonical Polyadic or CP decomposition. However, today's datasets are not static and these datasets often dynamically growing and changing with time. To operate on such large data, we present OCTen the first ever compression-based online parallel implementation for the CP decomposition. We conduct an extensive empirical analysis of the algorithms in terms of fitness, memory used and CPU time, and in order to demonstrate the compression and scalability of the method, we apply OCTen to big tensor data. Indicatively, OCTen performs on-par or better than state-of-the-art online and online methods in terms of decomposition accuracy and efficiency, while saving up to 40-200 % memory space.\",\n",
       "  'Title: neuro symbolic execution the feasibility of an inductive approach to symbolic execution\\nAbstract: Symbolic execution is a powerful technique for program analysis. However, it has many limitations in practical applicability: the path explosion problem encumbers scalability, the need for language-specific implementation, the inability to handle complex dependencies, and the limited expressiveness of theories supported by underlying satisfiability checkers. Often, relationships between variables of interest are not expressible directly as purely symbolic constraints. To this end, we present a new approach -- neuro-symbolic execution -- which learns an approximation of the relationship as a neural net. It features a constraint solver that can solve mixed constraints, involving both symbolic expressions and neural network representation. To do so, we envision such constraint solving as procedure combining SMT solving and gradient-based optimization. We demonstrate the utility of neuro-symbolic execution in constructing exploits for buffer overflows. We report success on 13/14 programs which have difficult constraints, known to require specialized extensions to symbolic execution. In addition, our technique solves $100$\\\\% of the given neuro-symbolic constraints in $73$ programs from standard verification and invariant synthesis benchmarks.',\n",
       "  'Title: search rank fraud de anonymization in online systems\\nAbstract: We introduce the fraud de-anonymization problem, that goes beyond fraud detection, to unmask the human masterminds responsible for posting search rank fraud in online systems. We collect and study search rank fraud data from Upwork, and survey the capabilities and behaviors of 58 search rank fraudsters recruited from 6 crowdsourcing sites. We propose Dolos, a fraud de-anonymization system that leverages traits and behaviors extracted from these studies, to attribute detected fraud to crowdsourcing site fraudsters, thus to real identities and bank accounts. We introduce MCDense, a min-cut dense component detection algorithm to uncover groups of user accounts controlled by different fraudsters, and leverage stylometry and deep learning to attribute them to crowdsourcing site profiles. Dolos correctly identified the owners of 95% of fraudster-controlled communities, and uncovered fraudsters who promoted as many as 97.5% of fraud apps we collected from Google Play. When evaluated on 13,087 apps (820,760 reviews), which we monitored over more than 6 months, Dolos identified 1,056 apps with suspicious reviewer groups. We report orthogonal evidence of their fraud, including fraud duplicates and fraud re-posts.',\n",
       "  'Title: efficient parallel self assembly under uniform control inputs\\nAbstract: We prove that by successively combining subassemblies, we can achieve sublinear construction times for \"staged\" assembly of micro-scale objects from a large number of tiny particles, for vast classes of shapes; this is a significant advance in the context of programmable matter and self-assembly for building high-yield micro-factories.The underlying model has particles moving under the influence of uniform external forces until they hit an obstacle; particles bond when forced together with a compatible particle. Previous work considered sequential composition of objects, resulting in construction time that is linear in the number N of particles, which is inefficient for large N. Our progress implies critical speedup for constructible shapes; for convex polyominoes, even a constant construction time is possible. We also show that our construction process can be used for pipelining, resulting in an amortized constant production time.',\n",
       "  'Title: transfer learning for clinical time series analysis using recurrent neural networks\\nAbstract: Deep neural networks have shown promising results for various clinical prediction tasks such as diagnosis, mortality prediction, predicting duration of stay in hospital, etc. However, training deep networks -- such as those based on Recurrent Neural Networks (RNNs) -- requires large labeled data, high computational resources, and significant hyperparameter tuning effort. In this work, we investigate as to what extent can transfer learning address these issues when using deep RNNs to model multivariate clinical time series. We consider transferring the knowledge captured in an RNN trained on several source tasks simultaneously using a large labeled dataset to build the model for a target task with limited labeled data. An RNN pre-trained on several tasks provides generic features, which are then used to build simpler linear models for new target tasks without training task-specific RNNs. For evaluation, we train a deep RNN to identify several patient phenotypes on time series from MIMIC-III database, and then use the features extracted using that RNN to build classifiers for identifying previously unseen phenotypes, and also for a seemingly unrelated task of in-hospital mortality. We demonstrate that (i) models trained on features extracted using pre-trained RNN outperform or, in the worst case, perform as well as task-specific RNNs; (ii) the models using features from pre-trained models are more robust to the size of labeled data than task-specific RNNs; and (iii) features extracted using pre-trained RNN are generic enough and perform better than typical statistical hand-crafted features.',\n",
       "  'Title: computing the metric dimension by decomposing graphs into extended biconnected components\\nAbstract: A vertex set $U \\\\subseteq V$ of an undirected graph $G=(V,E)$ is a $\\\\textit{resolving set}$ for $G$, if for every two distinct vertices $u,v \\\\in V$ there is a vertex $w \\\\in U$ such that the distances between $u$ and $w$ and the distance between $v$ and $w$ are different. The $\\\\textit{Metric Dimension}$ of $G$ is the size of a smallest resolving set for $G$. Deciding whether a given graph $G$ has Metric Dimension at most $k$ for some integer $k$ is well-known to be NP-complete. Many research has been done to understand the complexity of this problem on restricted graph classes. In this paper, we decompose a graph into its so called $\\\\textit{extended biconnected components}$ and present an efficient algorithm for computing the metric dimension for a class of graphs having a minimum resolving set with a bounded number of vertices in every extended biconnected component. Further we show that the decision problem METRIC DIMENSION remains NP-complete when the above limitation is extended to usual biconnected components.',\n",
       "  'Title: asymptotic analysis of spatial coupling coding for compute and forward relaying\\nAbstract: Compute-and-forward (CAF) relaying is effective to increase bandwidth efficiency of wireless two-way relay channels. In a CAF scheme, a relay is designed to decode a linear combination composed of transmitted messages from other terminals or relays. Design for error-correcting codes and its decoding algorithms suitable for CAF relaying schemes remain as an important issue to be studied. As described in this paper, we will present an asymptotic performance analysis of LDPC codes over two-way relay channels based on density evolution (DE). Because of the asymmetric characteristics of the channel, we use the population dynamics DE combined with DE formulas for asymmetric channels to obtain BP thresholds. Additionally, we also evaluate the asymptotic performance of spatially coupled LDPC codes for two-way relay channels. The results indicate that the spatial coupling codes yield improvements in the BP threshold compared with corresponding uncoupled codes for two-way relay channels. Finally, we will compare the mutual information rate and rate achievability between the CAF scheme and the MAC separation decoding scheme. We demonstrate the possibility that the CAF scheme has higher reliability in the high-rate region.',\n",
       "  \"Title: deepmove learning place representations through large scale movement data\\nAbstract: Understanding and reasoning about places and their relationships are critical for many applications. Places are traditionally curated by a small group of people as place gazetteers and are represented by an ID with spatial extent, category, and other descriptions. However, a place context is described to a large extent by movements made from/to other places. Places are linked and related to each other by these movements. This important context is missing from the traditional representation. #R##N#We present DeepMove, a novel approach for learning latent representations of places. DeepMove advances the current deep learning based place representations by directly model movements between places. We demonstrate DeepMove's latent representations on place categorization and clustering tasks on large place and movement datasets with respect to important parameters. Our results show that DeepMove outperforms state-of-the-art baselines. DeepMove's representations can provide up to 15% higher than competing methods in matching rate of place category and result in up to 39% higher silhouette coefficient value for place clusters. #R##N#DeepMove is spatial and temporal context aware. It is scalable. It outperforms competing models using much smaller training dataset (a month or 1/12 of data). These qualities make it suitable for a broad class of real-world applications.\",\n",
       "  'Title: derivation of the closed form ber expressions for dl noma over nakagami m fading channels\\nAbstract: NOMA is as a strong candidate for the Future Radio Access Network (FRA) due to its potential to support massive connectivity and high spectral efficiency. However, the most important drawback of NOMA is the error during Successive Interference Canceller (SIC) is implemented because of the inter-user interferences. In this paper, we derive closed-form exact Bit-Error Rate expressions for Downlink(DL) NOMA over Nakagami-m fading channels in the presence of SIC errors. The derived expressions are validated by the computer simulations. It is shown that the m parameter still represents the diversity order like as OMA systems. Besides, the BER performances of users for NOMA have substantially depended on the power allocation coefficient.',\n",
       "  'Title: optimization of battery energy storage to improve power system oscillation damping\\nAbstract: This paper studies the optimization of both the placement and controller parameters for Battery Energy Storage Systems (BESSs) to improve power system oscillation damping. For each BESS, dynamic power output characteristics of the power converter interface are modeled considering the power limit, State of Charge limit, and time constant. Then, a black-box mixed-integer optimization problem is formulated and tackled by interfacing time-domain simulation with a mixed-integer Particle Swarm Optimization algorithm. The proposed optimization approach is demonstrated on the New England 39-bus system and a Nordic test system. The optimal results are also verified by time-domain simulation. To improve the applicability and efficiency of the proposed method, seasonal load changes and the minimum number of BESS units to be placed are considered. The proposed controller is also compared to other controllers to validate its performance.',\n",
       "  'Title: dla compiler and fpga overlay for neural network inference acceleration\\nAbstract: Overlays have shown significant promise for field-programmable gate-arrays (FPGAs) as they allow for fast development cycles and remove many of the challenges of the traditional FPGA hardware design flow. However, this often comes with a significant performance burden resulting in very little adoption of overlays for practical applications. In this paper, we tailor an overlay to a specific application domain, and we show how we maintain its full programmability without paying for the performance overhead traditionally associated with overlays. Specifically, we introduce an overlay targeted for deep neural network inference with only ~1% overhead to support the control and reprogramming logic using a lightweight very-long instruction word (VLIW) network. Additionally, we implement a sophisticated domain specific graph compiler that compiles deep learning languages such as Caffe or Tensorflow to easily target our overlay. We show how our graph compiler performs architecture-driven software optimizations to significantly boost performance of both convolutional and recurrent neural networks (CNNs/RNNs) - we demonstrate a 3x improvement on ResNet-101 and a 12x improvement for long short-term memory (LSTM) cells, compared to naive implementations. Finally, we describe how we can tailor our hardware overlay, and use our graph compiler to achieve ~900 fps on GoogLeNet on an Intel Arria 10 1150 - the fastest ever reported on comparable FPGAs.',\n",
       "  'Title: gated fusion network for joint image deblurring and super resolution\\nAbstract: Single-image super-resolution is a fundamental task for vision applications to enhance the image quality with respect to spatial resolution. If the input image contains degraded pixels, the artifacts caused by the degradation could be amplified by super-resolution methods. Image blur is a common degradation source. Images captured by moving or still cameras are inevitably affected by motion blur due to relative movements between sensors and objects. In this work, we focus on the super-resolution task with the presence of motion blur. We propose a deep gated fusion convolution neural network to generate a clear high-resolution frame from a single natural image with severe blur. By decomposing the feature extraction step into two task-independent streams, the dual-branch design can facilitate the training process by avoiding learning the mixed degradation all-in-one and thus enhance the final high-resolution prediction results. Extensive experiments demonstrate that our method generates sharper super-resolved images from low-resolution inputs with high computational efficiency.',\n",
       "  'Title: visualizing convolutional networks for mri based diagnosis of alzheimer s disease\\nAbstract: Visualizing and interpreting convolutional neural networks (CNNs) is an important task to increase trust in automatic medical decision making systems. In this study, we train a 3D CNN to detect Alzheimer’s disease based on structural MRI scans of the brain. Then, we apply four different gradient-based and occlusion-based visualization methods that explain the network’s classification decisions by highlighting relevant areas in the input image. We compare the methods qualitatively and quantitatively. We find that all four methods focus on brain regions known to be involved in Alzheimer’s disease, such as inferior and middle temporal gyrus. While the occlusion-based methods focus more on specific regions, the gradient-based methods pick up distributed relevance patterns. Additionally, we find that the distribution of relevance varies across patients, with some having a stronger focus on the temporal lobe, whereas for others more cortical areas are relevant. In summary, we show that applying different visualization methods is important to understand the decisions of a CNN, a step that is crucial to increase clinical impact and trust in computer-based decision support systems.',\n",
       "  'Title: parallelization does not accelerate convex optimization adaptivity lower bounds for non smooth convex minimization\\nAbstract: In this paper we study the limitations of parallelization in convex optimization. A convenient approach to study parallelization is through the prism of \\\\emph{adaptivity} which is an information theoretic measure of the parallel runtime of an algorithm. Informally, adaptivity is the number of sequential rounds an algorithm needs to make when it can execute polynomially-many queries in parallel at every round. For combinatorial optimization with black-box oracle access, the study of adaptivity has recently led to exponential accelerations in parallel runtime and the natural question is whether dramatic accelerations are achievable for convex optimization. #R##N#Our main result is a spoiler. We show that, in general, parallelization does not accelerate convex optimization. In particular, for the problem of minimizing a non-smooth Lipschitz and strongly convex function with black-box oracle access we give information theoretic lower bounds that indicate that the number of adaptive rounds of any randomized algorithm exactly match the upper bounds of single-query-per-round (i.e. non-parallel) algorithms.',\n",
       "  'Title: cognitive techniques for early detection of cybersecurity events\\nAbstract: The early detection of cybersecurity events such as attacks is challenging given the constantly evolving threat landscape. Even with advanced monitoring, sophisticated attackers can spend as many as 146 days in a system before being detected. This paper describes a novel, cognitive framework that assists a security analyst by exploiting the power of semantically rich knowledge representation and reasoning with machine learning techniques. Our Cognitive Cybersecurity system ingests information from textual sources, and various agents representing host and network-based sensors, and represents this information in a knowledge graph. This graph uses terms from an extended version of the Unified Cybersecurity Ontology. The system reasons over the knowledge graph to derive better actionable intelligence to security administrators, thus decreasing their cognitive load and increasing their confidence in the system. We have developed a proof of concept framework for our approach and demonstrate its capabilities using a custom-built ransomware instance that is similar to WannaCry.',\n",
       "  'Title: progressive operational perceptron with memory\\nAbstract: Generalized Operational Perceptron (GOP) was proposed to generalize the linear neuron model in the traditional Multilayer Perceptron (MLP) and this model can mimic the synaptic connections of the biological neurons that have nonlinear neurochemical behaviours. Progressive Operational Perceptron (POP) is a multilayer network composing of GOPs which is formed layer-wise progressively. In this work, we propose major modifications that can accelerate as well as augment the progressive learning procedure of POP by incorporating an information-preserving, linear projection path from the input to the output layer at each progressive step. The proposed extensions can be interpreted as a mechanism that provides direct information extracted from the previously learned layers to the network, hence the term \"memory\". This allows the network to learn deeper architectures with better data representations. An extensive set of experiments show that the proposed modifications can surpass the learning capability of the original POPs and other related algorithms.',\n",
       "  'Title: quantified markov logic networks\\nAbstract: Markov Logic Networks (MLNs) are well-suited for expressing statistics such as \"with high probability a smoker knows another smoker\" but not for expressing statements such as \"there is a smoker who knows most other smokers\", which is necessary for modeling, e.g. influencers in social networks. To overcome this shortcoming, we study quantified MLNs which generalize MLNs by introducing statistical universal quantifiers, allowing to express also the latter type of statistics in a principled way. Our main technical contribution is to show that the standard reasoning tasks in quantified MLNs, maximum a posteriori and marginal inference, can be reduced to their respective MLN counterparts in polynomial time.',\n",
       "  'Title: towards a new extracting and querying approach of fuzzy summaries\\nAbstract: Diversification of DB applications highlighted the limitations of relational database management system (RDBMS) particularly on the modeling plan. In fact, in the real world, we are increasingly faced with the situation where applications need to handle imprecise data and to offer a flexible querying to their users. Several theoretical solutions have been proposed. However, the impact of this work in practice remained negligible with the exception of a few research prototypes based on the formal model GEFRED. In this chapter, the authors propose a new approach for exploitation of fuzzy relational databases (FRDB) described by the model GEFRED. This approach consists of 1) a new technique for extracting summary fuzzy data, Fuzzy SAINTETIQ, based on the classification of fuzzy data and formal concepts analysis; 2) an approach of assessing flexible queries in the context of FDB based on the set of fuzzy summaries generated by our fuzzy SAINTETIQ system; 3) an approach of repairing and substituting unanswered query.',\n",
       "  'Title: video to video synthesis\\nAbstract: We study the problem of video-to-video synthesis, whose goal is to learn a mapping function from an input source video (e.g., a sequence of semantic segmentation masks) to an output photorealistic video that precisely depicts the content of the source video. While its image counterpart, the image-to-image synthesis problem, is a popular topic, the video-to-video synthesis problem is less explored in the literature. Without understanding temporal dynamics, directly applying existing image synthesis approaches to an input video often results in temporally incoherent videos of low visual quality. In this paper, we propose a novel video-to-video synthesis approach under the generative adversarial learning framework. Through carefully-designed generator and discriminator architectures, coupled with a spatio-temporal adversarial objective, we achieve high-resolution, photorealistic, temporally coherent video results on a diverse set of input formats including segmentation masks, sketches, and poses. Experiments on multiple benchmarks show the advantage of our method compared to strong baselines. In particular, our model is capable of synthesizing 2K resolution videos of street scenes up to 30 seconds long, which significantly advances the state-of-the-art of video synthesis. Finally, we apply our approach to future video prediction, outperforming several state-of-the-art competing systems.',\n",
       "  'Title: application of end to end deep learning in wireless communications systems\\nAbstract: Deep learning is a potential paradigm changer for the design of wireless communications systems (WCS), from conventional handcrafted schemes based on sophisticated mathematical models with assumptions to autonomous schemes based on the end-to-end deep learning using a large number of data. In this article, we present a basic concept of the deep learning and its application to WCS by investigating the resource allocation (RA) scheme based on a deep neural network (DNN) where multiple goals with various constraints can be satisfied through the end-to-end deep learning. Especially, the optimality and feasibility of the DNN based RA are verified through simulation. Then, we discuss the technical challenges regarding the application of deep learning in WCS.',\n",
       "  'Title: reasoning with justifiable exceptions in contextual hierarchies appendix\\nAbstract: This paper is an appendix to the paper \"Reasoning with Justifiable Exceptions in Contextual Hierarchies\" by Bozzato, Serafini and Eiter, 2018. It provides further details on the language, the complexity results and the datalog translation introduced in the main paper.',\n",
       "  'Title: estimating failure in brittle materials using graph theory\\nAbstract: In brittle fracture applications, failure paths, regions where the failure occurs and damage statistics, are some of the key quantities of interest (QoI). High-fidelity models for brittle failure that accurately predict these QoI exist but are highly computationally intensive, making them infeasible to incorporate in upscaling and uncertainty quantification frameworks. The goal of this paper is to provide a fast heuristic to reasonably estimate quantities such as failure path and damage in the process of brittle failure. Towards this goal, we first present a method to predict failure paths under tensile loading conditions and low-strain rates. The method uses a $k$-nearest neighbors algorithm built on fracture process zone theory, and identifies the set of all possible pre-existing cracks that are likely to join early to form a large crack. The method then identifies zone of failure and failure paths using weighted graphs algorithms. We compare these failure paths to those computed with a high-fidelity model called the Hybrid Optimization Software Simulation Suite (HOSS). A probabilistic evolution model for average damage in a system is also developed that is trained using 150 HOSS simulations and tested on 40 simulations. A non-parametric approach based on confidence intervals is used to determine the damage evolution over time along the dominant failure path. For upscaling, damage is the key QoI needed as an input by the continuum models. This needs to be informed accurately by the surrogate models for calculating effective modulii at continuum-scale. We show that for the proposed average damage evolution model, the prediction accuracy on the test data is more than 90\\\\%. In terms of the computational time, the proposed models are $\\\\approx \\\\mathcal{O}(10^6)$ times faster compared to high-fidelity HOSS.',\n",
       "  'Title: towards audio to scene image synthesis using generative adversarial network\\nAbstract: Humans can imagine a scene from a sound. We want machines to do so by using conditional generative adversarial networks (GANs). By applying the techniques including spectral norm, projection discriminator and auxiliary classifier, compared with naive conditional GAN, the model can generate images with better quality in terms of both subjective and objective evaluations. Almost three-fourth of people agree that our model have the ability to generate images related to sounds. By inputting different volumes of the same sound, our model output different scales of changes based on the volumes, showing that our model truly knows the relationship between sounds and images to some extent.',\n",
       "  'Title: unsupervised learning of foreground object segmentation\\nAbstract: Unsupervised learning represents one of the most interesting challenges in computer vision today. The task has an immense practical value with many applications in artificial intelligence and emerging technologies, as large quantities of unlabeled images and videos can be collected at low cost. In this paper, we address the unsupervised learning problem in the context of segmenting the main foreground objects in single images. We propose an unsupervised learning system, which has two pathways, the teacher and the student, respectively. The system is designed to learn over several generations of teachers and students. At every generation the teacher performs unsupervised object discovery in videos or collections of images and an automatic selection module picks up good frame segmentations and passes them to the student pathway for training. At every generation multiple students are trained, with different deep network architectures to ensure a better diversity. The students at one iteration help in training a better selection module, forming together a more powerful teacher pathway at the next iteration. In experiments, we show that the improvement in the selection power, the training of multiple students and the increase in unlabeled data significantly improve segmentation accuracy from one generation to the next. Our method achieves top results on three current datasets for object discovery in video, unsupervised image segmentation and saliency detection. At test time, the proposed system is fast, being one to two orders of magnitude faster than published unsupervised methods. We also test the strength of our unsupervised features within a well known transfer learning setup and achieve competitive performance, proving that our unsupervised approach can be reliably used in a variety of computer vision tasks.',\n",
       "  'Title: deepmag source specific motion magnification using gradient ascent\\nAbstract: Many important physical phenomena involve subtle signals that are difficult to observe with the unaided eye, yet visualizing them can be very informative. Current motion magnification techniques can reveal these small temporal variations in video, but require precise prior knowledge about the target signal, and cannot deal with interference motions at a similar frequency. We present DeepMag an end-to-end deep neural video-processing framework based on gradient ascent that enables automated magnification of subtle color and motion signals from a specific source, even in the presence of large motions of various velocities. While the approach is generalizable, the advantages of DeepMag are highlighted via the task of video-based physiological visualization. Through systematic quantitative and qualitative evaluation of the approach on videos with different levels of head motion, we compare the magnification of pulse and respiration to existing state-of-the-art methods. Our method produces magnified videos with substantially fewer artifacts and blurring whilst magnifying the physiological changes by a similar degree.',\n",
       "  'Title: who is really affected by fraudulent reviews an analysis of shilling attacks on recommender systems in real world scenarios\\nAbstract: We present the results of an initial analysis conducted on a real-life setting to quantify the effect of shilling attacks on recommender systems. We focus on both algorithm performance as well as the types of users who are most affected by these attacks.',\n",
       "  'Title: stdp learning of image patches with convolutional spiking neural networks\\nAbstract: Spiking neural networks are motivated from principles of neural systems and may possess unexplored advantages in the context of machine learning. A class of \\\\textit{convolutional spiking neural networks} is introduced, trained to detect image features with an unsupervised, competitive learning mechanism. Image features can be shared within subpopulations of neurons, or each may evolve independently to capture different features in different regions of input space. We analyze the time and memory requirements of learning with and operating such networks. The MNIST dataset is used as an experimental testbed, and comparisons are made between the performance and convergence speed of a baseline spiking neural network.',\n",
       "  'Title: rigid body dynamic simulation with multiple convex contact patches\\nAbstract: We present a principled method for dynamic simulation of rigid bodies in intermittent contact with each other where the contact is assumed to be a non-convex contact patch that can be modeled as a union of convex patches. The prevalent assumption in simulating rigid bodies undergoing intermittent contact with each other is that the contact is a point contact. In recent work, we introduced an approach to simulate contacting rigid bodies with convex contact patches (line and surface contact). In this paper, for non-convex contact patches modeled as a union of convex patches, we formulate a discrete-time mixed complementarity problem where we solve the contact detection and integration of the equations of motion simultaneously. Thus, our method is a geometrically-implicit method and we prove that in our formulation, there is no artificial penetration between the contacting rigid bodies. We solve for the equivalent contact point (ECP) and contact impulse of each contact patch simultaneously along with the state, i.e., configuration and velocity of the objects. We provide empirical evidence to show that if the number of contact patches between two objects is less than or equal to three, the state evolution of the bodies is unique, although the contact impulses and ECP may not be unique. We also present simulation results showing that our method can seamlessly capture transition between different contact modes like non-convex patch to point (or line contact) and vice-versa during simulation.',\n",
       "  'Title: a simplicial complex model for dynamic epistemic logic to study distributed task computability\\nAbstract: The usual epistemic model S5n for a multi-agent system is based on a Kripke frame, which is a graph whose edges are labeled with agents that do not distinguish between two states. We propose to uncover the higher dimensional information implicit in this structure, by considering a dual, simplicial complex model. We use dynamic epistemic logic (DEL) to study how an epistemic simplicial complex model changes after a set of agents communicate with each other. We concentrate on an action model that represents the so called immediate snapshot communication patterns of asynchronous agents, because it is central to distributed computability (but our setting works for other communication patterns). There are topological invariants preserved from the initial epistemic complex to the one after the action model is applied, which determine the knowledge that the agents gain after communication. Finally, we describe how a distributed task specification can be modeled as a DEL action model, and show that the topological invariants determine whether the task is solvable. We thus provide a bridge between DEL and the topological theory of distributed computability, which studies task solvability in a shared memory or message passing architecture.',\n",
       "  'Title: data augmentation for neural online chat response selection\\nAbstract: Data augmentation seeks to manipulate the available data for training to improve the generalization ability of models. We investigate two data augmentation proxies, permutation and flipping, for neural dialog response selection task on various models over multiple datasets, including both Chinese and English languages. Different from standard data augmentation techniques, our method combines the original and synthesized data for prediction. Empirical results show that our approach can gain 1 to 3 recall-at-1 points over baseline models in both full-scale and small-scale settings.',\n",
       "  'Title: machine learning for semi linear pdes\\nAbstract: Recent machine learning algorithms dedicated to solving semi-linear PDEs are improved by using different neural network architectures and different parameterizations. These algorithms are compared to a new one that solves a fixed point problem by using deep learning techniques. This new algorithm appears to be competitive in terms of accuracy with the best existing algorithms.',\n",
       "  'Title: an energy based analysis of reduced order models of networked synchronous machines\\nAbstract: Stability of power networks is an increasingly important topic because of the high penetration of renewable distributed generation units. This requires the development of advanced (typically model-based) techniques for the analysis and controller design of power networks. Although there are widely accepted reduced-order models to describe the dynamic behavior of power networks, they are commonly presented without details about the reduction procedure, hampering the understanding of the physical phenomena behind them. The present paper aims to provide a modular model derivation of multi-machine power networks. Starting from first-principle fundamental physics, we present detailed dynamical models of synchronous machines and clearly state the underlying assumptions which lead to some of the standard reduced-order multi-machine models, including the classical second-order swing equations. In addition, the energy functions for the reduced-order multi-machine models are derived, which allows to represent the multi-machine systems as port-Hamiltonian systems. Moreover, the systems are proven to be passive with respect to its steady states, which permits for a power-preserving interconnection with other passive components, including passive controllers. As a result, the corresponding energy function or Hamiltonian can be used to provide a rigorous stability analysis of advanced models for the power network without having to linearize the system.',\n",
       "  'Title: xml navigation and transformation by tree walking automata and transducers with visible and invisible pebbles\\nAbstract: The pebble tree automaton and the pebble tree transducer are enhanced by additionally allowing an unbounded number of \"invisible\" pebbles (as opposed to the usual \"visible\" ones). The resulting pebble tree automata recognize the regular tree languages (i.e., can validate all generalized DTD\\'s) and hence can find all matches of MSO definable patterns. Moreover, when viewed as a navigational device, they lead to an XPath-like formalism that has a path expression for every MSO definable binary pattern. The resulting pebble tree transducers can apply arbitrary MSO definable tests to (the observable part of) their configurations, they (still) have a decidable typechecking problem, and they can model the recursion mechanism of XSLT. The time complexity of the typechecking problem for conjunctive queries that use MSO definable patterns can often be reduced through the use of invisible pebbles.',\n",
       "  'Title: multi context deep network for angle closure glaucoma screening in anterior segment oct\\nAbstract: A major cause of irreversible visual impairment is angle-closure glaucoma, which can be screened through imagery from Anterior Segment Optical Coherence Tomography (AS-OCT). Previous computational diagnostic techniques address this screening problem by extracting specific clinical measurements or handcrafted visual features from the images for classification. In this paper, we instead propose to learn from training data a discriminative representation that may capture subtle visual cues not modeled by predefined features. Based on clinical priors, we formulate this learning with a presented Multi-Context Deep Network (MCDN) architecture, in which parallel Convolutional Neural Networks are applied to particular image regions and at corresponding scales known to be informative for clinically diagnosing angle-closure glaucoma. The output feature maps of the parallel streams are merged into a classification layer to produce the deep screening result. Moreover, we incorporate estimated clinical parameters to further enhance performance. On a clinical AS-OCT dataset, our system is validated through comparisons to previous screening methods.',\n",
       "  \"Title: reinforcement learning in topology based representation for human body movement with whole arm manipulation\\nAbstract: Moving a human body or a large and bulky object can require the strength of whole arm manipulation (WAM). This type of manipulation places the load on the robot's arms and relies on global properties of the interaction to succeed---rather than local contacts such as grasping or non-prehensile pushing. In this paper, we learn to generate motions that enable WAM for holding and transporting of humans in certain rescue or patient care scenarios. We model the task as a reinforcement learning problem in order to provide a behavior that can directly respond to external perturbation and human motion. For this, we represent global properties of the robot-human interaction with topology-based coordinates that are computed from arm and torso positions. These coordinates also allow transferring the learned policy to other body shapes and sizes. For training and evaluation, we simulate a dynamic sea rescue scenario and show in quantitative experiments that the policy can solve unseen scenarios with differently-shaped humans, floating humans, or with perception noise. Our qualitative experiments show the subsequent transporting after holding is achieved and we demonstrate that the policy can be directly transferred to a real world setting.\",\n",
       "  'Title: deep compressive autoencoder for action potential compression in large scale neural recording\\nAbstract: Understanding the coordinated activity underlying brain computations requires large-scale, simultaneous recordings from distributed neuronal structures at a cellular-level resolution. One major hurdle to design high-bandwidth, high-precision, large-scale neural interfaces lies in the formidable data streams that are generated by the recorder chip and need to be online transferred to a remote computer. The data rates can require hundreds to thousands of I/O pads on the recorder chip and power consumption on the order of Watts for data streaming alone. We developed a deep learning-based compression model to reduce the data rate of multichannel action potentials. The proposed model is built upon a deep compressive autoencoder (CAE) with discrete latent embeddings. The encoder is equipped with residual transformations to extract representative features from spikes, which are mapped into the latent embedding space and updated via vector quantization (VQ). The decoder network reconstructs spike waveforms from the quantized latent embeddings. Experimental results show that the proposed model consistently outperforms conventional methods by achieving much higher compression ratios (20-500x) and better or comparable reconstruction accuracies. Testing results also indicate that CAE is robust against a diverse range of imperfections, such as waveform variation and spike misalignment, and has minor influence on spike sorting accuracy. Furthermore, we have estimated the hardware cost and real-time performance of CAE and shown that it could support thousands of recording channels simultaneously without excessive power/heat dissipation. The proposed model can reduce the required data transmission bandwidth in large-scale recording experiments and maintain good signal qualities. The code of this work has been made available at this https URL',\n",
       "  'Title: i know what you want semantic learning for text comprehension\\nAbstract: Who did what to whom is a major focus in natural language understanding, which is right the aim of semantic role labeling (SRL). Although SRL is naturally essential to text comprehension tasks, it is surprisingly ignored in previous work. This paper thus makes the first attempt to let SRL enhance text comprehension and inference through specifying verbal arguments and their corresponding semantic roles. In terms of deep learning models, our embeddings are enhanced by semantic role labels for more fine-grained semantics. We show that the salient labels can be conveniently added to existing models and significantly improve deep learning models in challenging text comprehension tasks. Extensive experiments on benchmark machine reading comprehension and inference datasets verify that the proposed semantic learning helps our system reach new state-of-the-art.',\n",
       "  'Title: unsupervised cross lingual matching of product classifications\\nAbstract: Unsupervised cross-lingual embeddings mapping has provided a unique tool for completely unsupervised translation even for languages with different scripts. In this work we use this method for the task of unsupervised cross-lingual matching of product classifications. Our work also investigates limitations of unsupervised vector alignment and we also suggest two other techniques for aligning product classifications based on their descriptions: using hierarchical information and translations.',\n",
       "  'Title: decentralized search on decentralized web\\nAbstract: Decentralized Web, or DWeb, is envisioned as a promising future of the Web. Being decentralized, there are no dedicated web servers in DWeb; Devices that retrieve web contents also serve their cached data to peer devices with straight privacy-preserving mechanisms. The fact that contents in DWeb are distributed, replicated, and decentralized lead to a number of key advantages over the conventional web. These include better resiliency against network partitioning and distributed-denial-of-service attacks (DDoS), and better browsing experiences in terms of shorter latency and higher throughput. Moreover, DWeb provides tamper-proof contents because each content piece is uniquely identified by a cryptographic hash. DWeb also clicks well with future Internet architectures, such as Named Data Networking (NDN).Search engines have been an inseparable element of the Web. Contemporary (\"Web 2.0\") search engines, however, provide centralized services. They are thus subject to DDoS attacks, insider threat, and ethical issues like search bias and censorship. As the web moves from being centralized to being decentralized, search engines ought to follow. We propose QueenBee, a decentralized search engine for DWeb. QueenBee is so named because worker bees and honeycomb are a common metaphor for distributed architectures, with the queen being the one that holds the colony together. QueenBee aims to revolutionize the search engine business model by offering incentives to both content providers and peers that participate in QueenBee\\'s page indexing and ranking operations.',\n",
       "  'Title: deeplaser practical fault attack on deep neural networks\\nAbstract: As deep learning systems are widely adopted in safety- and security-critical applications, such as autonomous vehicles, banking systems, etc., malicious faults and attacks become a tremendous concern, which potentially could lead to catastrophic consequences. In this paper, we initiate the first study of leveraging physical fault injection attacks on Deep Neural Networks (DNNs), by using laser injection technique on embedded systems. In particular, our exploratory study targets four widely used activation functions in DNNs development, that are the general main building block of DNNs that creates non-linear behaviors -- ReLu, softmax, sigmoid, and tanh. Our results show that by targeting these functions, it is possible to achieve a misclassification by injecting faults into the hidden layer of the network. Such result can have practical implications for real-world applications, where faults can be introduced by simpler means (such as altering the supply voltage).',\n",
       "  'Title: temporal cliques admit sparse spanners\\nAbstract: Let ${\\\\cal G}=(G,\\\\lambda)$ be a labeled graph on $n$ vertices with $\\\\lambda:E_G\\\\to \\\\mathbb{N}$ a locally injective mapping that assigns to every edge a single integer label. The label is seen as a discrete time when the edge is present. This graph is {\\\\em temporally connected} if a path exists with increasing labels from every vertex to every other vertex. In a seminal paper, Kempe, Kleinberg, and Kumar (JCSS 2002) asked whether, given such a labeled graph, a {\\\\em sparse} subset of edges can always be found that preserves temporal connectivity if the other edges are removed -- we call such subsets {\\\\em temporal spanners}. Recently, Axiotis and Fotakis (ICALP 2016) answered negatively, exhibiting a family of minimally connected temporal graphs with $\\\\Omega(n^2)$ edges. The natural question then becomes whether sparse spanners can be found in specific classes of dense graphs. #R##N#In this article, we settle the question {\\\\em positively} for complete graphs, showing that one can always remove all but $o(n^2)$ edges, whatever the labels, while preserving temporal connectivity. The best approach so far led to removing only $O(n)$ edges, leaving the asymptotic density of the graph unchanged (Akrida et al., ToCS 2017). We start by observing that the same argument can be generalized to removing $O(n^2)$ edges (a sixth of the edges). Then, using a completely different approach, we establish a gradual set of results, showing that a quarter of the edges can be removed, then half of the edges, and eventually {\\\\em all} but $O(n \\\\log n)$ edges. This result is robust in the sense that it extends, under mild assumptions, to more general models of temporal cliques where the labels may not be locally unique and a same edge may have several labels. The main open question is now to understand where the separation occurs between graphs that admit sparse spanners and graphs that do not.',\n",
       "  'Title: learning to coordinate multiple reinforcement learning agents for diverse query reformulation\\nAbstract: We propose a method to efficiently learn diverse strategies in reinforcement learning for query reformulation in the tasks of document retrieval and question answering. In the proposed framework an agent consists of multiple specialized sub-agents and a meta-agent that learns to aggregate the answers from sub-agents to produce a final answer. Sub-agents are trained on disjoint partitions of the training data, while the meta-agent is trained on the full training set. Our method makes learning faster, because it is highly parallelizable, and has better generalization performance than strong baselines, such as an ensemble of agents trained on the full data. We show that the improved performance is due to the increased diversity of reformulation strategies.',\n",
       "  'Title: joint activity detection and channel estimation for iot networks phase transition and computation estimation tradeoff\\nAbstract: Massive device connectivity is a crucial communication challenge for Internet of Things (IoT) networks, which consist of a large number of devices with sporadic traffic. In each coherence block, the serving base station needs to identify the active devices and estimate their channel state information for effective communication. By exploiting the sparsity pattern of data transmission, we develop a structured group sparsity estimation method to simultaneously detect the active devices and estimate the corresponding channels. This method significantly reduces the signature sequence length while supporting massive IoT access. To determine the optimal signature sequence length, we study \\\\emph{the phase transition behavior} of the group sparsity estimation problem. Specifically, user activity can be successfully estimated with a high probability when the signature sequence length exceeds a threshold; otherwise, it fails with a high probability. The location and width of the phase transition region are characterized via the theory of conic integral geometry. We further develop a smoothing method to solve the high-dimensional structured estimation problem with a given limited time budget. This is achieved by sharply characterizing the convergence rate in terms of the smoothing parameter, signature sequence length and estimation accuracy, yielding a trade-off between the estimation accuracy and computational cost. Numerical results are provided to illustrate the accuracy of our theoretical results and the benefits of smoothing techniques.',\n",
       "  'Title: operations on partial orders\\nAbstract: We define analogues of Boolean operations on not necessarily complete partial orders, they often have as results sets of elements rather than single elements. It proves useful to add to such sets X if they are intended to be sup(X) or inf(X), even if sup and inf do not always exist. #R##N#We then define the height of an element as the maximal length of chains going from BOTTOM to that element, and use height to define probability measures.',\n",
       "  'Title: learning without interaction requires separation\\nAbstract: One of the key resources in large-scale learning systems is the number of rounds of communication between the server and the clients holding the data points. We study this resource for systems with two types of constraints on the communication from each of the clients: local differential privacy and limited number of bits communicated. For both models the number of rounds of communications is captured by the number of rounds of interaction when solving the learning problem in the statistical query (SQ) model. For many learning problems known efficient algorithms require many rounds of interaction. Yet little is known on whether this is actually necessary. In the context of classification in the PAC learning model, Kasiviswanathan et al. (2008) constructed an artificial class of functions that is PAC learnable with respect to a fixed distribution but cannot be learned by an efficient non-interactive (or one-round) SQ algorithm. Here we show that a similar separation holds for learning linear separators and decision lists without assumptions on the distribution. To prove this separation we show that non-interactive SQ algorithms can only learn function classes of low margin complexity, that is classes of functions that can be represented as large-margin linear separators.',\n",
       "  'Title: cello 3d estimating the covariance of icp in the real world\\nAbstract: The fusion of Iterative Closest Point (ICP) reg- istrations in existing state estimation frameworks relies on an accurate estimation of their uncertainty. In this paper, we study the estimation of this uncertainty in the form of a covariance. First, we scrutinize the limitations of existing closed-form covariance estimation algorithms over 3D datasets. Then, we set out to estimate the covariance of ICP registrations through a data-driven approach, with over 5 100 000 registrations on 1020 pairs from real 3D point clouds. We assess our solution upon a wide spectrum of environments, ranging from structured to unstructured and indoor to outdoor. The capacity of our algorithm to predict covariances is accurately assessed, as well as the usefulness of these estimations for uncertainty estimation over trajectories. The proposed method estimates covariances better than existing closed-form solutions, and makes predictions that are consistent with observed trajectories.',\n",
       "  'Title: learning depth with convolutional spatial propagation network\\nAbstract: Depth prediction is one of the fundamental problems in computer vision. In this paper, we propose a simple yet effective convolutional spatial propagation network (CSPN) to learn the affinity matrix for various depth estimation tasks. Specifically, it is an efficient linear propagation model, in which the propagation is performed with a manner of recurrent convolutional operation, and the affinity among neighboring pixels is learned through a deep convolutional neural network (CNN). We can append this module to any output from a state-of-the-art (SOTA) depth estimation networks to improve their performances. In practice, we further extend CSPN in two aspects: 1) take sparse depth map as additional input, which is useful for the task of depth completion; 2) similar to commonly used 3D convolution operation in CNNs, we propose 3D CSPN to handle features with one additional dimension, which is effective in the task of stereo matching using 3D cost volume. For the tasks of sparse to dense, a.k.a depth completion. We experimented the proposed CPSN conjunct algorithms over the popular NYU v2 and KITTI datasets, where we show that our proposed algorithms not only produce high quality (e.g., 30% more reduction in depth error), but also run faster (e.g., 2 to 5x faster) than previous SOTA spatial propagation network. We also evaluated our stereo matching algorithm on the Scene Flow and KITTI Stereo datasets, and rank 1st on both the KITTI Stereo 2012 and 2015 benchmarks, which demonstrates the effectiveness of the proposed module. The code of CSPN proposed in this work will be released at this https URL.',\n",
       "  'Title: adversarial examples a complete characterisation of the phenomenon\\nAbstract: We provide a complete characterisation of the phenomenon of adversarial examples - inputs intentionally crafted to fool machine learning models. We aim to cover all the important concerns in this field of study: (1) the conjectures on the existence of adversarial examples, (2) the security, safety and robustness implications, (3) the methods used to generate and (4) protect against adversarial examples and (5) the ability of adversarial examples to transfer between different machine learning models. We provide ample background information in an effort to make this document self-contained. Therefore, this document can be used as survey, tutorial or as a catalog of attacks and defences using adversarial examples.',\n",
       "  \"Title: corrections to wyner s common information under r enyi divergence measures\\nAbstract: In this correspondence, we correct an erroneous argument in the proof of Theorem 1 of the paper above, which is a statement generalizing that for Wyner's common information.\",\n",
       "  'Title: indirect mechanism design for efficient and stable renewable energy aggregation\\nAbstract: Mechanism design is studied for aggregating renewable power producers (RPPs) in a two-settlement power market. Employing an indirect mechanism design framework, a payoff allocation mechanism (PAM) is derived from the competitive equilibrium (CE) of a specially formulated market with transferrable payoff. Given the designed mechanism, the strategic behaviors of the participating RPPs entail a non-cooperative game: It is proven that a unique pure Nash equilibrium (NE) exists among the RPPs, for which a closed-form expression is found. Moreover, it is proven that the designed mechanism achieves a number of key desirable properties at the NE: these include efficiency (i.e., an ideal \"Price of Anarchy\" of one), stability (i.e., \"in the core\" from a coalitional game theoretic perspective), and no collusion. In addition, it is shown that a set of desirable \"ex-post\" properties are also achieved by the designed mechanism. Extensive simulations are conducted and corroborate the theoretical results.',\n",
       "  \"Title: neural variational hybrid collaborative filtering\\nAbstract: Collaborative Filtering (CF) is one of the most used methods for Recommender System. Because of the Bayesian nature and nonlinearity, deep generative models, e.g. Variational Autoencoder (VAE), have been applied into CF task, and have achieved great performance. However, most VAE-based methods suffer from matrix sparsity and consider the prior of users' latent factors to be the same, which leads to poor latent representations of users and items. Additionally, most existing methods model latent factors of users only and but not items, which makes them not be able to recommend items to a new user. To tackle these problems, we propose a Neural Variational Hybrid Collaborative Filtering, NVHCF. Specifically, we consider both the generative processes of users and items, and the prior of latent factors of users and items to be side informationspecific, which enables our model to alleviate matrix sparsity and learn better latent representations of users and items. For inference purpose, we derived a Stochastic Gradient Variational Bayes (SGVB) algorithm to analytically approximate the intractable distributions of latent factors of users and items. Experiments conducted on two large datasets have showed our methods significantly outperform the state-of-the-art CF methods, including the VAE-based methods.\",\n",
       "  'Title: analyzing and interpreting convolutional neural networks in nlp\\nAbstract: Convolutional neural networks have been successfully applied to various NLP tasks. However, it is not obvious whether they model different linguistic patterns such as negation, intensification, and clause compositionality to help the decision-making process. In this paper, we apply visualization techniques to observe how the model can capture different linguistic features and how these features can affect the performance of the model. Later on, we try to identify the model errors and their sources. We believe that interpreting CNNs is the first step to understand the underlying semantic features which can raise awareness to further improve the performance and explainability of CNN models.',\n",
       "  'Title: real time self adaptive deep stereo\\nAbstract: Deep convolutional neural networks trained end-to-end are the undisputed state-of-the-art methods to regress dense disparity maps directly from stereo pairs. However, such methods suffer from notable accuracy drops when exposed to scenarios significantly different from those seen in the training phase (e.g.real vs synthetic images, indoor vs outdoor, etc). As it is unlikely to be able to gather enough samples to achieve effective training/ tuning in any target domain, we propose to perform unsupervised and continuous online adaptation of a deep stereo network in order to preserve its accuracy independently of the sensed environment. However, such a strategy can be extremely demanding regarding computational resources and thus not enabling real-time performance. Therefore, we address this side effect by introducing a new lightweight, yet effective, deep stereo architecture Modularly ADaptive Network (MADNet) and by developing Modular ADaptation (MAD), an algorithm to train independently only sub-portions of our model. By deploying MADNet together with MAD we propose the first ever realtime self-adaptive deep stereo system.',\n",
       "  'Title: a modern take on the bias variance tradeoff in neural networks\\nAbstract: The bias-variance tradeoff tells us that as model complexity increases, bias falls and variances increases, leading to a U-shaped test error curve. However, recent empirical results with over-parameterized neural networks are marked by a striking absence of the classic U-shaped test error curve: test error keeps decreasing in wider networks. This suggests that there might not be a bias-variance tradeoff in neural networks with respect to network width, unlike was originally claimed by, e.g., Geman et al. (1992). Motivated by the shaky evidence used to support this claim in neural networks, we measure bias and variance in the modern setting. We find that both bias and variance can decrease as the number of parameters grows. To better understand this, we introduce a new decomposition of the variance to disentangle the effects of optimization and data sampling. We also provide theoretical analysis in a simplified setting that is consistent with our empirical findings.',\n",
       "  'Title: online learning with feedback graphs and switching costs\\nAbstract: We study online learning when partial feedback information is provided following every action of the learning process, and the learner incurs switching costs for changing his actions. In this setting, the feedback information system can be represented by a graph, and previous work provided the expected regret of the learner in the case of a clique (Expert setup), or disconnected single loops (Multi-Armed Bandits). We provide a lower bound on the expected regret in the partial information (PI) setting, namely for general feedback graphs ---excluding the clique. We show that all algorithms that are optimal without switching costs are necessarily sub-optimal in the presence of switching costs, which motivates the need to design new algorithms in this setup. We propose two novel algorithms: Threshold Based EXP3 and EXP3.SC. For the two special cases of symmetric PI setting and Multi-Armed-Bandits, we show that the expected regret of both algorithms is order optimal in the duration of the learning process with a pre-constant dependent on the feedback system. Additionally, we show that Threshold Based EXP3 is order optimal in the switching cost, whereas EXP3.SC is not. Finally, empirical evaluations show that Threshold Based EXP3 outperforms previous algorithm EXP3 SET in the presence of switching costs, and Batch EXP3 in the special setting of Multi-Armed Bandits with switching costs, where both algorithms are order optimal.',\n",
       "  \"Title: continual state representation learning for reinforcement learning using generative replay\\nAbstract: We consider the problem of building a state representation model in a continual fashion. As the environment changes, the aim is to efficiently compress the sensory state's information without losing past knowledge. The learned features are then fed to a Reinforcement Learning algorithm to learn a policy. We propose to use Variational Auto-Encoders for state representation, and Generative Replay, i.e. the use of generated samples, to maintain past knowledge. We also provide a general and statistically sound method for automatic environment change detection. Our method provides efficient state representation as well as forward transfer, and avoids catastrophic forgetting. The resulting model is capable of incrementally learning information without using past data and with a bounded system size.\",\n",
       "  'Title: human competitive patches in automatic program repair with repairnator\\nAbstract: Repairnator is a bot. It constantly monitors software bugs discovered during continuous integration of open-source software and tries to fix them automatically. If it succeeds to synthesize a valid patch, Repairnator proposes the patch to the human developers, disguised under a fake human identity. To date, Repairnator has been able to produce 5 patches that were accepted by the human developers and permanently merged in the code base. This is a milestone for human-competitiveness in software engineering research on automatic program repair.',\n",
       "  'Title: design challenges of multi uav systems in cyber physical applications a comprehensive survey and future directions\\nAbstract: Unmanned Aerial Vehicles (UAVs) have recently rapidly grown to facilitate a wide range of innovative applications that can fundamentally change the way cyber-physical systems (CPSs) are designed. CPSs are a modern generation of systems with synergic cooperation between computational and physical potentials that can interact with humans through several new mechanisms. The main advantages of using UAVs in CPS application is their exceptional features, including their mobility, dynamism, effortless deployment, adaptive altitude, agility, adjustability, and effective appraisal of real-world functions anytime and anywhere. Furthermore, from the technology perspective, UAVs are predicted to be a vital element of the development of advanced CPSs. Therefore, in this survey, we aim to pinpoint the most fundamental and important design challenges of multi-UAV systems for CPS applications. We highlight key and versatile aspects that span the coverage and tracking of targets and infrastructure objects, energy-efficient navigation, and image analysis using machine learning for fine-grained CPS applications. Key prototypes and testbeds are also investigated to show how these practical technologies can facilitate CPS applications. We present and propose state-of-the-art algorithms to address design challenges with both quantitative and qualitative methods and map these challenges with important CPS applications to draw insightful conclusions on the challenges of each application. Finally, we summarize potential new directions and ideas that could shape future research in these areas.',\n",
       "  'Title: conceptual organization is revealed by consumer activity patterns\\nAbstract: Meaning may arise from an element\\'s role or interactions within a larger system. For example, hitting nails is more central to people\\'s concept of a hammer than its particular material composition or other intrinsic features. Likewise, the importance of a web page may result from its links with other pages rather than solely from its content. One example of meaning arising from extrinsic relationships are approaches that extract the meaning of word concepts from co-occurrence patterns in large, text corpora. The success of these methods suggest that human activity patterns may reveal conceptual organization. However, texts do not directly reflect human activity, but instead serve a communicative function and are usually highly curated or edited to suit an audience. Here, we apply methods devised for text to a data source that directly reflects thousands of individuals\\' activity patterns, namely supermarket purchases. Using product co-occurrence data from nearly 1.3m shopping baskets, we trained a topic model to learn 25 high-level concepts (or \"topics\"). These topics were found to be comprehensible and coherent by both retail experts and consumers. Topics ranged from specific (e.g., ingredients for a stir-fry) to general (e.g., cooking from scratch). Topics tended to be goal-directed and situational, consistent with the notion that human conceptual knowledge is tailored to support action. Individual differences in the topics sampled predicted basic demographic characteristics. These results suggest that human activity patterns reveal conceptual organization and may give rise to it.',\n",
       "  'Title: improved hybrid ctc attention model for speech recognition\\nAbstract: Recently, end-to-end speech recognition with a hybrid model consisting of the connectionist temporal classification(CTC) and the attention encoder-decoder achieved state-of-the-art results. In this paper, we propose a novel CTC decoder structure based on the experiments we conducted and explore the relation between decoding performance and the depth of encoder. We also apply attention smoothing mechanism to acquire more context information for subword-based decoding. Taken together, these strategies allow us to achieve a word error rate(WER) of 4.43% without LM and 3.34% with RNN-LM on the test-clean subset of the LibriSpeech corpora, which by far are the best reported WERs for end-to-end ASR systems on this dataset.',\n",
       "  'Title: physical unclonable function based key sharing for iot security\\nAbstract: In many Industry Internet of Things (IIoT) applications, resources like CPU, memory, and battery power are limited and cannot afford the classic cryptographic security solutions. Silicon Physical Unclonable Function (PUF) is a lightweight security primitive that exploits manufacturing variations during the chip fabrication process for key generation and/or device authentication. However, traditional weak PUFs such as Ring Oscillator (RO) PUF generate chip-unique key for each device, which restricts their application in security protocols where the same key is required to be shared in resource-constrained devices. In order to address this issue, we propose a PUF-based key sharing method for the first time. The basic idea is to implement one-to-one input-output mapping with Lookup Table (LUT)-based interstage crossing structures in each level of inverters of RO PUF. Individual customization on configuration bits of interstage crossing structure and different RO selections with challenges bring high flexibility. Therefore, with the flexible configuration of interstage crossing structures and challenges, CRO PUF can generate the same shared key for resource-constrained devices, which enables a new application for lightweight key sharing protocols.',\n",
       "  'Title: structure learning of deep networks via dna computing algorithm\\nAbstract: Convolutional Neural Network (CNN) has gained state-of-the-art results in many pattern recognition and computer vision tasks. However, most of the CNN structures are manually designed by experienced researchers. Therefore, auto- matically building high performance networks becomes an important problem. In this paper, we introduce the idea of using DNA computing algorithm to automatically learn high-performance architectures. In DNA computing algorithm, we use short DNA strands to represent layers and long DNA strands to represent overall networks. We found that most of the learned models perform similarly, and only those performing worse during the first runs of training will perform worse finally than others. The indicates that: 1) Using DNA computing algorithm to learn deep architectures is feasible; 2) Local minima should not be a problem of deep networks; 3) We can use early stop to kill the models with the bad performance just after several runs of training. In our experiments, an accuracy 99.73% was obtained on the MNIST data set and an accuracy 95.10% was obtained on the CIFAR-10 data set.',\n",
       "  'Title: law and adversarial machine learning\\nAbstract: When machine learning systems fail because of adversarial manipulation, how should society expect the law to respond? Through scenarios grounded in adversarial ML literature, we explore how some aspects of computer crime, copyright, and tort law interface with perturbation, poisoning, model stealing and model inversion attacks to show how some attacks are more likely to result in liability than others. We end with a call for action to ML researchers to invest in transparent benchmarks of attacks and defenses; architect ML systems with forensics in mind and finally, think more about adversarial machine learning in the context of civil liberties. The paper is targeted towards ML researchers who have no legal background.',\n",
       "  'Title: unsupervised data selection for supervised learning\\nAbstract: Recent research put a big effort in the development of deep learning architectures and optimizers obtaining impressive results in areas ranging from vision to language processing. However little attention has been addressed to the need of a methodological process of data collection. In this work we hypothesize that high quality data for supervised learning can be selected in an unsupervised manner and that by doing so one can obtain models capable to generalize better than in the case of random training set construction. However, preliminary results are not robust and further studies on the subject should be carried out.',\n",
       "  'Title: exploiting the laws of order in smart contracts\\nAbstract: We investigate a family of bugs in blockchain-based smart contracts, which we call event-ordering (or EO) bugs. These bugs are intimately related to the dynamic ordering of contract events, i.e., calls of its functions on the blockchain, and enable potential exploits of millions of USD worth of Ether. Known examples of such bugs and prior techniques to detect them have been restricted to a small number of event orderings, typicall 1 or 2. Our work provides a new formulation of this general class of EO bugs as finding concurrency properties arising in long permutations of such events. The technical challenge in detecting our formulation of EO bugs is the inherent combinatorial blowup in path and state space analysis, even for simple contracts. We propose the first use of partial-order reduction techniques, using happen-before relations extracted automatically for contracts, along with several other optimizations built on a dynamic symbolic execution technique. We build an automatic tool called ETHRACER that requires no hints from users and runs directly on Ethereum bytecode. It flags 7-11% of over ten thousand contracts analyzed in roughly 18.5 minutes per contract, providing compact event traces that human analysts can run as witnesses. These witnesses are so compact that confirmations require only a few minutes of human effort. Half of the flagged contracts have subtle EO bugs, including in ERC-20 contracts that carry hundreds of millions of dollars worth of Ether. Thus, ETHRACER is effective at detecting a subtle yet dangerous class of bugs which existing tools miss.',\n",
       "  'Title: neural music synthesis for flexible timbre control\\nAbstract: The recent success of raw audio waveform synthesis models like WaveNet motivates a new approach for music synthesis, in which the entire process --- creating audio samples from a score and instrument information --- is modeled using generative neural networks. This paper describes a neural music synthesis model with flexible timbre controls, which consists of a recurrent neural network conditioned on a learned instrument embedding followed by a WaveNet vocoder. The learned embedding space successfully captures the diverse variations in timbres within a large dataset and enables timbre control and morphing by interpolating between instruments in the embedding space. The synthesis quality is evaluated both numerically and perceptually, and an interactive web demo is presented.',\n",
       "  'Title: a review for weighted minhash algorithms\\nAbstract: Data similarity (or distance) computation is a fundamental research topic which underpins many high-level applications based on similarity measures in machine learning and data mining. However, in large-scale real-world scenarios, the exact similarity computation has become daunting due to \"3V\" nature (volume, velocity and variety) of big data. In such cases, the hashing techniques have been verified to efficiently conduct similarity estimation in terms of both theory and practice. Currently, MinHash is a popular technique for efficiently estimating the Jaccard similarity of binary sets and furthermore, weighted MinHash is generalized to estimate the generalized Jaccard similarity of weighted sets. This review focuses on categorizing and discussing the existing works of weighted MinHash algorithms. In this review, we mainly categorize the Weighted MinHash algorithms into quantization-based approaches, \"active index\"-based ones and others, and show the evolution and inherent connection of the weighted MinHash algorithms, from the integer weighted MinHash algorithms to real-valued weighted MinHash ones (particularly the Consistent Weighted Sampling scheme). Also, we have developed a python toolbox for the algorithms, and released it in our github. Based on the toolbox, we experimentally conduct a comprehensive comparative study of the standard MinHash algorithm and the weighted MinHash ones.',\n",
       "  'Title: copy the old or paint anew an adversarial framework for non parametric image stylization\\nAbstract: Parametric generative deep models are state-of-the-art for photo and non-photo realistic image stylization. However, learning complicated image representations requires compute-intense models parametrized by a huge number of weights, which in turn requires large datasets to make learning successful. Non-parametric exemplar-based generation is a technique that works well to reproduce style from small datasets, but is also compute-intensive. These aspects are a drawback for the practice of digital AI artists: typically one wants to use a small set of stylization images, and needs a fast flexible model in order to experiment with it. With this motivation, our work has these contributions: (i) a novel stylization method called Fully Adversarial Mosaics (FAMOS) that combines the strengths of both parametric and non-parametric approaches; (ii) multiple ablations and image examples that analyze the method and show its capabilities; (iii) source code that will empower artists and machine learning researchers to use and modify FAMOS.',\n",
       "  'Title: fotonnet a hw efficient object detection system using 3d depth segmentation and 2d dnn classifier\\nAbstract: Object detection and classification is one of the most important computer vision problems. Ever since the introduction of deep learning \\\\cite{krizhevsky2012imagenet}, we have witnessed a dramatic increase in the accuracy of this object detection problem. However, most of these improvements have occurred using conventional 2D image processing. Recently, low-cost 3D-image sensors, such as the Microsoft Kinect (Time-of-Flight) or the Apple FaceID (Structured-Light), can provide 3D-depth or point cloud data that can be added to a convolutional neural network, acting as an extra set of dimensions. In our proposed approach, we introduce a new 2D + 3D system that takes the 3D-data to determine the object region followed by any conventional 2D-DNN, such as AlexNet. In this method, our approach can easily dissociate the information collection from the Point Cloud and 2D-Image data and combine both operations later. Hence, our system can use any existing trained 2D network on a large image dataset, and does not require a large 3D-depth dataset for new training. Experimental object detection results across 30 images show an accuracy of 0.67, versus 0.54 and 0.51 for RCNN and YOLO, respectively.',\n",
       "  'Title: attention please adversarial defense via attention rectification and preservation\\nAbstract: This study provides a new understanding of the adversarial attack problem by examining the correlation between adversarial attack and visual attention change. In particular, we observed that: (1) images with incomplete attention regions are more vulnerable to adversarial attacks; and (2) successful adversarial attacks lead to deviated and scattered attention map. Accordingly, an attention-based adversarial defense framework is designed to simultaneously rectify the attention map for prediction and preserve the attention area between adversarial and original images. The problem of adding iteratively attacked samples is also discussed in the context of visual attention change. We hope the attention-related data analysis and defense solution in this study will shed some light on the mechanism behind the adversarial attack and also facilitate future adversarial defense/attack model design.',\n",
       "  'Title: lightweight lipschitz margin training for certified defense against adversarial examples\\nAbstract: How can we make machine learning provably robust against adversarial examples in a scalable way? Since certified defense methods, which ensure $\\\\epsilon$-robust, consume huge resources, they can only achieve small degree of robustness in practice. Lipschitz margin training (LMT) is a scalable certified defense, but it can also only achieve small robustness due to over-regularization. How can we make certified defense more efficiently? We present LC-LMT, a light weight Lipschitz margin training which solves the above problem. Our method has the following properties; (a) efficient: it can achieve $\\\\epsilon$-robustness at early epoch, and (b) robust: it has a potential to get higher robustness than LMT. In the evaluation, we demonstrate the benefits of the proposed method. LC-LMT can achieve required robustness more than 30 epoch earlier than LMT in MNIST, and shows more than 90 $\\\\%$ accuracy against both legitimate and adversarial inputs.',\n",
       "  \"Title: practical deep reinforcement learning approach for stock trading\\nAbstract: Stock trading strategy plays a crucial role in investment companies. However, it is challenging to obtain optimal strategy in the complex and dynamic stock market. We explore the potential of deep reinforcement learning to optimize stock trading strategy and thus maximize investment return. 30 stocks are selected as our trading stocks and their daily prices are used as the training and trading market environment. We train a deep reinforcement learning agent and obtain an adaptive trading strategy. The agent's performance is evaluated and compared with Dow Jones Industrial Average and the traditional min-variance portfolio allocation strategy. The proposed deep reinforcement learning approach is shown to outperform the two baselines in terms of both the Sharpe ratio and cumulative returns.\",\n",
       "  'Title: pair wise exchangeable feature extraction for arbitrary style transfer\\nAbstract: Style transfer has been an important topic in both computer vision and graphics. Gatys et al. first prove that deep features extracted by the pre-trained VGG network represent both content and style features of an image and hence, style transfer can be achieved through optimization in feature space. Huang et al. then show that real-time arbitrary style transfer can be done by simply aligning the mean and variance of each feature channel. In this paper, however, we argue that only aligning the global statistics of deep features cannot always guarantee a good style transfer. Instead, we propose to jointly analyze the input image pair and extract common/exchangeable style features between the two. Besides, a new fusion mode is developed for combining content and style information in feature space. Qualitative and quantitative experiments demonstrate the advantages of our approach.',\n",
       "  'Title: on buildings that compute a proposal\\nAbstract: We present ideas aimed at bringing revolutionary changes on architectures and buildings of tomorrow by radically advancing the technology for the building material concrete and hence building components. We propose that by using nanotechnology we could embed computation and sensing directly into the material used for construction. Intelligent concrete blocks and panels advanced with stimuli-responsive smart paints are the core of the proposed architecture. In particular, the photo-responsive paint would sense the buildings internal and external environment while the nano-material-concrete composite material would be capable of sensing the building environment and implement massive-parallel information processing resulting in distributed decision making. A calibration of the proposed materials with in-materio suitable computational methods and corresponding building information modelling, computer-aided design and digital manufacturing tools could be achievedvia models and prototypes of information processing at nano-level. The emergent technology sees a building as high-level massive-parallel computer—assembled of computing concrete blocks. Based on the generic principles of neuromorphic computation and reservoir computing we envisage a single building or an urban quarter to turn into a large-scale sensing substrate. It could behave as a universal computer, collecting and processing environmental information in situ enabling appropriate data fusion. The broad range of spatio-temporal effects include infrastructural and human mobility, energy, bio-diversity, digital activity, urban management, art and socializing, robustness with regard to damage and noise or real-time monitoring of environmental changes. The proposed intelligent architectures will increase sustainability and viability in digitised urban environments by decreasing information transfer bandwidth by e.g, utilising 5G networks. The emergence of socio-cultural effect will create a cybernetic relationship with our dwellings and cities.',\n",
       "  'Title: limited gradient descent learning with noisy labels\\nAbstract: Label noise may affect the generalization of classifiers, and the effective learning of main patterns from samples with noisy labels is an important challenge. Recent studies have shown that deep neural networks tend to prioritize the learning of simple patterns over the memorization of noise patterns. This suggests a possible method to search for the best generalization that learns the main pattern until the noise begins to be memorized. Traditional approaches often employ a clean validation set to find the best stop timing of learning, i.e., early stopping. However, the generalization performance of such methods relies on the quality of validation sets. Further, in practice, a clean validation set is sometimes difficult to obtain. To solve this problem, we propose a method that can estimate the optimal stopping timing without a clean validation set, called limited gradient descent. We modified the labels of a few samples in a noisy dataset to obtain false labels and to create a reverse pattern. By monitoring the learning progress of the noisy and reverse samples, we can determine the stop timing of learning. In this paper, we also theoretically provide some necessary conditions on learning with noisy labels. Experimental results on CIFAR-10 and CIFAR-100 datasets demonstrate that our approach has a comparable generalization performance to methods relying on a clean validation set. Thus, on the noisy Clothing-1M dataset, our approach surpasses methods that rely on a clean validation set.',\n",
       "  'Title: mixed precision training for nlp and speech recognition with openseq2seq\\nAbstract: We present OpenSeq2Seq - a TensorFlow-based toolkit for training sequence-to-sequence models that features distributed and mixed-precision training. Benchmarks on machine translation and speech recognition tasks show that models built using OpenSeq2Seq give state-of-the-art performance at 1.5-3x less training time. OpenSeq2Seq currently provides building blocks for models that solve a wide range of tasks including neural machine translation, automatic speech recognition, and speech synthesis.',\n",
       "  'Title: efficient and scalable multi task regression on massive number of tasks\\nAbstract: Many real-world large-scale regression problems can be formulated as Multi-task Learning (MTL) problems with a massive number of tasks, as in retail and transportation domains. However, existing MTL methods still fail to offer both the generalization performance and the scalability for such problems. Scaling up MTL methods to problems with a tremendous number of tasks is a big challenge. Here, we propose a novel algorithm, named Convex Clustering Multi-Task regression Learning (CCMTL), which integrates with convex clustering on the k-nearest neighbor graph of the prediction models. Further, CCMTL efficiently solves the underlying convex problem with a newly proposed optimization method. CCMTL is accurate, efficient to train, and empirically scales linearly in the number of tasks. On both synthetic and real-world datasets, the proposed CCMTL outperforms seven state-of-the-art (SoA) multi-task learning methods in terms of prediction accuracy as well as computational efficiency. On a real-world retail dataset with 23,812 tasks, CCMTL requires only around 30 seconds to train on a single thread, while the SoA methods need up to hours or even days.',\n",
       "  \"Title: a study of the feasibility of co located app attacks against ble and a large scale analysis of the current application layer security landscape\\nAbstract: Bluetooth Low Energy (BLE) is a fast-growing wireless technology with a large number of potential use cases, particularly in the IoT domain. Increasingly, these use cases require the storage of sensitive user data or critical device controls on the BLE device, as well as the access of this data by an augmentative mobile application. Uncontrolled access to such data could violate user privacy, cause a device to malfunction, or even endanger lives. The BLE standard provides security mechanisms such as pairing and bonding to protect sensitive data such that only authenticated devices can access it. In this paper we show how unauthorized co-located Android applications can access pairing-protected BLE data, without the user's knowledge. We discuss mitigation strategies in terms of the various stakeholders involved in this ecosystem, and argue that at present, the only possible option for securing BLE data is for BLE developers to implement remedial measures in the form of application-layer security between the BLE device and the Android application. We introduce BLECryptracer, a tool for identifying the presence of such application-layer security, and present the results of a large-scale static analysis over 18,900+ BLE-enabled Android applications. Our findings indicate that over 45% of these applications do not implement measures to protect BLE data, and that cryptography is sometimes applied incorrectly in those that do. This implies that a potentially large number of corresponding BLE peripheral devices are vulnerable to unauthorized data access.\",\n",
       "  'Title: model evaluation model selection and algorithm selection in machine learning\\nAbstract: The correct use of model evaluation, model selection, and algorithm selection techniques is vital in academic machine learning research as well as in many industrial settings. This article reviews different techniques that can be used for each of these three subtasks and discusses the main advantages and disadvantages of each technique with references to theoretical and empirical studies. Further, recommendations are given to encourage best yet feasible practices in research and applications of machine learning. Common methods such as the holdout method for model evaluation and selection are covered, which are not recommended when working with small datasets. Different flavors of the bootstrap technique are introduced for estimating the uncertainty of performance estimates, as an alternative to confidence intervals via normal approximation if bootstrapping is computationally feasible. Common cross-validation techniques such as leave-one-out cross-validation and k-fold cross-validation are reviewed, the bias-variance trade-off for choosing k is discussed, and practical tips for the optimal choice of k are given based on empirical evidence. Different statistical tests for algorithm comparisons are presented, and strategies for dealing with multiple comparisons such as omnibus tests and multiple-comparison corrections are discussed. Finally, alternative methods for algorithm selection, such as the combined F-test 5x2 cross-validation and nested cross-validation, are recommended for comparing machine learning algorithms when datasets are small.',\n",
       "  'Title: dense xunit networks\\nAbstract: Deep net architectures have constantly evolved over the past few years, leading to significant advancements in a wide array of computer vision tasks. However, besides high accuracy, many applications also require a low computational load and limited memory footprint. To date, efficiency has typically been achieved either by architectural choices at the macro level (e.g. using skip connections or pruning techniques) or modifications at the level of the individual layers (e.g. using depth-wise convolutions or channel shuffle operations). Interestingly, much less attention has been devoted to the role of the activation functions in constructing efficient nets. Recently, Kligvasser et al. showed that incorporating spatial connections within the activation functions, enables a significant boost in performance in image restoration tasks, at any given budget of parameters. However, the effectiveness of their xUnit module has only been tested on simple small models, which are not characteristic of those used in high-level vision tasks. In this paper, we adopt and improve the xUnit activation, show how it can be incorporated into the DenseNet architecture, and illustrate its high effectiveness for classification and image restoration tasks alike. While the DenseNet architecture is extremely efficient to begin with, our dense xUnit net (DxNet) can typically achieve the same performance with far fewer parameters. For example, on ImageNet, our DxNet outperforms a ReLU-based DenseNet having 30% more parameters and achieves state-of-the-art results for this budget of parameters. Furthermore, in denoising and super-resolution, DxNet significantly improves upon all existing lightweight solutions, including the xUnit-based nets of Kligvasser et al.',\n",
       "  'Title: document structure measure for hypernym discovery\\nAbstract: Hypernym discovery is the problem of finding terms that have is-a relationship with a given term. We introduce a new context type, and a relatedness measure to differentiate hypernyms from other types of semantic relationships. Our Document Structure measure is based on hierarchical position of terms in a document, and their presence or otherwise in definition text. This measure quantifies the document structure using multiple attributes, and classes of weighted distance functions.',\n",
       "  'Title: towards secure and efficient payment channels\\nAbstract: Micropayment channels are the most prominent solution to the limitation on transaction throughput in current blockchain systems. However, in practice channels are risky because participants have to be online constantly to avoid fraud, and inefficient because participants have to open multiple channels and lock funds in them. To address the security issue, we propose a novel mechanism that involves watchtowers incentivized to watch the channels and reveal a fraud. Our protocol does not require participants to be online constantly watching the blockchain. The protocol is secure, incentive compatible and lightweight in communication. Furthermore, we present an adaptation of our protocol implementable on the Lightning protocol. Towards efficiency, we examine specific topological structures in the blockchain transaction graph and generalize the construction of channels to enable topologies better suited to specific real-world needs. In these cases, our construction reduces the required amount of signatures for a transaction and the total amount of locked funds in the system.',\n",
       "  'Title: explaining the ambiguity of object detection and 6d pose from visual data\\nAbstract: 3D object detection and pose estimation from a single image are two inherently ambiguous problems. Oftentimes, objects appear similar from different viewpoints due to shape symmetries, occlusion and repetitive textures. This ambiguity in both detection and pose estimation means that an object instance can be perfectly described by several different poses and even classes. In this work we propose to explicitly deal with this uncertainty. For each object instance we predict multiple pose and class outcomes to estimate the specific pose distribution generated by symmetries and repetitive textures. The distribution collapses to a single outcome when the visual appearance uniquely identifies just one valid pose. We show the benefits of our approach which provides not only a better explanation for pose ambiguity, but also a higher accuracy in terms of pose estimation.',\n",
       "  'Title: adversarial defense by stratified convolutional sparse coding\\nAbstract: We propose an adversarial defense method that achieves state-of-the-art performance among attack-agnostic adversarial defense methods while also maintaining robustness to input resolution, scale of adversarial perturbation, and scale of dataset size. Based on convolutional sparse coding, we construct a stratified low-dimensional quasi-natural image space that faithfully approximates the natural image space while also removing adversarial perturbations. We introduce a novel Sparse Transformation Layer (STL) in between the input image and the first layer of the neural network to efficiently project images into our quasi-natural image space. Our experiments show state-of-the-art performance of our method compared to other attack-agnostic adversarial defense methods in various adversarial settings.',\n",
       "  \"Title: modeling treatment delays for patients using feature label pairs in a time series\\nAbstract: Pharmaceutical targeting is one of key inputs for making sales and marketing strategy planning. Targeting list is built on predicting physician's sales potential of certain type of patient. In this paper, we present a time-sensitive targeting framework leveraging time series model to predict patient's disease and treatment progression. We create time features by extracting service history within a certain period, and record whether the event happens in a look-forward period. Such feature-label pairs are examined across all time periods and all patients to train a model. It keeps the inherent order of services and evaluates features associated to the imminent future, which contribute to improved accuracy.\",\n",
       "  'Title: deepvoxels learning persistent 3d feature embeddings\\nAbstract: In this work, we address the lack of 3D understanding of generative neural networks by introducing a persistent 3D feature embedding for view synthesis. To this end, we propose DeepVoxels, a learned representation that encodes the view-dependent appearance of a 3D scene without having to explicitly model its geometry. At its core, our approach is based on a Cartesian 3D grid of persistent embedded features that learn to make use of the underlying 3D scene structure. Our approach combines insights from 3D geometric computer vision with recent advances in learning image-to-image mappings based on adversarial loss functions. DeepVoxels is supervised, without requiring a 3D reconstruction of the scene, using a 2D re-rendering loss and enforces perspective and multi-view geometry in a principled manner. We apply our persistent 3D scene representation to the problem of novel view synthesis demonstrating high-quality results for a variety of challenging scenes.',\n",
       "  'Title: binary sequence set design for interferer rejection in multi branch modulation\\nAbstract: Wideband communication is often expected to deal with a very wide spectrum, which in many environments of interest includes strong interferers. Thus receivers for the wideband communication systems often need to mitigate interferers to reduce the distortion caused by the amplifier nonlinearity and noise. Recently, a new architecture for communication receivers known as random modulation mixes a signal with different pseudorandom sequences using multiple branches of channels before sampling. While random modulation is used in these receivers to acquire the signal at low sampling rates, the modulation sequences used lack the ability to suppress interferers due to their flat spectra. In previous work, we introduced the design of a single spectrally shaped binary sequence that mitigates interferers to replace the pseudorandom sequence in a channel. However, the designed sequences cannot provide the stable recovery achieved by pseudorandom sequence approaches. In this paper, we extend our previous sequence design to guarantee stable recovery by designing a set of sequences to be orthogonal to each other. We show that it is difficult to find the necessary number of sequences featuring mutual orthogonality and introduce oversampling to the sequence set design to improve the recovery performance. We propose an algorithm for multi-branch sequence design as a binary optimization problem, which is solved using a semidefinite program relaxation and randomized projection. While it is common to model narrowband interferers as a subspace spanned by a subset of elements from the Fourier basis, we show that the Slepian basis provides an alternative and more suitable compact representation for signals with components contained in narrow spectrum bands. Numerical experiments using the proposed sequence sets show their advantages against pseudorandom sequences and our previous work.',\n",
       "  'Title: enumeration structural and dimensional synthesis of robotic hands theory and implementation\\nAbstract: Designing robotic hands for specific tasks could help in the creation of optimized end-effectors for grasping and manipulation. However the systematic design of robotic hands for a simultaneous task of all fingertips presents many challenges. In this work the algorithms and implementation of an overall synthesis process is presented, which could be a first step towards a complete design tool for robotic end-effectors. #R##N#Type synthesis for a given task and number of fingers, solvability and dimensional synthesis for arbitrary topologies are developed and implemented. The resulting solver is a powerful tool that can aid in the creation of innovative robotic hands with arbitrary number of fingers and palms. Several examples of type synthesis, solvability calculations and dimensional synthesis are presented.',\n",
       "  'Title: levy flights of the collective imagination\\nAbstract: We present a structured random-walk model that captures key aspects of how people communicate in groups. Our model takes the form of a correlated Levy flight that quantifies the balance between focused discussion of an idea and long-distance leaps in semantic space. We apply our model to three cases of increasing structural complexity: philosophical texts by Aristotle, Hume, and Kant; four days of parliamentary debate during the French Revolution; and branching comment trees on the discussion website Reddit. In the philosophical and parliamentary cases, the model parameters that describe this balance converge under coarse-graining to limit regions that demonstrate the emergence of large-scale structure, a result which is robust to translation between languages. Meanwhile, we find that the political forum we consider on Reddit exhibits a debate-like pattern, while communities dedicated to the discussion of science and news show much less temporal order, and may make use of the emergent, tree-like topology of comment replies to structure their epistemic explorations. Our model allows us to quantify the ways in which social technologies such as parliamentary procedures and online commenting systems shape the joint exploration of ideas.',\n",
       "  'Title: hybrid reinforcement learning with expert state sequences\\nAbstract: Existing imitation learning approaches often require that the complete demonstration data, including sequences of actions and states, are available. In this paper, we consider a more realistic and difficult scenario where a reinforcement learning agent only has access to the state sequences of an expert, while the expert actions are unobserved. We propose a novel tensor-based model to infer the unobserved actions of the expert state sequences. The policy of the agent is then optimized via a hybrid objective combining reinforcement learning and imitation learning. We evaluated our hybrid approach on an illustrative domain and Atari games. The empirical results show that (1) the agents are able to leverage state expert sequences to learn faster than pure reinforcement learning baselines, (2) our tensor-based action inference model is advantageous compared to standard deep neural networks in inferring expert actions, and (3) the hybrid policy optimization objective is robust against noise in expert state sequences.',\n",
       "  'Title: an improvement of paa on trend based approximation for time series\\nAbstract: Piecewise Aggregate Approximation (PAA) is a competitive basic dimension reduction method for high-dimensional time series mining. When deployed, however, the limitations are obvious that some important information will be missed, especially the trend. In this paper, we propose two new approaches for time series that utilize approximate trend feature information. Our first method is based on relative mean value of each segment to record the trend, which divide each segment into two parts and use the numerical average respectively to represent the trend. We proved that this method satisfies lower bound which guarantee no false dismissals. Our second method uses a binary string to record the trend which is also relative to mean in each segment. Our methods are applied on similarity measurement in classification and anomaly detection, the experimental results show the improvement of accuracy and effectiveness by extracting the trend feature suitably.',\n",
       "  'Title: the isti rapid response on exploring cloud computing 2018\\nAbstract: This report describes eighteen projects that explored how commercial cloud computing services can be utilized for scientific computation at national laboratories. These demonstrations ranged from deploying proprietary software in a cloud environment to leveraging established cloud-based analytics workflows for processing scientific datasets. By and large, the projects were successful and collectively they suggest that cloud computing can be a valuable computational resource for scientific computation at national laboratories.',\n",
       "  'Title: risk aware multi armed bandits using conditional value at risk\\nAbstract: Traditional multi-armed bandit problems are geared towards finding the arm with the highest expected value -- an objective that is risk-neutral. In several practical applications, e.g., finance, a risk-sensitive objective is to control the worst-case losses and Conditional Value-at-Risk (CVaR) is a popular risk measure for modelling the aforementioned objective. We consider the CVaR optimization problem in a best-arm identification framework under a fixed budget. First, we derive a novel two-sided concentration bound for a well-known CVaR estimator using empirical distribution function, assuming that the underlying distribution is unbounded, but either sub-Gaussian or light-tailed. This bound may be of independent interest. Second, we adapt the well-known successive rejects algorithm to incorporate a CVaR-based criterion and derive an upper-bound on the probability of incorrect identification of our proposed algorithm.',\n",
       "  'Title: visibly pushdown languages over sliding windows\\nAbstract: We investigate the class of visibly pushdown languages in the sliding window model. A sliding window algorithm for a language $L$ receives a stream of symbols and has to decide at each time step whether the suffix of length $n$ belongs to $L$ or not. The window size $n$ is either a fixed number (in the fixed-size model) or can be controlled by an adversary in a limited way (in the variable-size model). The main result of this paper states that for every visibly pushdown language the space complexity in the variable-size sliding window model is either constant, logarithmic or linear in the window size. This extends previous results for regular languages.',\n",
       "  \"Title: actor conditioned attention maps for video action detection\\nAbstract: Interactions with surrounding objects and people contain important information towards understanding human actions. In order to model such interactions explicitly, we propose to generate attention maps that rank each spatio-temporal region's importance to a detected actor. We refer to these as Actor-Conditioned Attention Maps (ACAM), and these maps serve as weights to the features extracted from the whole scene. These resulting actor-conditioned features help focus the learned model on regions that are important/relevant to the conditioned actor. Another novelty of our approach is in the use of pre-trained object detectors, instead of region proposals, that generalize better to videos from different sources. Detailed experimental results on the AVA 2.1 datasets demonstrate the importance of interactions, with a performance improvement of 5 mAP with respect to state of the art published results.\",\n",
       "  'Title: transfer learning from language models to image caption generators better models may not transfer better\\nAbstract: When designing a neural caption generator, a convolutional neural network can be used to extract image features. Is it possible to also use a neural language model to extract sentence prefix features? We answer this question by trying different ways to transfer the recurrent neural network and embedding layer from a neural language model to an image caption generator. We find that image caption generators with transferred parameters perform better than those trained from scratch, even when simply pre-training them on the text of the same captions dataset it will later be trained on. We also find that the best language models (in terms of perplexity) do not result in the best caption generators after transfer learning.',\n",
       "  \"Title: non asymptotic fundamental limits of guessing subject to distortion\\nAbstract: This paper investigates the problem of guessing subject to distortion, which was introduced by Arikan and Merhav. While the primary concern of the previous study was asymptotic analysis, our primary concern is non-asymptotic analysis. We prove non-asymptotic achievability and converse bounds of the moment of the number of guesses without side information (resp. with side information) by using a quantity based on the R\\\\'enyi entropy (resp. the Arimoto-R\\\\'enyi conditional entropy). Also, we introduce an error probability and show similar results. Further, from our bounds, we derive a single-letter characterization of the asymptotic exponent of guessing moment for a stationary memoryless source.\",\n",
       "  'Title: composite shape modeling via latent space factorization\\nAbstract: We present a novel neural network architecture, termed Decomposer-Composer, for semantic structure-aware 3D shape modeling. Our method utilizes an auto-encoder-based pipeline, and produces a novel factorized shape embedding space, where the semantic structure of the shape collection translates into a data-dependent sub-space factorization, and where shape composition and decomposition become simple linear operations on the embedding coordinates. We further propose to model shape assembly using an explicit learned part deformation module, which utilizes a 3D spatial transformer network to perform an in-network volumetric grid deformation, and which allows us to train the whole system end-to-end. The resulting network allows us to perform part-level shape manipulation, unattainable by existing approaches. Our extensive ablation study, comparison to baseline methods and qualitative analysis demonstrate the improved performance of the proposed method.',\n",
       "  'Title: storns stochastic radio access network slicing\\nAbstract: Recently released 5G networks empower the novel Network Slicing concept. %which enables novel business models; Network slicing introduces new business models such as allowing telecom providers to lease a virtualized slice of their infrastructure to tenants such as industry verticals, e.g. automotive, e-health, factories, etc. However, this new paradigm poses a major challenge when applied to Radio Access Networks (RAN): how to achieve revenue maximization while meeting the diverse service level agreements (SLAs) requested by the infrastructure tenants? #R##N#In this paper, we propose a new analytical framework, based on stochastic geometry theory, to model realistic RANs that leverage the business opportunities offered by network slicing. We mathematically prove the benefits of slicing radio access networks as compared to non-sliced infrastructures. Based on this, we design a new admission control functional block, STORNS, which takes decisions considering per slice SLA guaranteed average experienced throughput. A radio resource allocation strategy is introduced to optimally allocate transmit power and bandwidth (i.e., a slice of radio access resources) to the users of each infrastructure tenant. Numerical results are illustrated to validate our proposed solution in terms of potential spectral efficiency, and compare it against a non-slicing benchmark.',\n",
       "  'Title: look before you sweep visibility aware motion planning\\nAbstract: This paper addresses the problem of planning for a robot with a directional obstacle-detection sensor that must move through a cluttered environment. The planning objective is to remain safe by finding a path for the complete robot, including sensor, that guarantees that the robot will not move into any part of the workspace before it has been seen by the sensor. Although a great deal of work has addressed a version of this problem in which the \"field of view\" of the sensor is a sphere around the robot, there is very little work addressing robots with a narrow or occluded field of view. We give a formal definition of the problem, several solution methods with different computational trade-offs, and experimental results in illustrative domains.',\n",
       "  'Title: gif2video color dequantization and temporal interpolation of gif images\\nAbstract: Graphics Interchange Format (GIF) is a highly portable graphics format that is ubiquitous on the Internet. Despite their small sizes, GIF images often contain undesirable visual artifacts such as flat color regions, false contours, color shift, and dotted patterns. In this paper, we propose GIF2Video, the first learning-based method for enhancing the visual quality of GIFs in the wild. We focus on the challenging task of GIF restoration by recovering information lost in the three steps of GIF creation: frame sampling, color quantization, and color dithering. We first propose a novel CNN architecture for color dequantization. It is built upon a compositional architecture for multi-step color correction, with a comprehensive loss function designed to handle large quantization errors. We then adapt the SuperSlomo network for temporal interpolation of GIF frames. We introduce two large datasets, namely GIF-Faces and GIF-Moments, for both training and evaluation. Experimental results show that our method can significantly improve the visual quality of GIFs, and outperforms direct baseline and state-of-the-art approaches.',\n",
       "  'Title: prototypical metric transfer learning for continuous speech keyword spotting with limited training data\\nAbstract: Continuous Speech Keyword Spotting (CSKS) is the problem of spotting keywords in recorded conversations, when a small number of instances of keywords are available in training data. Unlike the more common Keyword Spotting, where an algorithm needs to detect lone keywords or short phrases like \"Alexa\", \"Cortana\", \"Hi Alexa!\", \"Whatsup Octavia?\" etc. in speech, CSKS needs to filter out embedded words from a continuous flow of speech, ie. spot \"Anna\" and \"github\" in \"I know a developer named Anna who can look into this github issue.\" Apart from the issue of limited training data availability, CSKS is an extremely imbalanced classification problem. We address the limitations of simple keyword spotting baselines for both aforementioned challenges by using a novel combination of loss functions (Prototypical networks\\' loss and metric loss) and transfer learning. Our method improves F1 score by over 10%.',\n",
       "  'Title: real time monitoring of social media and digital press\\nAbstract: Talaia is a platform for monitoring social media and digital press. A configurable crawler gathers content with respect to user defined domains or topics. Crawled data is processed by means of the EliXa Sentiment Analysis system. A Django powered interface provides data visualization for a user-based analysis of the data. This paper presents the architecture of the system and describes in detail its different components. To prove the validity of the approach, two real use cases are accounted for: one in the cultural domain and one in the political domain. Evaluation for the sentiment analysis task in both scenarios is also provided, showing the capacity for domain adaptation.',\n",
       "  \"Title: on geometric complexity theory multiplicity obstructions are stronger than occurrence obstructions\\nAbstract: Geometric Complexity Theory as initiated by Mulmuley and Sohoni in two papers (SIAM J Comput 2001, 2008) aims to separate algebraic complexity classes via representation theoretic multiplicities in coordinate rings of specific group varieties. The papers also conjecture that the vanishing behavior of these multiplicities would be sufficient to separate complexity classes (so-called occurrence obstructions). The existence of such strong occurrence obstructions has been recently disproven in 2016 in two successive papers, Ikenmeyer-Panova (Adv. Math.) and Burgisser-Ikenmeyer-Panova (J. AMS). This raises the question whether separating group varieties via representation theoretic multiplicities is stronger than separating them via occurrences. This paper provides for the first time a setting where separating with multiplicities can be achieved, while the separation with occurrences is provably impossible. Our setting is surprisingly simple and natural: We study the variety of products of homogeneous linear forms (the so-called Chow variety) and the variety of polynomials of bounded border Waring rank (i.e. a higher secant variety of the Veronese variety). As a side result we prove a slight generalization of Hermite's reciprocity theorem, which proves Foulkes' conjecture for a new infinite family of cases.\",\n",
       "  'Title: stabilization time in weighted minority processes\\nAbstract: A minority process in a weighted graph is a dynamically changing coloring. Each node repeatedly changes its color in order to minimize the sum of weighted conflicts with its neighbors. We study the number of steps until such a process stabilizes. Our main contribution is an exponential lower bound on stabilization time. We first present a construction showing this bound in the adversarial sequential model, and then we show how to extend the construction to establish the same bound in the benevolent sequential model, as well as in any reasonable concurrent model. Furthermore, we show that the stabilization time of our construction remains exponential even for very strict switching conditions, namely, if a node only changes color when almost all (i.e., any specific fraction) of its neighbors have the same color. Our lower bound works in a wide range of settings, both for node-weighted and edge-weighted graphs, or if we restrict minority processes to the class of sparse graphs.',\n",
       "  'Title: a data driven method of optimizing feedforward compensator for autonomous vehicle\\nAbstract: A reliable controller is critical and essential for the execution of safe and smooth maneuvers of an autonomous vehicle.The controller must be robust to external disturbances, such as road surface, weather, and wind conditions, and so on.It also needs to deal with the internal parametric variations of vehicle sub-systems, including power-train efficiency, measurement errors, time delay,so on.Moreover, as in most production vehicles, the low-control commands for the engine, brake, and steering systems are delivered through separate electronic control units.These aforementioned factors introduce opaque and ineffectiveness issues in controller performance.In this paper, we design a feed-forward compensate process via a data-driven method to model and further optimize the controller performance.We apply the principal component analysis to the extraction of most influential features.Subsequently,we adopt a time delay neural network and include the accuracy of the predicted error in a future time horizon.Utilizing the predicted error,we then design a feed-forward compensate process to improve the control performance.Finally,we demonstrate the effectiveness of the proposed feed-forward compensate process in simulation scenarios.',\n",
       "  'Title: on the capacity region of bipartite and tripartite entanglement switching\\nAbstract: We study a quantum switch serving a set of users. The function of the switch is to create bi-or tripartite entangled state among users at the highest possible rates at a fixed ratio. We model a set of randomized switching policies. Discovering that some are better than others, we present analytical results for the case where the switch stores one qubit per user, and find that the best policies outperform a time division multiplexing (TDM) policy for sharing the switch between bipartite and tripartite state generation. This performance improvement decreases as the number of users grows. The model is easily augmented to study the capacity region in the presence of qubit decoherence, obtaining similar results. Moreover, decoherence appears to have little effect on capacity. We also study a smaller class of policies when the switch stores two qubits per user.',\n",
       "  \"Title: analysis of data harvesting by unmanned aerial vehicles\\nAbstract: This paper explores an emerging wireless architecture based on Unmanned Aerial Vehicles (UAVs), i.e., drones. We consider a network where UAVs at fixed altitude harvest data from Internet-of-Things (IoT) devices on the ground. In such a system, the UAVs' motion activates IoT uplink transmissions and so the motion triggers the interference field and determines the network performance. To analyze the performance, we propose a stochastic geometry model. The coverage area of each UAV, referred to as the activation window, is modeled for simplicity as a rectangle where at most one IoT device is scheduled to transmit at a time. In this setting, we analyze the signal-to-interference and data rate from two typical perspectives, namely from a typical UAV's and from a typical IoT device's points of view. Our stochastic geometry model enables us to explore the size of the activation window which maximizes the UAV networks' harvesting capacity. Finally, we present a network extension of the proposed model and derive the network performance.\",\n",
       "  'Title: interpretable neural networks for predicting mortality risk using multi modal electronic health records\\nAbstract: We present an interpretable neural network for predicting an important clinical outcome (1-year mortality) from multi-modal Electronic Health Record (EHR) data. Our approach builds on prior multi-modal machine learning models by now enabling visualization of how individual factors contribute to the overall outcome risk, assuming other factors remain constant, which was previously impossible. #R##N#We demonstrate the value of this approach using a large multi-modal clinical dataset including both EHR data and 31,278 echocardiographic videos of the heart from 26,793 patients. We generated separate models for (i) clinical data only (CD) (e.g. age, sex, diagnoses and laboratory values), (ii) numeric variables derived from the videos, which we call echocardiography-derived measures (EDM), and (iii) CD+EDM+raw videos (pixel data). The interpretable multi-modal model maintained performance compared to non-interpretable models (Random Forest, XGBoost), and also performed significantly better than a model using a single modality (average AUC=0.82). Clinically relevant insights and multi-modal variable importance rankings were also facilitated by the new model, which have previously been impossible.',\n",
       "  \"Title: managing popularity bias in recommender systems with personalized re ranking\\nAbstract: Many recommender systems suffer from popularity bias: popular items are recommended frequently while less popular, niche products, are recommended rarely or not at all. However, recommending the ignored products in the `long tail' is critical for businesses as they are less likely to be discovered. In this paper, we introduce a personalized diversification re-ranking approach to increase the representation of less popular items in recommendations while maintaining acceptable recommendation accuracy. Our approach is a post-processing step that can be applied to the output of any recommender system. We show that our approach is capable of managing popularity bias more effectively, compared with an existing method based on regularization. We also examine both new and existing metrics to measure the coverage of long-tail items in the recommendation.\",\n",
       "  'Title: a deterministic approach to avoid saddle points\\nAbstract: Loss functions with a large number of saddle points are one of the main obstacles to training many modern machine learning models. Gradient descent (GD) is a fundamental algorithm for machine learning and converges to a saddle point for certain initial data. We call the region formed by these initial values the \"attraction region.\" For quadratic functions, GD converges to a saddle point if the initial data is in a subspace of up to n-1 dimensions. In this paper, we prove that a small modification of the recently proposed Laplacian smoothing gradient descent (LSGD) [Osher, et al., arXiv:1806.06317] contributes to avoiding saddle points without sacrificing the convergence rate of GD. In particular, we show that the dimension of the LSGD\\'s attraction region is at most floor((n-1)/2) for a class of quadratic functions which is significantly smaller than GD\\'s (n-1)-dimensional attraction region.',\n",
       "  'Title: capacity allocation analysis of neural networks a tool for principled architecture design\\nAbstract: Designing neural network architectures is a task that lies somewhere between science and art. For a given task, some architectures are eventually preferred over others, based on a mix of intuition, experience, experimentation and luck. For many tasks, the final word is attributed to the loss function, while for some others a further perceptual evaluation is necessary to assess and compare performance across models. In this paper, we introduce the concept of capacity allocation analysis, with the aim of shedding some light on what network architectures focus their modelling capacity on, when used on a given task. We focus more particularly on spatial capacity allocation, which analyzes a posteriori the effective number of parameters that a given model has allocated for modelling dependencies on a given point or region in the input space, in linear settings. We use this framework to perform a quantitative comparison between some classical architectures on various synthetic tasks. Finally, we consider how capacity allocation might translate in non-linear settings.',\n",
       "  'Title: towards jointly optimal placement and delivery to code or not to code in wireless caching networks\\nAbstract: Coded caching techniques have received significant attention lately due to their provable gains in reducing the cost of data delivery in wireless networks. These gains, however, have only been demonstrated under the assumption of a free placement phase. This unrealistic assumption poses a significant limitation, especially in cases where aggressive placement strategies can lead to a significant transmission cost that may even be higher than the corresponding cost of the delivery phase. In this paper, we relax this assumption and propose a general caching framework that captures the transmission cost of the two phases, and hence, results in minimizing the overall rate of the caching network. We model the dynamic nature of the network through a cost structure that allows for varying the network architecture and cost per transmission, across the placement and delivery phases. We start with the scenario where the individual users have no limit on the available caching memory and characterize the jointly optimal solution as a function of the different parameters in our cost structure. Then, we characterize the effect of memory constraints on the optimal solution in certain special cases. Interestingly, our results identify regions where the uncoded caching scheme outperforms its coded counterpart. Further, coded caching is shown to offer performance gains only when the network architecture during the placement phase is different from that during the delivery phase.',\n",
       "  \"Title: situation aware pedestrian trajectory prediction with spatio temporal attention model\\nAbstract: Pedestrian trajectory prediction is essential for collision avoidance in autonomous driving and robot navigation. However, predicting a pedestrian's trajectory in crowded environments is non-trivial as it is influenced by other pedestrians' motion and static structures that are present in the scene. Such human-human and human-space interactions lead to non-linearities in the trajectories. In this paper, we present a new spatio-temporal graph based Long Short-Term Memory (LSTM) network for predicting pedestrian trajectory in crowded environments, which takes into account the interaction with static (physical objects) and dynamic (other pedestrians) elements in the scene. Our results are based on two widely-used datasets to demonstrate that the proposed method outperforms the state-of-the-art approaches in human trajectory prediction. In particular, our method leads to a reduction in Average Displacement Error (ADE) and Final Displacement Error (FDE) of up to 55% and 61% respectively over state-of-the-art approaches.\",\n",
       "  'Title: wikilinkgraphs a complete longitudinal and multi language dataset of the wikipedia link networks\\nAbstract: Wikipedia articles contain multiple links connecting a subject to other pages of the encyclopedia. In Wikipedia parlance, these links are called internal links or wikilinks. We present a complete dataset of the network of internal Wikipedia links for the $9$ largest language editions. The dataset contains yearly snapshots of the network and spans $17$ years, from the creation of Wikipedia in 2001 to March 1st, 2018. While previous work has mostly focused on the complete hyperlink graph which includes also links automatically generated by templates, we parsed each revision of each article to track links appearing in the main text. In this way we obtained a cleaner network, discarding more than half of the links and representing all and only the links intentionally added by editors. We describe in detail how the Wikipedia dumps have been processed and the challenges we have encountered, including the need to handle special pages such as redirects, i.e., alternative article titles. We present descriptive statistics of several snapshots of this network. Finally, we propose several research opportunities that can be explored using this new dataset.',\n",
       "  'Title: learning for dc opf classifying active sets using neural nets\\nAbstract: The optimal power flow is an optimization problem used in power systems operational planning to maximize economic efficiency while satisfying demand and maintaining safety margins. Due to uncertainty and variability in renewable energy generation and demand, the optimal solution needs to be updated in response to observed uncertainty realizations or near real-time forecast updates. To address the challenge of computing such frequent real-time updates to the optimal solution, recent literature has proposed the use of machine learning to learn the mapping between the uncertainty realization and the optimal solution. Further, learning the active set of constraints at optimality, as opposed to directly learning the optimal solution, has been shown to significantly simplify the machine learning task, and the learnt model can be used to predict optimal solutions in real-time. In this paper, we propose the use of classification algorithms to learn the mapping between the uncertainty realization and the active set of constraints at optimality, thus further enhancing the computational efficiency of the real-time prediction. We employ neural net classifiers for this task and demonstrate the excellent performance of this approach on a number of systems in the IEEE PES PGLib-OPF benchmark library.',\n",
       "  'Title: exploration of performance and energy trade offs for heterogeneous multicore architectures\\nAbstract: Energy-efficiency has become a major challenge in modern computer systems. To address this challenge, candidate systems increasingly integrate heterogeneous cores in order to satisfy diverse computation requirements by selecting cores with suitable features. In particular, single-ISA heterogeneous multicore processors such as ARM big.LITTLE have become very attractive since they offer good opportunities in terms of performance and power consumption trade-off. While existing works already showed that this feature can improve system energy-efficiency, further gains are possible by generalizing the principle to higher levels of heterogeneity. The present paper aims to explore these gains by considering single-ISA heterogeneous multicore architectures including three different types of cores. For this purpose, we use the Samsung Exynos Octa 5422 chip as baseline architecture. Then, we model and evaluate Cortex A7, A9, and A15 cores using the gem5 simulation framework coupled to McPAT for power estimation. We demonstrate that varying the level of heterogeneity as well as the different core ratio can lead to up to 2.3x gains in energy efficiency and up to 1.5x in performance. This study further provides insights on the impact of workload nature on performance/energy trade-off and draws recommendations concerning suitable architecture configurations. This contributes in fine to guide future research towards dynamically reconfigurable HSAs in which some cores/clusters can be disabled momentarily so as to optimize certain metrics such as energy efficiency. This is of particular interest when dealing with quality-tunable algorithms in which accuracy can be then traded for compute effort, thereby enabling to use only those cores that provide the best energy-efficiency for the chosen algorithm.',\n",
       "  'Title: robustness certificates against adversarial examples for relu networks\\nAbstract: While neural networks have achieved high performance in different learning tasks, their accuracy drops significantly in the presence of small adversarial perturbations to inputs. Defenses based on regularization and adversarial training are often followed by new attacks to defeat them. In this paper, we propose attack-agnostic robustness certificates for a multi-label classification problem using a deep ReLU network. Although computing the exact distance of a given input sample to the classification decision boundary requires solving a non-convex optimization, we characterize two lower bounds for such distances, namely the simplex certificate and the decision boundary certificate. These robustness certificates leverage the piece-wise linear structure of ReLU networks and use the fact that in a polyhedron around a given sample, the prediction function is linear. In particular, the proposed simplex certificate has a closed-form, is differentiable and is an order of magnitude faster to compute than the existing methods even for deep networks. In addition to theoretical bounds, we provide numerical results for our certificates over MNIST and compare them with some existing upper bounds.',\n",
       "  'Title: linear inequality constraints for neural network activations\\nAbstract: We propose a method to impose linear inequality constraints on neural network activations. The proposed method allows a data-driven training approach to be combined with modeling prior knowledge about the task. Our algorithm computes a suitable parameterization of the feasible set at initialization and uses standard variants of stochastic gradient descent to find solutions to the constrained network. Thus, the modeling constraints are always satisfied during training. Crucially, our approach avoids to solve a sub-optimization problem at each training step or to manually trade-off data and constraint fidelity with additional hyperparameters. We consider constrained generative modeling as an important application domain and experimentally demonstrate the proposed method by constraining a variational autoencoder.',\n",
       "  \"Title: synthesizing facial photometries and corresponding geometries using generative adversarial networks\\nAbstract: Artificial data synthesis is currently a well studied topic with useful applications in data science, computer vision, graphics and many other fields. Generating realistic data is especially challenging since human perception is highly sensitive to non realistic appearance. In recent times, new levels of realism have been achieved by advances in GAN training procedures and architectures. These successful models, however, are tuned mostly for use with regularly sampled data such as images, audio and video. Despite the successful application of the architecture on these types of media, applying the same tools to geometric data poses a far greater challenge. The study of geometric deep learning is still a debated issue within the academic community as the lack of intrinsic parametrization inherent to geometric objects prohibits the direct use of convolutional filters, a main building block of today's machine learning systems. In this paper we propose a new method for generating realistic human facial geometries coupled with overlayed textures. We circumvent the parametrization issue by imposing a global mapping from our data to the unit rectangle. We further discuss how to design such a mapping to control the mapping distortion and conserve area within the mapped image. By representing geometric textures and geometries as images, we are able to use advanced GAN methodologies to generate new geometries. We address the often neglected topic of relation between texture and geometry and propose to use this correlation to match between generated textures and their corresponding geometries. We offer a new method for training GAN models on partially corrupted data. Finally, we provide empirical evidence demonstrating our generative model's ability to produce examples of new identities independent from the training data while maintaining a high level of realism, two traits that are often at odds.\",\n",
       "  'Title: deep learning for bridge load capacity estimation in post disaster and conflict zones\\nAbstract: Many post-disaster and -conflict regions do not have sufficient data on their transportation infrastructure assets, hindering both mobility and reconstruction. In particular, as the number of aging and deteriorating bridges increase, it is necessary to quantify their load characteristics in order to inform maintenance and prevent failure. The load carrying capacity and the design load are considered as the main aspects of any civil structures. Human examination can be costly and slow when expertise is lacking in challenging scenarios. In this paper, we propose to employ deep learning as method to estimate the load carrying capacity from crowd sourced images. A new convolutional neural network architecture is trained on data from over 6000 bridges, which will benefit future research and applications. We tackle significant variations in the dataset (e.g. class interval, image completion, image colour) and quantify their impact on the prediction accuracy, precision, recall and F1 score. Finally, practical optimisation is performed by converting multiclass classification into binary classification to achieve a promising field use performance.',\n",
       "  \"Title: confidence trigger detection an approach to build real time tracking by detection system\\nAbstract: With deep learning based image analysis getting popular in recent years, a lot of multiple objects tracking applications are in demand. Some of these applications (e.g., surveillance camera, intelligent robotics, and autonomous driving) require the system runs in real-time. Though recent proposed methods reach fairly high accuracy, the speed is still slower than real-time application requirement. In order to increase tracking-by-detection system's speed for real-time tracking, we proposed confidence trigger detection (CTD) approach which uses confidence of tracker to decide when to trigger object detection. Using this approach, system can safely skip detection of images frames that objects barely move. We had studied the influence of different confidences in three popular detectors separately. Though we found trade-off between speed and accuracy, our approach reaches higher accuracy at given speed.\",\n",
       "  'Title: geogan a conditional gan with reconstruction and style loss to generate standard layer of maps from satellite images\\nAbstract: Automatically generating maps from satellite images is an important task. There is a body of literature which tries to address this challenge. We created a more expansive survey of the task by experimenting with different models and adding new loss functions to improve results. We created a database of pairs of satellite images and the corresponding map of the area. Our model translates the satellite image to the corresponding standard layer map image using three main model architectures: (i) a conditional Generative Adversarial Network (GAN) which compresses the images down to a learned embedding, (ii) a generator which is trained as a normalizing flow (RealNVP) model, and (iii) a conditional GAN where the generator translates via a series of convolutions to the standard layer of a map and the discriminator input is the concatenation of the real/generated map and the satellite image. Model (iii) was by far the most promising of three models. To improve the results we also added a reconstruction loss and style transfer loss in addition to the GAN losses. The third model architecture produced the best quality of sampled images. In contrast to the other generative model where evaluation of the model is a challenging problem. since we have access to the real map for a given satellite image, we are able to assign a quantitative metric to the quality of the generated images in addition to inspecting them visually. While we are continuing to work on increasing the accuracy of the model, one challenge has been the coarse resolution of the data which upper-bounds the quality of the results of our model. Nevertheless, as will be seen in the results, the generated map is more accurate in the features it produces since the generator architecture demands a pixel-wise image translation/pixel-wise coloring. A video presentation summarizing this paper is available at: this https URL',\n",
       "  \"Title: engineered self organization for resilient robot self assembly with minimal surprise\\nAbstract: In collective robotic systems, the automatic generation of controllers for complex tasks is still a challenging problem. Open-ended evolution of complex robot behaviors can be a possible solution whereby an intrinsic driver for pattern formation and self-organization may prove to be important. We implement such a driver in collective robot systems by evolving prediction networks as world models in pair with action-selection networks. Fitness is given for good predictions which causes a bias towards easily predictable environments and behaviors in the form of emergent patterns, that is, environments of minimal surprise. There is no task-dependent bias or any other explicit predetermination for the different qualities of the emerging patterns. A careful configuration of actions, sensor models, and the environment is required to stimulate the emergence of complex behaviors. We study self-assembly to increase the scenario's complexity for our minimal surprise approach and, at the same time, limit the complexity of our simulations to a grid world to manage the feasibility of this approach. We investigate the impact of different swarm densities and the shape of the environment on the emergent patterns. Furthermore, we study how evolution can be biased towards the emergence of desired patterns. We analyze the resilience of the resulting self-assembly behaviors by causing damages to the assembled pattern and observe the self-organized self-repair process. In summary, we evolved swarm behaviors for resilient self-assembly and successfully engineered self-organization in simulation. In future work, we plan to transfer our approach to a swarm of real robots.\",\n",
       "  'Title: on efficient optimal transport an analysis of greedy and accelerated mirror descent algorithms\\nAbstract: We provide theoretical analyses for two algorithms that solve the regularized optimal transport (OT) problem between two discrete probability measures with at most $n$ atoms. We show that a greedy variant of the classical Sinkhorn algorithm, known as the Greenkhorn algorithm, can be improved to $\\\\widetilde{\\\\mathcal{O}}\\\\left(\\\\frac{n^2}{\\\\varepsilon^2}\\\\right)$, improving on the best known complexity bound of $\\\\widetilde{\\\\mathcal{O}}\\\\left(\\\\frac{n^2}{\\\\varepsilon^3}\\\\right)$. Notably, this matches the best known complexity bound for the Sinkhorn algorithm and helps explain why the Greenkhorn algorithm can outperform the Sinkhorn algorithm in practice. Our proof technique, which is based on a primal-dual formulation and a novel upper bound for the dual solution, also leads to a new class of algorithms that we refer to as adaptive primal-dual accelerated mirror descent (APDAMD) algorithms. We prove that the complexity of these algorithms is $\\\\widetilde{\\\\mathcal{O}}\\\\left(\\\\frac{n^2\\\\gamma^{1/2}}{\\\\varepsilon}\\\\right)$, where $\\\\gamma>0$ refers to the inverse of the strong convexity module of Bregman divergence with respect to $\\\\left\\\\|\\\\cdot\\\\right\\\\|_\\\\infty$. This implies that the APDAMD algorithm is faster than the Sinkhorn and Greenkhorn algorithms in terms of $\\\\varepsilon$. Experimental results on synthetic and real datasets demonstrate the favorable performance of the Greenkhorn and APDAMD algorithms in practice.',\n",
       "  'Title: trust region guided proximal policy optimization\\nAbstract: Model-free reinforcement learning relies heavily on a safe yet exploratory policy search. Proximal policy optimization (PPO) is a prominent algorithm to address the safe search problem, by exploiting a heuristic clipping mechanism motivated by a theoretically-justified \"trust region\" guidance. However, we found that the clipping mechanism of PPO could lead to a lack of exploration issue. Based on this finding, we improve the original PPO with an adaptive clipping mechanism guided by a \"trust region\" criterion. Our method, termed as Trust Region-Guided PPO (TRPPO), improves PPO with more exploration and better sample efficiency, while maintains the safe search property and design simplicity of PPO. On several benchmark tasks, TRPPO significantly outperforms the original PPO and is competitive with several state-of-the-art methods.',\n",
       "  'Title: deep learning based motion planning for autonomous vehicle using spatiotemporal lstm network\\nAbstract: Motion Planning, as a fundamental technology of automatic navigation for the autonomous vehicle, is still an open challenging issue in the real-life traffic situation and is mostly applied by the model-based approaches. However, due to the complexity of the traffic situations and the uncertainty of the edge cases, it is hard to devise a general motion planning system for the autonomous vehicle. In this paper, we proposed a motion planning model based on deep learning (named as spatiotemporal LSTM network), which is able to generate a real-time reflection based on spatiotemporal information extraction. To be specific, the model based on spatiotemporal LSTM network has three main structure. Firstly, the Convolutional Long-short Term Memory (Conv-LSTM) is used to extract hidden features through sequential image data. Then, the 3D Convolutional Neural Network(3D-CNN) is applied to extract the spatiotemporal information from the multi-frame feature information. Finally, the fully connected neural networks are used to construct a control model for autonomous vehicle steering angle. The experiments demonstrated that the proposed method can generate a robust and accurate visual motion planning results for the autonomous vehicle.',\n",
       "  \"Title: automated quality control in image segmentation application to the uk biobank cardiac mr imaging study\\nAbstract: Background: The trend towards large-scale studies including population imaging poses new challenges in terms of quality control (QC). This is a particular issue when automatic processing tools, e.g. image segmentation methods, are employed to derive quantitative measures or biomarkers for later analyses. Manual inspection and visual QC of each segmentation isn't feasible at large scale. However, it's important to be able to automatically detect when a segmentation method fails so as to avoid inclusion of wrong measurements into subsequent analyses which could lead to incorrect conclusions. Methods: To overcome this challenge, we explore an approach for predicting segmentation quality based on Reverse Classification Accuracy, which enables us to discriminate between successful and failed segmentations on a per-cases basis. We validate this approach on a new, large-scale manually-annotated set of 4,800 cardiac magnetic resonance scans. We then apply our method to a large cohort of 7,250 cardiac MRI on which we have performed manual QC. Results: We report results used for predicting segmentation quality metrics including Dice Similarity Coefficient (DSC) and surface-distance measures. As initial validation, we present data for 400 scans demonstrating 99% accuracy for classifying low and high quality segmentations using predicted DSC scores. As further validation we show high correlation between real and predicted scores and 95% classification accuracy on 4,800 scans for which manual segmentations were available. We mimic real-world application of the method on 7,250 cardiac MRI where we show good agreement between predicted quality metrics and manual visual QC scores. Conclusions: We show that RCA has the potential for accurate and fully automatic segmentation QC on a per-case basis in the context of large-scale population imaging as in the UK Biobank Imaging Study.\",\n",
       "  'Title: busyhands a hand tool interaction database for assembly tasks semantic segmentation\\nAbstract: Visual segmentation has seen tremendous advancement recently with ready solutions for a wide variety of scene types, including human hands and other body parts. However, focus on segmentation of human hands while performing complex tasks, such as manual assembly, is still severely lacking. Segmenting hands from tools, work pieces, background and other body parts is extremely difficult because of self-occlusions and intricate hand grips and poses. In this paper we introduce BusyHands, a large open dataset of pixel-level annotated images of hands performing 13 different tool-based assembly tasks, from both real-world captures and virtual-world renderings. A total of 7906 samples are included in our first-in-kind dataset, with both RGB and depth images as obtained from a Kinect V2 camera and Blender. We evaluate several state-of-the-art semantic segmentation methods on our dataset as a proposed performance benchmark.',\n",
       "  'Title: towards real time eyeblink detection in the wild dataset theory and practices\\nAbstract: Effective and real-time eyeblink detection is of wide-range applications, such as deception detection, drive fatigue detection, face anti-spoofing, etc. Although numerous of efforts have already been paid, most of them focus on addressing the eyeblink detection problem under the constrained indoor conditions with the relative consistent subject and environment setup. Nevertheless, towards the practical applications eyeblink detection in the wild is more required, and of greater challenges. However, to our knowledge this has not been well studied before. In this paper, we shed the light to this research topic. A labelled eyeblink in the wild dataset (i.e., HUST-LEBW) of 673 eyeblink video samples (i.e., 381 positives, and 292 negatives) is first established by us. These samples are captured from the unconstrained movies, with the dramatic variation on human attribute, human pose, illumination condition, imaging configuration, etc. Then, we formulate eyeblink detection task as a spatial-temporal pattern recognition problem. After locating and tracking human eye using SeetaFace engine and KCF tracker respectively, a modified LSTM model able to capture the multi-scale temporal information is proposed to execute eyeblink verification. A feature extraction approach that reveals appearance and motion characteristics simultaneously is also proposed. The experiments on HUST-LEBW reveal the superiority and efficiency of our approach. It also verifies that, the existing eyeblink detection methods cannot achieve satisfactory performance in the wild.',\n",
       "  'Title: investigating generalisation in continuous deep reinforcement learning\\nAbstract: Deep Reinforcement Learning has shown great success in a variety of control tasks. However, it is unclear how close we are to the vision of putting Deep RL into practice to solve real world problems. In particular, common practice in the field is to train policies on largely deterministic simulators and to evaluate algorithms through training performance alone, without a train/test distinction to ensure models generalise and are not overfitted. Moreover, it is not standard practice to check for generalisation under domain shift, although robustness to such system change between training and testing would be necessary for real-world Deep RL control, for example, in robotics. In this paper we study these issues by first characterising the sources of uncertainty that provide generalisation challenges in Deep RL. We then provide a new benchmark and thorough empirical evaluation of generalisation challenges for state of the art Deep RL methods. In particular, we show that, if generalisation is the goal, then common practice of evaluating algorithms based on their training performance leads to the wrong conclusions about algorithm choice. Finally, we evaluate several techniques for improving generalisation and draw conclusions about the most robust techniques to date.',\n",
       "  'Title: robust re identification of manta rays from natural markings by learning pose invariant embeddings\\nAbstract: Visual identification of individual animals that bear unique natural body markings is an important task in wildlife conservation. The photo databases of animal markings grow larger and each new observation has to be matched against thousands of images. Existing photo-identification solutions have constraints on image quality and appearance of the pattern of interest in the image. These constraints limit the use of photos from citizen scientists. We present a novel system for visual re-identification based on unique natural markings that is robust to occlusions, viewpoint and illumination changes. We adapt methods developed for face re-identification and implement a deep convolutional neural network (CNN) to learn embeddings for images of natural markings. The distance between the learned embedding points provides a dissimilarity measure between the corresponding input images. The network is optimized using the triplet loss function and the online semi-hard triplet mining strategy. The proposed re-identification method is generic and not species specific. We evaluate the proposed system on image databases of manta ray belly patterns and humpback whale flukes. To be of practical value and adopted by marine biologists, a re-identification system needs to have a top-10 accuracy of at least 95%. The proposed system achieves this performance standard.',\n",
       "  'Title: coded distributed computing with heterogeneous function assignments\\nAbstract: Coded distributed computing (CDC) introduced by Li et. al. is an effective technique to trade computation load for communication load in a MapReduce framework. CDC achieves an optimal trade-off by duplicating map computations at $r$ computing nodes to yield multicasting opportunities such that $r$ nodes are served simultaneously in the Shuffle phase. However, in general, the state-of-the-art CDC scheme is mainly designed only for homogeneous networks, where the computing nodes are assumed to have the same storage, computation and communication capabilities. In this work, we explore two novel approaches of heterogeneous CDC design. First, we study CDC schemes which operate on multiple, collaborating homogeneous computing networks. Second, we allow heterogeneous function assignment in the CDC design, where nodes are assigned a varying number of reduce functions. Finally, we propose an expandable heterogeneous CDC scheme where $r-1$ nodes are served simultaneously in the Shuffle phase. In comparison to the state-of-the-art homogeneous CDC scheme with an equivalent computation load, we find our newly proposed heterogeneous CDC scheme has a smaller communication load in some cases.',\n",
       "  'Title: bacsoft a tool to archive data on bacteria\\nAbstract: Recently, DNA data storage systems have attracted many researchers worldwide. Motivated by the success stories of such systems, in this work we propose a software called BacSoft to clone the data in a bacterial plasmid by using the concept of genetic engineering. We consider the encoding schemes such that it satisfies constraints significant for bacterial data storage.',\n",
       "  'Title: representative task self selection for flexible clustered lifelong learning\\nAbstract: Consider the lifelong learning paradigm whose objective is to learn a sequence of tasks depending on previous experiences, e.g., knowledge library or deep network weights. However, the knowledge libraries or deep networks for most recent lifelong learning models are with prescribed size, and can degenerate the performance for both learned tasks and coming ones when facing with a new task environment (cluster). To address this challenge, we propose a novel incremental clustered lifelong learning framework with two knowledge libraries: feature learning library and model knowledge library, called Flexible Clustered Lifelong Learning (FCL3). Specifically, the feature learning library modeled by an autoencoder architecture maintains a set of representation common across all the observed tasks, and the model knowledge library can be self-selected by identifying and adding new representative models (clusters). When a new task arrives, our proposed FCL3 model firstly transfers knowledge from these libraries to encode the new task, i.e., effectively and selectively soft-assigning this new task to multiple representative models over feature learning library. Then, 1) the new task with a higher outlier probability will then be judged as a new representative, and used to redefine both feature learning library and representative models over time; or 2) the new task with lower outlier probability will only refine the feature learning library. For model optimization, we cast this lifelong learning problem as an alternating direction minimization problem as a new task comes. Finally, we evaluate the proposed framework by analyzing several multi-task datasets, and the experimental results demonstrate that our FCL3 model can achieve better performance than most lifelong learning frameworks, even batch clustered multi-task learning models.',\n",
       "  'Title: graph neural networks for modelling traffic participant interaction\\nAbstract: By interpreting a traffic scene as a graph of interacting vehicles, we gain a flexible abstract representation which allows us to apply Graph Neural Network (GNN) models for traffic prediction. These naturally take interaction between traffic participants into account while being computationally efficient and providing large model capacity. We evaluate two state-of-the art GNN architectures and introduce several adaptations for our specific scenario. We show that prediction error in scenarios with much interaction decreases by 30% compared to a model that does not take interactions into account. This suggests a graph interpretation of interacting traffic participants is a worthwhile addition to traffic prediction systems.',\n",
       "  'Title: rgbd based dimensional decomposition residual network for 3d semantic scene completion\\nAbstract: RGB images differentiate from depth images as they carry more details about the color and texture information, which can be utilized as a vital complementary to depth for boosting the performance of 3D semantic scene completion (SSC). SSC is composed of 3D shape completion (SC) and semantic scene labeling while most of the existing methods use depth as the sole input which causes the performance bottleneck. Moreover, the state-of-the-art methods employ 3D CNNs which have cumbersome networks and tremendous parameters. We introduce a light-weight Dimensional Decomposition Residual network (DDR) for 3D dense prediction tasks. The novel factorized convolution layer is effective for reducing the network parameters, and the proposed multi-scale fusion mechanism for depth and color image can improve the completion and segmentation accuracy simultaneously. Our method demonstrates excellent performance on two public datasets. Compared with the latest method SSCNet, we achieve 5.9% gains in SC-IoU and 5.7% gains in SSC-IOU, albeit with only 21% network parameters and 16.6% FLOPs employed compared with that of SSCNet.',\n",
       "  'Title: styleremix an interpretable representation for neural image style transfer\\nAbstract: Multi-Style Transfer (MST) intents to capture the high-level visual vocabulary of different styles and expresses these vocabularies in a joint model to transfer each specific style. Recently, Style Embedding Learning (SEL) based methods represent each style with an explicit set of parameters to perform MST task. However, most existing SEL methods either learn explicit style representation with numerous independent parameters or learn a relatively black-box style representation, which makes them difficult to control the stylized results. In this paper, we outline a novel MST model, StyleRemix, to compactly and explicitly integrate multiple styles into one network. By decomposing diverse styles into the same basis, StyleRemix represents a specific style in a continuous vector space with 1-dimensional coefficients. With the interpretable style representation, StyleRemix not only enables the style visualization task but also allows several ways of remixing styles in the smooth style embedding space.~Extensive experiments demonstrate the effectiveness of StyleRemix on various MST tasks compared to state-of-the-art SEL approaches.',\n",
       "  \"Title: offensive language analysis using deep learning architecture\\nAbstract: SemEval-2019 Task 6 (Zampieri et al., 2019b) requires us to identify and categorise offensive language in social media. In this paper we will describe the process we took to tackle this challenge. Our process is heavily inspired by Sosa (2017) where he proposed CNN-LSTM and LSTM-CNN models to conduct twitter sentiment analysis. We decided to follow his approach as well as further his work by testing out different variations of RNN models with CNN. Specifically, we have divided the challenge into two parts: data processing and sampling and choosing the optimal deep learning architecture. In preprocessing, we experimented with two techniques, SMOTE and Class Weights to counter the imbalance between classes. Once we are happy with the quality of our input data, we proceed to choosing the optimal deep learning architecture for this task. Given the quality and quantity of data we have been given, we found that the addition of CNN layer provides very little to no additional improvement to our model's performance and sometimes even lead to a decrease in our F1-score. In the end, the deep learning architecture that gives us the highest macro F1-score is a simple BiLSTM-CNN.\",\n",
       "  'Title: correct by construction control synthesis for buck converters with event triggered state measurement\\nAbstract: In this paper, we illustrate a new correct-by-construction switching controller for a power converter with event-triggered measurements. The event-triggered measurement scheme is beneficial for high frequency power converters because it requires relatively low-speed sampling hardware and is immune to unmodeled switching transients. While providing guarantees on the closed-loop system behavior is crucial in this application, off-the-shelf abstraction-based techniques cannot be directly employed to synthesize a controller in this setting because controller cannot always get instantaneous access to the current state. As a result, the switching action has to be based on slightly out-of-date measurements. To tackle this challenge, we introduce the out-of-date measurement as an extra state variable and project out the inaccessible real state to construct a belief space abstraction. The properties preserved by this belief space abstraction are analyzed. Finally, an abstraction-based synthesis method is applied to this abstraction. We demonstrate the controller on a constant on-time buck voltage regulator plant with an event-triggered sampler. The simulation verifies the effectiveness of our controller.',\n",
       "  \"Title: interacting spreading processes in multilayer networks\\nAbstract: The world of network science is fascinating and filled with complex phenomena that we aspire to understand. One of them is the dynamics of spreading processes over complex networked structures. Building the knowledge-base in the field where we can face more than one spreading process propagating over a network that has more than one layer is a challenging task where the complexity comes from both environments in which the spread happens and from characteristics and interplay of spreads' propagation. As the field has rapidly grown over the last decade, there is a need to comprehensively review the current state-of-the-art and offer to the research community a roadmap that helps to organise the future research in this area. Thus, this survey is a first attempt to present the current landscape of the multi-processes spread over multilayer networks and to suggest the potential ways forward.\",\n",
       "  'Title: botgraph web bot detection based on sitemap\\nAbstract: The web bots have been blamed for consuming large amount of Internet traffic and undermining the interest of the scraped sites for years. Traditional bot detection studies focus mainly on signature-based solution, but advanced bots usually forge their identities to bypass such detection. With increasing cloud migration, cloud providers provide new opportunities for an effective bot detection based on big data to solve this issue. In this paper, we present a behavior-based bot detection scheme called BotGraph that combines sitemap and convolutional neural network (CNN) to detect inner behavior of bots. Experimental results show that BotGraph achieves ~95% recall and precision on 35-day production data traces from different customers including the Bing search engine and several sites.',\n",
       "  'Title: neural network optimized 1 bit precoding for massive mu mimo\\nAbstract: Base station (BS) architectures for massive multi-user (MU) multiple-input multiple-output (MIMO) wireless systems are equipped with hundreds of antennas to serve tens of users on the same time-frequency channel. The immense number of BS antennas incurs high system costs, power, and interconnect bandwidth. To circumvent these obstacles, sophisticated MU precoding algorithms that enable the use of 1-bit DACs have been proposed. Many of these precoders feature parameters that are, traditionally, tuned manually to optimize their performance. We propose to use deep-learning tools to automatically tune such 1-bit precoders. Specifically, we optimize the biConvex 1-bit PrecOding (C2PO) algorithm using neural networks. Compared to the original C2PO algorithm, our neural-network optimized (NNO-)C2PO achieves the same error-rate performance at $\\\\bf 2\\\\boldsymbol\\\\times$ lower complexity. Moreover, by training NNO-C2PO for different channel models, we show that 1-bit precoding can be made robust to vastly changing propagation conditions.',\n",
       "  \"Title: a distributed observer for a discrete time linear system\\nAbstract: A simply structured distributed observer is described for estimating the state of a discrete-time, jointly observable, input-free, linear system whose sensed outputs are distributed across a time-varying network. It is explained how to construct the local estimators which comprise the observer so that their state estimation errors all converge exponentially fast to zero at a fixed, but arbitrarily chosen rate provided the network's graph is strongly connected for all time. This is accomplished by exploiting several well-known properties of invariant subspaces plus several kinds of suitably defined matrix norms.\",\n",
       "  'Title: a new approach for distributed hypothesis testing with extensions to byzantine resilience\\nAbstract: We study a setting where a group of agents, each receiving partially informative private observations, seek to collaboratively learn the true state (among a set of hypotheses) that explains their joint observation profiles over time. To solve this problem, we propose a distributed learning rule that differs fundamentally from existing approaches, in the sense, that it does not employ any form of \"belief-averaging\". Specifically, every agent maintains a local belief (on each hypothesis) that is updated in a Bayesian manner without any network influence, and an actual belief that is updated (up to normalization) as the minimum of its own local belief and the actual beliefs of its neighbors. Under minimal requirements on the signal structures of the agents and the underlying communication graph, we establish consistency of the proposed belief update rule, i.e., we show that the actual beliefs of the agents asymptotically concentrate on the true state almost surely. As one of the key benefits of our approach, we show that our learning rule can be extended to scenarios that capture misbehavior on the part of certain agents in the network, modeled via the Byzantine adversary model. In particular, we prove that each non-adversarial agent can asymptotically learn the true state of the world almost surely, under appropriate conditions on the observation model and the network topology.',\n",
       "  'Title: context constrained accurate contour extraction for occlusion edge detection\\nAbstract: Occlusion edge detection requires both accurate locations and context constraints of the contour. Existing CNN-based pipeline does not utilize adaptive methods to filter the noise introduced by low-level features. To address this dilemma, we propose a novel Context-constrained accurate Contour Extraction Network (CCENet). Spatial details are retained and contour-sensitive context is augmented through two extraction blocks, respectively. Then, an elaborately designed fusion module is available to integrate features, which plays a complementary role to restore details and remove clutter. Weight response of attention mechanism is eventually utilized to enhance occluded contours and suppress noise. The proposed CCENet significantly surpasses state-of-the-art methods on PIOD and BSDS ownership dataset of object edge detection and occlusion orientation detection.',\n",
       "  'Title: safety guided deep reinforcement learning via online gaussian process estimation\\nAbstract: An important facet of reinforcement learning (RL) has to do with how the agent goes about exploring the environment. Traditional exploration strategies typically focus on efficiency and ignore safety. However, for practical applications, ensuring safety of the agent during exploration is crucial since performing an unsafe action or reaching an unsafe state could result in irreversible damage to the agent. The main challenge of safe exploration is that characterizing the unsafe states and actions is difficult for large continuous state or action spaces and unknown environments. In this paper, we propose a novel approach to incorporate estimations of safety to guide exploration and policy search in deep reinforcement learning. By using a cost function to capture trajectory-based safety, our key idea is to formulate the state-action value function of this safety cost as a candidate Lyapunov function and extend control-theoretic results to approximate its derivative using online Gaussian Process (GP) estimation. We show how to use these statistical models to guide the agent in unknown environments to obtain high-performance control policies with provable stability certificates.',\n",
       "  'Title: trends challenges and adopted strategies in robocup home 2019 version\\nAbstract: Scientific competitions are crucial in the field of service robotics. They foster knowledge exchange and benchmarking, allowing teams to test their research in unstandardized scenarios. In this paper, we summarize the trending solutions and approaches used in RoboCup@Home. Further on, we discuss the attained achievements and challenges to overcome in relation with the progress required to fulfill the long-term goal of the league. Consequently, we propose a set of milestones for upcoming competitions by considering the current capabilities of the robots and their limitations. #R##N#With this work we aim at laying the foundations towards the creation of roadmaps that can help to direct efforts in testing and benchmarking in robotics competitions.',\n",
       "  \"Title: rescue a framework for real time feedback on behavioral cues using multimodal anomaly detection\\nAbstract: Executive coaching has been drawing more and more attention for developing corporate managers. While conversing with managers, coach practitioners are also required to understand internal states of coachees through objective observations. In this paper, we present REsCUE, an automated system to aid coach practitioners in detecting unconscious behaviors of their clients. Using an unsupervised anomaly detection algorithm applied to multimodal behavior data such as the subject's posture and gaze, REsCUE notifies behavioral cues for coaches via intuitive and interpretive feedback in real-time. Our evaluation with actual coaching scenes confirms that REsCUE provides the informative cues to understand internal states of coachees. Since REsCUE is based on the unsupervised method and does not assume any prior knowledge, further applications beside executive coaching are conceivable using our framework.\",\n",
       "  'Title: mixture of pre processing experts model for noise robust deep learning on resource constrained platforms\\nAbstract: Deep learning on an edge device requires energy efficient operation due to ever diminishing power budget. Intentional low quality data during the data acquisition for longer battery life, and natural noise from the low cost sensor degrade the quality of target output which hinders adoption of deep learning on an edge device. To overcome these problems, we propose simple yet efficient mixture of pre-processing experts (MoPE) model to handle various image distortions including low resolution and noisy images. We also propose to use adversarially trained auto encoder as a pre-processing expert for the noisy images. We evaluate our proposed method for various machine learning tasks including object detection on MS-COCO 2014 dataset, multiple object tracking problem on MOT-Challenge dataset, and human activity classification on UCF 101 dataset. Experimental results show that the proposed method achieves better detection, tracking and activity classification accuracies under noise without sacrificing accuracies for the clean images. The overheads of our proposed MoPE are 0.67% and 0.17% in terms of memory and computation compared to the baseline object detection network.',\n",
       "  'Title: knowledge adaptation for efficient semantic segmentation\\nAbstract: Both accuracy and efficiency are of significant importance to the task of semantic segmentation. Existing deep FCNs suffer from heavy computations due to a series of high-resolution feature maps for preserving the detailed knowledge in dense estimation. Although reducing the feature map resolution (i.e., applying a large overall stride) via subsampling operations (e.g., pooling and convolution striding) can instantly increase the efficiency, it dramatically decreases the estimation accuracy. To tackle this dilemma, we propose a knowledge distillation method tailored for semantic segmentation to improve the performance of the compact FCNs with large overall stride. To handle the inconsistency between the features of the student and teacher network, we optimize the feature similarity in a transferred latent domain formulated by utilizing a pre-trained autoencoder. Moreover, an affinity distillation module is proposed to capture the long-range dependency by calculating the non-local interactions across the whole image. To validate the effectiveness of our proposed method, extensive experiments have been conducted on three popular benchmarks: Pascal VOC, Cityscapes and Pascal Context. Built upon a highly competitive baseline, our proposed method can improve the performance of a student network by 2.5\\\\% (mIOU boosts from 70.2 to 72.7 on the cityscapes test set) and can train a better compact model with only 8\\\\% float operations (FLOPS) of a model that achieves comparable performances.',\n",
       "  'Title: towards motion invariant authentication for on body iot devices\\nAbstract: As the rapid proliferation of on-body Internet of Things (IoT) devices, their security vulnerabilities have raised serious privacy and safety issues. Traditional efforts to secure these devices against impersonation attacks mainly rely on either dedicated sensors or specified user motions, impeding their wide-scale adoption. This paper transcends these limitations with a general security solution by leveraging ubiquitous wireless chips available in IoT devices. Particularly, representative time and frequency features are first extracted from received signal strengths (RSSs) to characterize radio propagation profiles. Then, an adversarial multi-player network is developed to recognize underlying radio propagation patterns and facilitate on-body device authentication. We prove that at equilibrium, our adversarial model can extract all information about propagation patterns and eliminate any irrelevant information caused by motion variances. We build a prototype of our system using universal software radio peripheral (USRP) devices and conduct extensive experiments with both static and dynamic body motions in typical indoor and outdoor environments. The experimental results show that our system achieves an average authentication accuracy of 90.4%, with a high area under the receiver operating characteristic curve (AUROC) of 0.958 and better generalization performance in comparison with the conventional non-adversarial-based approach.',\n",
       "  'Title: learning matchable colorspace transformations for long term metric visual localization\\nAbstract: Long-term metric localization is an essential capability of autonomous mobile robots, but remains challenging for vision-based systems in the presence of appearance change caused by lighting, weather or seasonal variations. While experience-based mapping has proven to be an effective technique for enabling visual localization across appearance change, the number of experiences required for reliable long-term localization can be large, and methods for reducing the necessary number of experiences are desired. Taking inspiration from physics-based models of color constancy, we propose a method for learning a nonlinear mapping from RGB to grayscale colorspaces that maximizes the number of feature matches for images captured under varying lighting and weather conditions. Our key insight is that useful image transformations can be learned by approximating conventional non-differentiable localization pipelines with a differentiable learned model that can predict a convenient measure of localization quality, such as the number of feature matches, for a given pair of images. Moreover, we find that the generality of appearance-robust RGB-to-grayscale mappings can be improved by incorporating a learned low-dimensional context feature computed for a specific image pair. Using synthetic and real-world datasets, we show that our method substantially improves feature matching across day-night cycles and presents a viable strategy for significantly improving the efficiency of experience-based visual localization.',\n",
       "  'Title: rate optimal denoising with deep neural networks\\nAbstract: Deep neural networks provide state-of-the-art performance for image denoising, where the goal is to recover a near noise-free image from a noisy observation. The underlying principle is that neural networks trained on large datasets have empirically been shown to be able to generate natural images well from a low-dimensional latent representation of the image. Given such a generator network, a noisy image can be denoised by i) finding the closest image in the range of the generator or by ii) passing it through an encoder-generator architecture (known as an autoencoder). However, there is little theory to justify this success, let alone to predict the denoising performance as a function of the network parameters. In this paper we consider the problem of denoising an image from additive Gaussian noise using the two generator based approaches. In both cases, we assume the image is well described by a deep neural network with ReLU activations functions, mapping a $k$-dimensional code to an $n$-dimensional image. In the case of the autoencoder, we show that the feedforward network reduces noise energy by a factor of $O(k/n)$. In the case of optimizing over the range of a generative model, we state and analyze a simple gradient algorithm that minimizes a non-convex loss function, and provably reduces noise energy by a factor of $O(k/n)$. We also demonstrate in numerical experiments that this denoising performance is, indeed, achieved by generative priors learned from data.',\n",
       "  'Title: achieving greater concurrency in execution of smart contracts using object semantics\\nAbstract: Popular blockchain such as Ethereum and several others execute complex transactions in blocks through user defined scripts known as smart contracts. Normally, a block of the chain consists of multiple transactions of smart contracts which are added by a miner. To append a correct block into blockchain, miners execute these smart contract transactions (SCT) sequentially. Later the validators serially re-execute the SCT of the block. In the current era of multi-core processors, by employing serial execution of the transactions, the miners and validators fail to utilize the cores properly and as a result have poor throughput. By adding concurrency using object semantics to smart contracts execution, we can achieve the better efficiency and higher throughput. Some authors have used read-write STMs (RWSTMs) for the concurrent execution of SCT. Working with higher level operations provide greater concurrency, better throughput and reduces the number of aborts than RWSTMs. In this paper, we develop an efficient framework to execute the SCT concurrently by miner using optimistic Object-Based Software Transactional Memory systems (OSTMs) and Multi-Version OSTMs (MV-OSTM). A proposed block includes SCT, final states of the shared data-items, hash of the previous block and a block graph (BG). BG captures the conflicting relations among the transactions. Later, the validators re-execute the same SCT concurrently and deterministically with the help of BG given by miner to verify the final state. If the validation is successful then proposed block appended into the blockchain and miner gets incentive otherwise discard the proposed block. MV-OSTM and OSTM miner performs 4.5x and 3.86x average speedups over serial miner. Along with, MV-OSTM and OSTM validator outperforms average 32.81x and 29.76x than serial validator.',\n",
       "  \"Title: curls whey boosting black box adversarial attacks\\nAbstract: Image classifiers based on deep neural networks suffer from harassment caused by adversarial examples. Two defects exist in black-box iterative attacks that generate adversarial examples by incrementally adjusting the noise-adding direction for each step. On the one hand, existing iterative attacks add noises monotonically along the direction of gradient ascent, resulting in a lack of diversity and adaptability of the generated iterative trajectories. On the other hand, it is trivial to perform adversarial attack by adding excessive noises, but currently there is no refinement mechanism to squeeze redundant noises. In this work, we propose Curls & Whey black-box attack to fix the above two defects. During Curls iteration, by combining gradient ascent and descent, we `curl' up iterative trajectories to integrate more diversity and transferability into adversarial examples. Curls iteration also alleviates the diminishing marginal effect in existing iterative attacks. The Whey optimization further squeezes the `whey' of noises by exploiting the robustness of adversarial perturbation. Extensive experiments on Imagenet and Tiny-Imagenet demonstrate that our approach achieves impressive decrease on noise magnitude in l2 norm. Curls & Whey attack also shows promising transferability against ensemble models as well as adversarially trained models. In addition, we extend our attack to the targeted misclassification, effectively reducing the difficulty of targeted attacks under black-box condition.\",\n",
       "  'Title: an asymmetric adaptive scl decoder hardware for ultra low error rate polar codes\\nAbstract: In theory, Polar codes do not exhibit an error floor under successive-cancellation (SC) decoding. In practice, frame error rate (FER) down to $10^{-12}$ has not been reported with a real SC list (SCL) decoder hardware. This paper presents an asymmetric adaptive SCL (A2SCL) decoder, implemented in real hardware, for high-throughput and ultra-reliable communications. We propose to concatenate multiple SC decoders with an SCL decoder, in which the numbers of SC/SCL decoders are balanced with respect to their area and latency. In addition, a novel unequal-quantization technique is adopted. The two optimizations are crucial for improving SCL throughput within limited chip area. As an application, we build a link-level FPGA emulation platform to measure ultra-low FERs of 3GPP NR Polar codes (with parity-check and CRC bits). It is flexible to support all list sizes up to $8$, code lengths up to $1024$ and arbitrary code rates. With the proposed hardware, decoding speed is 7000 times faster than a CPU core. For the first time, FER as low as $10^{-12}$ is measured and quantization effect is analyzed.',\n",
       "  'Title: 360 panorama synthesis from a sparse set of images with unknown field of view\\nAbstract: 360 images represent scenes captured in all possible viewing directions and enable viewers to navigate freely around the scene thereby providing an immersive experience. Conversely, conventional images represent scenes in a single viewing direction with a small or limited field of view (FOV). As a result, only certain parts of the scenes are observed, and valuable information about the surroundings is lost. In this paper, a learning-based approach that reconstructs the scene in 360 x 180 from a sparse set of conventional images (typically 4 images) is proposed. The proposed approach first estimates the FOV of input images relative to the panorama. The estimated FOV is then used as the prior for synthesizing a high-resolution 360 panoramic output. The proposed method overcomes the difficulty of learning-based approach in synthesizing high resolution images (up to 512$\\\\times$1024). Experimental results demonstrate that the proposed method produces 360 panorama with reasonable quality. Results also show that the proposed method outperforms the alternative method and can be generalized for non-panoramic scenes and images captured by a smartphone camera.',\n",
       "  'Title: structural scaffolds for citation intent classification in scientific publications\\nAbstract: Identifying the intent of a citation in scientific papers (e.g., background information, use of methods, comparing results) is critical for machine reading of individual publications and automated analysis of the scientific literature. We propose structural scaffolds, a multitask model to incorporate structural information of scientific papers into citations for effective classification of citation intents. Our model achieves a new state-of-the-art on an existing ACL anthology dataset (ACL-ARC) with a 13.3% absolute increase in F1 score, without relying on external linguistic resources or hand-engineered features as done in existing methods. In addition, we introduce a new dataset of citation intents (SciCite) which is more than five times larger and covers multiple scientific domains compared with existing datasets. Our code and data are available at: this https URL.',\n",
       "  'Title: semantic nearest neighbor fields monocular edge visual odometry\\nAbstract: Recent advances in deep learning for edge detection and segmentation opens up a new path for semantic-edge-based ego-motion estimation. In this work, we propose a robust monocular visual odometry (VO) framework using category-aware semantic edges. It can reconstruct large-scale semantic maps in challenging outdoor environments. The core of our approach is a semantic nearest neighbor field that facilitates a robust data association of edges across frames using semantics. This significantly enlarges the convergence radius during tracking phases. The proposed edge registration method can be easily integrated into direct VO frameworks to estimate photometrically, geometrically, and semantically consistent camera motions. Different types of edges are evaluated and extensive experiments demonstrate that our proposed system outperforms state-of-art indirect, direct, and semantic monocular VO systems.',\n",
       "  'Title: occlusion guided compact template learning for ensemble deep network based pose invariant face recognition\\nAbstract: Concatenation of the deep network representations extracted from different facial patches helps to improve face recognition performance. However, the concatenated facial template increases in size and contains redundant information. Previous solutions aim to reduce the dimensionality of the facial template without considering the occlusion pattern of the facial patches. In this paper, we propose an occlusion-guided compact template learning (OGCTL) approach that only uses the information from visible patches to construct the compact template. The compact face representation is not sensitive to the number of patches that are used to construct the facial template and is more suitable for incorporating the information from different view angles for image-set based face recognition. Instead of using occlusion masks in face matching (e.g., DPRFS [38]), the proposed method uses occlusion masks in template construction and achieves significantly better image-set based face verification performance on a challenging database with a template size that is an order-of-magnitude smaller than DPRFS.',\n",
       "  'Title: proving tree algorithms for succinct data structures\\nAbstract: Succinct data structures give space-efficient representations of large amounts of data without sacrificing performance. They rely one cleverly designed data representations and algorithms. We present here the formalization in Coq/SSReflect of two different tree-based succinct representations and their accompanying algorithms. One is the Level-Order Unary Degree Sequence, which encodes the structure of a tree in breadth-first order as a sequence of bits, where access operations can be defined in terms of Rank and Select, which work in constant time for static bit sequences. The other represents dynamic bit sequences as binary balanced trees, where Rank and Select present a low logarithmic overhead compared to their static versions, and with efficient insertion and deletion. The two can be stacked to provide a dynamic representation of dictionaries for instance. While both representations are well-known, we believe this to be their first formalization and a needed step towards provably-safe implementations of big data.',\n",
       "  \"Title: unsupervised acoustic unit discovery for speech synthesis using discrete latent variable neural networks\\nAbstract: For our submission to the ZeroSpeech 2019 challenge, we apply discrete latent-variable neural networks to unlabelled speech and use the discovered units for speech synthesis. Unsupervised discrete subword modelling could be useful for studies of phonetic category learning in infants or in low-resource speech technology requiring symbolic input. We use an autoencoder (AE) architecture with intermediate discretisation. We decouple acoustic unit discovery from speaker modelling by conditioning the AE's decoder on the training speaker identity. At test time, unit discovery is performed on speech from an unseen speaker, followed by unit decoding conditioned on a known target speaker to obtain reconstructed filterbanks. This output is fed to a neural vocoder to synthesise speech in the target speaker's voice. For discretisation, categorical variational autoencoders (CatVAEs), vector-quantised VAEs (VQ-VAEs) and straight-through estimation are compared at different compression levels on two languages. Our final model uses convolutional encoding, VQ-VAE discretisation, deconvolutional decoding and an FFTNet vocoder. We show that decoupled speaker conditioning intrinsically improves discrete acoustic representations, yielding competitive synthesis quality compared to the challenge baseline.\",\n",
       "  'Title: learning discriminative model prediction for tracking\\nAbstract: The current strive towards end-to-end trainable computer vision systems imposes major challenges for the task of visual tracking. In contrast to most other vision problems, tracking requires the learning of a robust target-specific appearance model online, during the inference stage. To be end-to-end trainable, the online learning of the target model thus needs to be embedded in the tracking architecture itself. Due to these difficulties, the popular Siamese paradigm simply predicts a target feature template. However, such a model possesses limited discriminative power due to its inability of integrating background information. #R##N#We develop an end-to-end tracking architecture, capable of fully exploiting both target and background appearance information for target model prediction. Our architecture is derived from a discriminative learning loss by designing a dedicated optimization process that is capable of predicting a powerful model in only a few iterations. Furthermore, our approach is able to learn key aspects of the discriminative loss itself. The proposed tracker sets a new state-of-the-art on 6 tracking benchmarks, achieving an EAO score of 0.440 on VOT2018, while running at over 40 FPS.',\n",
       "  'Title: deepatlas joint semi supervised learning of image registration and segmentation\\nAbstract: Deep convolutional neural networks (CNNs) are state-of-the-art for semantic image segmentation, but typically require many labeled training samples. Obtaining 3D segmentations of medical images for supervised training is difficult and labor intensive. Motivated by classical approaches for joint segmentation and registration we therefore propose a deep learning framework that jointly learns networks for image registration and image segmentation. In contrast to previous work on deep unsupervised image registration, which showed the benefit of weak supervision via image segmentations, our approach can use existing segmentations when available and computes them via the segmentation network otherwise, thereby providing the same registration benefit. Conversely, segmentation network training benefits from the registration, which essentially provides a realistic form of data augmentation. Experiments on knee and brain 3D magnetic resonance (MR) images show that our approach achieves large simultaneous improvements of segmentation and registration accuracy (over independently trained networks) and allows training high-quality models with very limited training data. Specifically, in a one-shot-scenario (with only one manually labeled image) our approach increases Dice scores (%) over an unsupervised registration network by 2.7 and 1.8 on the knee and brain images respectively.',\n",
       "  'Title: trick or heat attack on amplification circuits to abuse critical temperature control systems\\nAbstract: Temperature sensors are extensively used in real-time monitoring and control of critical processes, such as maintaining thermal stability in incubators that treat low birth weight or sick newborns, or monitoring critical biological and chemical reactions. Therefore, compromising the data reliability of temperature sensors can lead to significant risks to safety-critical automated systems. In this paper, we show how an adversary can remotely spoof and maliciously control the measured temperature of infant incubators, which could lead to hyperthermia or hypothermia in newborns. The attack exploits the rectification effect of operational and differential amplifiers to generate controlled sensor outputs, tricking the internal control loop to heat up or cool down the cabin temperature without being detected by the automatic alarm system. We show how this attack is not limited to incubators, but affect several other critical and non-critical cyber-physical systems employing different temperature sensors, such as thermistors, RTDs, and thermocouples. Our results demonstrate how conventional shielding, filtering, and sensor redundancy techniques are not sufficient to eliminate the threat. So, we propose and implement a new anomaly detector for temperature-based critical applications to ensure the integrity of the temperature data.',\n",
       "  'Title: learning interpretable disentangled representations using adversarial vaes\\nAbstract: Learning Interpretable representation in medical applications is becoming essential for adopting data-driven models into clinical practice. It has been recently shown that learning a disentangled feature representation is important for a more compact and explainable representation of the data. In this paper, we introduce a novel adversarial variational autoencoder with a total correlation constraint to enforce independence on the latent representation while preserving the reconstruction fidelity. Our proposed method is validated on a publicly available dataset showing that the learned disentangled representation is not only interpretable, but also superior to the state-of-the-art methods. We report a relative improvement of 81.50% in terms of disentanglement, 11.60% in clustering, and 2% in supervised classification with a few amounts of labeled data.',\n",
       "  'Title: an efficient formula synthesis method with past signal temporal logic\\nAbstract: In this work, we propose a novel method to find temporal properties that lead to the unexpected behaviors from labeled dataset. We express these properties in past time Signal Temporal Logic (ptSTL). First, we present a novel approach for finding parameters of a template ptSTL formula, which extends the results on monotonicity based parameter synthesis. The proposed method optimizes a given monotone criteria while bounding an error. Then, we employ the parameter synthesis method in an iterative unguided formula synthesis framework. In particular, we combine optimized formulas iteratively to describe the causes of the labeled events while bounding the error. We illustrate the proposed framework on two examples.',\n",
       "  'Title: personalized sentence generation using generative adversarial networks with author specific word usage\\nAbstract: The author-specific word usage is a vital feature to let readers perceive the writing style of the author. In this work, a personalized sentence generation method based on generative adversarial networks (GANs) is proposed to cope with this issue. The frequently used function word and content word are incorporated not only as the input features but also as the sentence structure constraint for the GAN training. For the sentence generation with the related topics decided by the user, the Named Entity Recognition (NER) information of the input words is also used in the network training. We compared the proposed method with the GAN-based sentence generation methods, and the experimental results showed that the generated sentences using our method are more similar to the original sentences of the same author based on the objective evaluation such as BLEU and SimHash score.',\n",
       "  'Title: single pixel reconstruction for one stage instance segmentation\\nAbstract: Object instance segmentation is one of the most fundamental but challenging tasks in computer vision, and it requires the pixel-level image understanding. Most existing approaches address this problem by adding a mask prediction branch to a two-stage object detector with the Region Proposal Network (RPN). Although producing good segmentation results, the efficiency of these two-stage approaches is far from satisfactory, restricting their applicability in practice. In this paper, we propose a one-stage framework, SPRNet, which performs efficient instance segmentation by introducing a single pixel reconstruction (SPR) branch to off-the-shelf one-stage detectors. The added SPR branch reconstructs the pixel-level mask from every single pixel in the convolution feature map directly. Using the same ResNet-50 backbone, SPRNet achieves comparable mask AP to Mask R-CNN at a higher inference speed, and gains all-round improvements on box AP at every scale comparing with RetinaNet.',\n",
       "  'Title: hog feature extraction from encrypted images for privacy preserving machine learning\\nAbstract: In this paper, we propose an extraction method of HOG (histograms-of-oriented-gradients) features from encryption-then-compression (EtC) images for privacy-preserving machine learning, where EtC images are images encrypted by a block-based encryption method proposed for EtC systems with JPEG compression, and HOG is a feature descriptor used in computer vision for the purpose of object detection and image classification. Recently, cloud computing and machine learning have been spreading in many fields. However, the cloud computing has serious privacy issues for end users, due to unreliability of providers and some accidents. Accordingly, we propose a novel block-based extraction method of HOG features, and the proposed method enables us to carry out any machine learning algorithms without any influence, under some conditions. In an experiment, the proposed method is applied to a face image recognition problem under the use of two kinds of classifiers: linear support vector machine (SVM), gaussian SVM, to demonstrate the effectiveness.',\n",
       "  'Title: over the air computation via intelligent reflecting surfaces\\nAbstract: Over-the-air computation (AirComp) becomes a promising approach for fast wireless data aggregation via exploiting the superposition property in a multiple access channel. To further overcome the unfavorable signal propagation conditions for AirComp, in this paper, we propose an intelligent reflecting surface (IRS) aided AirComp system to build controllable wireless environments, thereby boosting the received signal power significantly. This is achieved by smartly tuning the phase shifts for the incoming electromagnetic waves at IRS, resulting in reconfigurable signal propagations. Unfortunately, it turns out that the joint design problem for AirComp transceivers and IRS phase shifts becomes a highly intractable nonconvex bi-quadratic programming problem, for which a novel alternating difference-of-convex (DC) programming algorithm is developed. This is achieved by providing a novel DC function representation for the rank-one constraint in the low-rank matrix optimization problem via matrix lifting. Simulation results demonstrate the algorithmic advantages and admirable performance of the proposed approaches compared with the state-of-art solutions.',\n",
       "  'Title: pan path integral based convolution for deep graph neural networks\\nAbstract: Convolution operations designed for graph-structured data usually utilize the graph Laplacian, which can be seen as message passing between the adjacent neighbors through a generic random walk. In this paper, we propose PAN, a new graph convolution framework that involves every path linking the message sender and receiver with learnable weights depending on the path length, which corresponds to the maximal entropy random walk. PAN generalizes the graph Laplacian to a new transition matrix we call \\\\emph{maximal entropy transition} (MET) matrix derived from a path integral formalism. Most previous graph convolutional network architectures can be adapted to our framework, and many variations and derivatives based on the path integral idea can be developed. Experimental results show that the path integral based graph neural networks have great learnability and fast convergence rate, and achieve state-of-the-art performance on benchmark tasks.',\n",
       "  'Title: fake news early detection a theory driven model\\nAbstract: The explosive growth of fake news and its erosion of democracy, justice, and public trust has significantly increased the demand for accurate fake news detection. Recent advancements in this area have proposed novel techniques that aim to detect fake news by exploring how it propagates on social networks. However, to achieve fake news early detection, one is only provided with limited to no information on news propagation; hence, motivating the need to develop approaches that can detect fake news by focusing mainly on news content. In this paper, a theory-driven model is proposed for fake news detection. The method investigates news content at various levels: lexicon-level, syntax-level, semantic-level and discourse-level. We represent news at each level, relying on well-established theories in social and forensic psychology. Fake news detection is then conducted within a supervised machine learning framework. As an interdisciplinary research, our work explores potential fake news patterns, enhances the interpretability in fake news feature engineering, and studies the relationships among fake news, deception/disinformation, and clickbaits. Experiments conducted on two real-world datasets indicate that the proposed method can outperform the state-of-the-art and enable fake news early detection, even when there is limited content information.',\n",
       "  'Title: cascademl an automatic neural network architecture evolution and training algorithm for multi label classification\\nAbstract: Multi-label classification is an approach which allows a datapoint to be labelled with more than one class at the same time. A common but trivial approach is to train individual binary classifiers per label, but the performance can be improved by considering associations within the labels. Like with any machine learning algorithm, hyperparameter tuning is important to train a good multi-label classifier model. The task of selecting the best hyperparameter settings for an algorithm is an optimisation problem. Very limited work has been done on automatic hyperparameter tuning and AutoML in the multi-label domain. This paper attempts to fill this gap by proposing a neural network algorithm, CascadeML, to train multi-label neural network based on cascade neural networks. This method requires minimal or no hyperparameter tuning and also considers pairwise label associations. The cascade algorithm grows the network architecture incrementally in a two phase process as it learns the weights using adaptive first order gradient algorithm, therefore omitting the requirement of preselecting the number of hidden layers, nodes and the learning rate. The method was tested on 10 multi-label datasets and compared with other multi-label classification algorithms. Results show that CascadeML performs very well without hyperparameter tuning.',\n",
       "  'Title: non rigid structure from motion by rank one basis shapes\\nAbstract: In this paper, we show that the affine, non-rigid structure-from-motion problem can be solved by rank-one, thus degenerate, basis shapes. It is a natural reformulation of the classic low-rank method by Bregler et al., where it was assumed that the deformable 3D structure is generated by a linear combination of rigid basis shapes. The non-rigid shape will be decomposed into the mean shape and the degenerate shapes, constructed from the right singular vectors of the low-rank decomposition. The right singular vectors are affinely back-projected into the 3D space, and the affine back-projections will also be solved as part of the factorisation. By construction, a direct interpretation for the right singular vectors of the low-rank decomposition will also follow: they can be seen as principal components, hence, the first variant of our method is referred to as Rank-1-PCA. The second variant, referred to as Rank-1-ICA, additionally estimates the orthogonal transform which maps the deformation modes into as statistically independent modes as possible. It has the advantage of pinpointing statistically dependent subspaces related to, for instance, lip movements on human faces. Moreover, in contrast to prior works, no predefined dimensionality for the subspaces is imposed. The experiments on several datasets show that the method achieves better results than the state-of-the-art, it can be computed faster, and it provides an intuitive interpretation for the deformation modes.',\n",
       "  'Title: deep ordinal reinforcement learning\\nAbstract: Reinforcement learning usually makes use of numerical rewards, which have nice properties but also come with drawbacks and difficulties. Using rewards on an ordinal scale (ordinal rewards) is an alternative to numerical rewards that has received more attention in recent years. In this paper, a general approach to adapting reinforcement learning problems to the use of ordinal rewards is presented and motivated. We show how to convert common reinforcement learning algorithms to an ordinal variation by the example of Q-learning and introduce Ordinal Deep Q-Networks, which adapt deep reinforcement learning to ordinal rewards. Additionally, we run evaluations on problems provided by the OpenAI Gym framework, showing that our ordinal variants exhibit a performance that is comparable to the numerical variations for a number of problems. We also give first evidence that our ordinal variant is able to produce better results for problems with less engineered and simpler-to-design reward signals.',\n",
       "  'Title: an efficient intelligent system for the classification of electroencephalography eeg brain signals using nuclear features for human cognitive tasks\\nAbstract: Representation and classification of Electroencephalography (EEG) brain signals are critical processes for their analysis in cognitive tasks. Particularly, extraction of discriminative features from raw EEG signals, without any pre-processing, is a challenging task. Motivated by nuclear norm, we observed that there is a significant difference between the variances of EEG signals captured from the same brain region when a subject performs different tasks. This observation lead us to use singular value decomposition for computing dominant variances of EEG signals captured from a certain brain region while performing a certain task and use them as features (nuclear features). A simple and efficient class means based minimum distance classifier (CMMDC) is enough to predict brain states. This approach results in the feature space of significantly small dimension and gives equally good classification results on clean as well as raw data. We validated the effectiveness and robustness of the technique using four datasets of different tasks: fluid intelligence clean data (FICD), fluid intelligence raw data (FIRD), memory recall task (MRT), and eyes open / eyes closed task (EOEC). For each task, we analyzed EEG signals over six (06) different brain regions with 8, 16, 20, 18, 18 and 100 electrodes. The nuclear features from frontal brain region gave the 100% prediction accuracy. The discriminant analysis of the nuclear features has been conducted using intra-class and inter-class variations. Comparisons with the state-of-the-art techniques showed the superiority of the proposed system.',\n",
       "  'Title: joint learning of self representation and indicator for multi view image clustering\\nAbstract: Multi-view subspace clustering aims to divide a set of multisource data into several groups according to their underlying subspace structure. Although the spectral clustering based methods achieve promotion in multi-view clustering, their utility is limited by the separate learning manner in which affinity matrix construction and cluster indicator estimation are isolated. In this paper, we propose to jointly learn the self-representation, continue and discrete cluster indicators in an unified model. Our model can explore the subspace structure of each view and fusion them to facilitate clustering simultaneously. Experimental results on two benchmark datasets demonstrate that our method outperforms other existing competitive multi-view clustering methods.',\n",
       "  'Title: accurate visual localization for automotive applications\\nAbstract: Accurate vehicle localization is a crucial step towards building effective Vehicle-to-Vehicle networks and automotive applications. Yet standard grade GPS data, such as that provided by mobile phones, is often noisy and exhibits significant localization errors in many urban areas. Approaches for accurate localization from imagery often rely on structure-based techniques, and thus are limited in scale and are expensive to compute. In this paper, we present a scalable visual localization approach geared for real-time performance. We propose a hybrid coarse-to-fine approach that leverages visual and GPS location cues. Our solution uses a self-supervised approach to learn a compact road image representation. This representation enables efficient visual retrieval and provides coarse localization cues, which are fused with vehicle ego-motion to obtain high accuracy location estimates. As a benchmark to evaluate the performance of our visual localization approach, we introduce a new large-scale driving dataset based on video and GPS data obtained from a large-scale network of connected dash-cams. Our experiments confirm that our approach is highly effective in challenging urban environments, reducing localization error by an order of magnitude.',\n",
       "  'Title: network coding gaps for completion times of multiple unicasts\\nAbstract: Arguably the most common network communication problem is multiple-unicasts: Distinct packets at different nodes in a network need to be delivered to a destination specific to each packet as fast as possible. #R##N#The famous multiple-unicast conjecture posits that, for this natural problem, there is no performance gap between routing and network coding, at least in terms of throughput. #R##N#We study the same network coding gap but in terms of completion time. While throughput corresponds to the completion time for asymptotically-large transmissions, we look at completion times of multiple unicasts for arbitrary amounts of data. We develop nearly-matching upper and lower bounds. In particular, we prove that the network coding gap for the completion time of $k$ unicasts is at most polylogarithmic in $k$, and there exist instances of $k$ unicasts for which this coding gap is polylogarithmic in $k$.',\n",
       "  'Title: a convolutional cost sensitive crack localization algorithm for automated and reliable rc bridge inspection\\nAbstract: Bridges are an essential part of the transportation infrastructure and need to be monitored periodically. Visual inspections by dedicated teams have been one of the primary tools in structural health monitoring (SHM) of bridge structures. However, such conventional methods have certain shortcomings. Manual inspections may be challenging in harsh environments and are commonly biased in nature. In the last decade, camera-equipped unmanned aerial vehicles (UAVs) have been widely used for visual inspections; however, the task of automatically extracting useful information from raw images is still challenging. In this paper, a deep learning semantic segmentation framework is proposed to automatically localize surface cracks. Due to the high imbalance of crack and background classes in images, different strategies are investigated to improve performance and reliability. The trained models are tested on real-world crack images showing impressive robustness in terms of the metrics defined by the concepts of precision and recall. These techniques can be used in SHM of bridges to extract useful information from the unprocessed images taken from UAVs.',\n",
       "  'Title: temporal attentive alignment for video domain adaptation\\nAbstract: Although various image-based domain adaptation (DA) techniques have been proposed in recent years, domain shift in videos is still not well-explored. Most previous works only evaluate performance on small-scale datasets which are saturated. Therefore, we first propose a larger-scale dataset with larger domain discrepancy: UCF-HMDB_full. Second, we investigate different DA integration methods for videos, and show that simultaneously aligning and learning temporal dynamics achieves effective alignment even without sophisticated DA methods. Finally, we propose Temporal Attentive Adversarial Adaptation Network (TA3N), which explicitly attends to the temporal dynamics using domain discrepancy for more effective domain alignment, achieving state-of-the-art performance on three video DA datasets. The code and data are released at this http URL.',\n",
       "  \"Title: on cycling risk and discomfort urban safety mapping and bike route recommendations\\nAbstract: Bike usage in Smart Cities becomes paramount for sustainable urban development. Cycling provides tremendous opportunities for a more healthy lifestyle, lower energy consumption and carbon emissions as well as reduction of traffic jams. While the number of cyclists increase along with the expansion of bike sharing initiatives and infrastructures, the number of bike accidents rises drastically threatening to jeopardize the bike urban movement. This paper studies cycling risk and discomfort using a diverse spectrum of data sources about geolocated bike accidents and their severity. Empirical continuous spatial risk estimations are calculated via kernel density contours that map safety in a case study of Zurich city. The role of weather, time, accident type and severity are illustrated. Given the predominance of self-caused accidents, an open-source software artifact for personalized route recommendations is introduced. The software is also used to collect open baseline route data that are compared with alternative ones that minimize risk or discomfort. These contributions can provide invaluable insights for urban planners to improve infrastructure. They can also improve the risk awareness of existing cyclists' as well as support new cyclists, such as tourists, to safely explore a new urban environment by bike.\",\n",
       "  'Title: using deep networks and transfer learning to address disinformation\\nAbstract: We apply an ensemble pipeline composed of a character-level convolutional neural network (CNN) and a long short-term memory (LSTM) as a general tool for addressing a range of disinformation problems. We also demonstrate the ability to use this architecture to transfer knowledge from labeled data in one domain to related (supervised and unsupervised) tasks. Character-level neural networks and transfer learning are particularly valuable tools in the disinformation space because of the messy nature of social media, lack of labeled data, and the multi-channel tactics of influence campaigns. We demonstrate their effectiveness in several tasks relevant for detecting disinformation: spam emails, review bombing, political sentiment, and conversation clustering.',\n",
       "  'Title: combine ppo with nes to improve exploration\\nAbstract: We introduce two approaches for combining neural evolution strategy (NES) and proximal policy optimization (PPO): parameter transfer and parameter space noise. Parameter transfer is a PPO agent with parameters transferred from a NES agent. Parameter space noise is to directly add noise to the PPO agent`s parameters. We demonstrate that PPO could benefit from both methods through experimental comparison on discrete action environments as well as continuous control tasks',\n",
       "  'Title: distributed pattern formation in a ring\\nAbstract: Motivated by concerns about diversity in social networks, we consider the following pattern formation problems in rings. Assume $n$ mobile agents are located at the nodes of an $n$-node ring network. Each agent is assigned a colour from the set $\\\\{c_1, c_2, \\\\ldots, c_q \\\\}$. The ring is divided into $k$ contiguous {\\\\em blocks} or neighbourhoods of length $p$. The agents are required to rearrange themselves in a distributed manner to satisfy given diversity requirements: in each block $j$ and for each colour $c_i$, there must be exactly $n_i(j) >0$ agents of colour $c_i$ in block $j$. Agents are assumed to be able to see agents in adjacent blocks, and move to any position in adjacent blocks in one time step. When the number of colours $q=2$, we give an algorithm that terminates in time $N_1/n^*_1 + k + 4$ where $N_1$ is the total number of agents of colour $c_1$ and $n^*_1$ is the minimum number of agents of colour $c_1$ required in any block. When the diversity requirements are the same in every block, our algorithm requires $3k+4$ steps, and is asymptotically optimal. Our algorithm generalizes for an arbitrary number of colours, and terminates in $O(nk)$ steps. We also show how to extend it to achieve arbitrary specific final patterns, provided there is at least one agent of every colour in every pattern.',\n",
       "  \"Title: a hypergraph based approach for the 4 constraint satisfaction problem tractability\\nAbstract: Constraint Satisfaction Problem (CSP) is a framework for modeling and solving a variety of real-world problems. Once the problem is expressed as a finite set of constraints, the goal is to find the variables' values satisfying them. Even though the problem is in general NP-complete, there are some approximation and practical techniques to tackle its intractability. One of the most widely used techniques is the Constraint Propagation. It consists in explicitly excluding values or combination of values for some variables whenever they make a given subset of constraints unsatisfied. In this paper, we deal with a CSP subclass which we call 4-CSP and whose constraint network infers relations of the form: $\\\\{ x \\\\sim \\\\alpha, x-y \\\\sim \\\\beta , (x-y) - (z-t) \\\\sim \\\\lambda \\\\}$, where $x, y, z$ and $t$ are real variables, $\\\\alpha , \\\\beta$ and $ \\\\lambda $ are real constants and $ \\\\sim \\\\in \\\\{\\\\leq , \\\\geq \\\\} $. The paper provides the first graph-based proofs of the 4-CSP tractability and elaborates algorithms for 4-CSP resolution based on the positive linear dependence theory, the hypergraph closure and the constraint propagation technique. Time and space complexities of the resolution algorithms are proved to be polynomial.\",\n",
       "  'Title: debiasing word embeddings improves multimodal machine translation\\nAbstract: In recent years, pretrained word embeddings have proved useful for multimodal neural machine translation (NMT) models to address the shortage of available datasets. However, the integration of pretrained word embeddings has not yet been explored extensively. Further, pretrained word embeddings in high dimensional spaces have been reported to suffer from the hubness problem. Although some debiasing techniques have been proposed to address this problem for other natural language processing tasks, they have seldom been studied for multimodal NMT models. In this study, we examine various kinds of word embeddings and introduce two debiasing techniques for three multimodal NMT models and two language pairs -- English-German translation and English-French translation. With our optimal settings, the overall performance of multimodal models was improved by up to +1.93 BLEU and +2.02 METEOR for English-German translation and +1.73 BLEU and +0.95 METEOR for English-French translation.',\n",
       "  'Title: fibinet combining feature importance and bilinear feature interaction for click through rate prediction\\nAbstract: Advertising and feed ranking are essential to many Internet companies such as Facebook and Sina Weibo. Among many real-world advertising and feed ranking systems, click through rate (CTR) prediction plays a central role. There are many proposed models in this field such as logistic regression, tree based models, factorization machine based models and deep learning based CTR models. However, many current works calculate the feature interactions in a simple way such as Hadamard product and inner product and they care less about the importance of features. In this paper, a new model named FiBiNET as an abbreviation for Feature Importance and Bilinear feature Interaction NETwork is proposed to dynamically learn the feature importance and fine-grained feature interactions. On the one hand, the FiBiNET can dynamically learn the importance of features via the Squeeze-Excitation network (SENET) mechanism; on the other hand, it is able to effectively learn the feature interactions via bilinear function. We conduct extensive experiments on two real-world datasets and show that our shallow model outperforms other shallow models such as factorization machine(FM) and field-aware factorization machine(FFM). In order to improve performance further, we combine a classical deep neural network(DNN) component with the shallow model to be a deep model. The deep FiBiNET consistently outperforms the other state-of-the-art deep models such as DeepFM and extreme deep factorization machine(XdeepFM).',\n",
       "  'Title: graph searches and their end vertices\\nAbstract: Graph search, the process of visiting vertices in a graph in a specific order, has demonstrated magical powers in many important algorithms. But a systematic study was only initiated by Corneil et al.~a decade ago, and only by then we started to realize how little we understand it. Even the apparently naive question \"which vertex can be the last visited by a graph search algorithm,\" known as the end vertex problem, turns out to be quite elusive. We give a full picture of all maximum cardinality searches on chordal graphs, which implies a polynomial-time algorithm for the end vertex problem of maximum cardinality search. It is complemented by a proof of NP-completeness of the same problem on weakly chordal graphs. #R##N#We also show linear-time algorithms for deciding end vertices of breadth-first searches on interval graphs, and end vertices of lexicographic depth-first searches on chordal graphs. Finally, we present $2^n\\\\cdot n^{O(1)}$-time algorithms for deciding the end vertices of breadth-first searches, depth-first searches, maximum cardinality searches, and maximum neighborhood searches on general graphs.',\n",
       "  'Title: a survey on the detection of android malicious apps\\nAbstract: Android-based smart devices are exponentially growing, and due to the ubiquity of the Internet, these devices are globally connected to the different devices/networks. Its popularity, attractive features, and mobility make malware creator to put number of malicious apps in the market to disrupt and annoy the victims. Although to identify the malicious apps, time-to-time various techniques are proposed. However, it appears that malware developers are always ahead of the anti-malware group, and the proposed techniques by the anti-malware groups are not sufficient to counter the advanced malicious apps. Therefore, to understand the various techniques proposed/used for the identification of Android malicious apps, in this paper, we present a survey conducted by us on the work done by the researchers in this field.',\n",
       "  'Title: parallel genetic algorithm for planning safe and optimal route for ship\\nAbstract: The paper represents an algorithm for planning safe and optimal routes for transport facilities with unrestricted movement direction that travel within areas with obstacles. Paper explains the algorithm using a ship as an example of such a transport facility. This paper also provides a survey of several existing solutions for the problem. The method employs an evolutionary algorithm to plan several locally optimal routes and a parallel genetic algorithm to create the final route by optimising the abovementioned set of routes. The routes are optimized against the arrival time, assuming that the optimal route is the route with the lowermost arrival time. It is also possible to apply additional restriction to the routes.',\n",
       "  \"Title: recurring concept meta learning for evolving data streams\\nAbstract: When concept drift is detected during classification in a data stream, a common remedy is to retrain a framework's classifier. However, this loses useful information if the classifier has learnt the current concept well, and this concept will recur again in the future. Some frameworks retain and reuse classifiers, but it can be time-consuming to select an appropriate classifier to reuse. These frameworks rarely match the accuracy of state-of-the-art ensemble approaches. For many data stream tasks, speed is important: fast, accurate frameworks are needed for time-dependent applications. We propose the Enhanced Concept Profiling Framework (ECPF), which aims to recognise recurring concepts and reuse a classifier trained previously, enabling accurate classification immediately following a drift. The novelty of ECPF is in how it uses similarity of classifications on new data, between a new classifier and existing classifiers, to quickly identify the best classifier to reuse. It always trains both a new classifier and a reused classifier, and retains the more accurate classifier when concept drift occurs. Finally, it creates a copy of reused classifiers, so a classifier well-suited for a recurring concept will not be impacted by being trained on a different concept. In our experiments, ECPF classifies significantly more accurately than a state-of-the-art classifier reuse framework (Diversity Pool) and a state-of-the-art ensemble technique (Adaptive Random Forest) on synthetic datasets with recurring concepts. It classifies real-world datasets five times faster than Diversity Pool, and six times faster than Adaptive Random Forest and is not significantly less accurate than either.\",\n",
       "  'Title: a cross domain transferable neural coherence model\\nAbstract: Coherence is an important aspect of text quality and is crucial for ensuring its readability. One important limitation of existing coherence models is that training on one domain does not easily generalize to unseen categories of text. Previous work advocates for generative models for cross-domain generalization, because for discriminative models, the space of incoherent sentence orderings to discriminate against during training is prohibitively large. In this work, we propose a local discriminative neural model with a much smaller negative sampling space that can efficiently learn against incorrect orderings. The proposed coherence model is simple in structure, yet it significantly outperforms previous state-of-art methods on a standard benchmark dataset on the Wall Street Journal corpus, as well as in multiple new challenging settings of transfer to unseen categories of discourse on Wikipedia articles.',\n",
       "  'Title: quantization loss re learning method\\nAbstract: In order to quantize the gate parameters of the LSTM (Long Short-Term Memory) neural network model with almost no recognition performance degraded, a new quantization method named Quantization Loss Re-Learn Method is proposed in this paper. The method does lossy quantization on gate parameters during training iterations, and the weight parameters learn to offset the loss of gate parameters quantization by adjusting the gradient in back propagation during weight parameters optimization. We proved the effectiveness of this method through theoretical derivation and experiments. The gate parameters had been quantized to 0, 0.5, 1 three values, and on the Named Entity Recognition dataset, the F1 score of the model with the new quantization method on gate parameters decreased by only 0.7% compared to the baseline model.',\n",
       "  'Title: a gender analysis of top scientists collaboration behavior evidence from italy\\nAbstract: This work analyzes the differences in collaboration behavior between males and females among a particular type of scholars: top scientists, and as compared to non top scientists. The field of observation consists of the Italian academic system and the co-authorships of scientific publications by 11,145 professors. The results obtained from a cross-sectional analysis covering the 5-year period 2006–2010 show that there are no significant differences in the overall propensity to collaborate in the top scientists of the two genders. At the level of single disciplines there are no differences in collaboration behavior, except in the case of: (1) international collaborations, for mathematics and chemistry—where the propensity for collaboration is greater for males; and (2) extramural domestic collaborations in physics, in which it is the females that show greater propensity for collaboration. Because international collaboration is positively correlated to research performance, findings can inform science policy aimed at increasing the representation of female top performers.',\n",
       "  \"Title: task classification model for visual fixation exploration and search\\nAbstract: Yarbus' claim to decode the observer's task from eye movements has received mixed reactions. In this paper, we have supported the hypothesis that it is possible to decode the task. We conducted an exploratory analysis on the dataset by projecting features and data points into a scatter plot to visualize the nuance properties for each task. Following this analysis, we eliminated highly correlated features before training an SVM and Ada Boosting classifier to predict the tasks from this filtered eye movements data. We achieve an accuracy of 95.4% on this task classification problem and hence, support the hypothesis that task classification is possible from a user's eye movement data.\",\n",
       "  'Title: temporally coherent full 3d mesh human pose recovery from monocular video\\nAbstract: Advances in Deep Learning have recently made it possible to recover full 3D meshes of human poses from individual images. However, extension of this notion to videos for recovering temporally coherent poses still remains unexplored. A major challenge in this regard is the lack of appropriately annotated video data for learning the desired deep models. Existing human pose datasets only provide 2D or 3D skeleton joint annotations, whereas the datasets are also recorded in constrained environments. We first contribute a technique to synthesize monocular action videos with rich 3D annotations that are suitable for learning computational models for full mesh 3D human pose recovery. Compared to the existing methods which simply \"texture-map\" clothes onto the 3D human pose models, our approach incorporates Physics based realistic cloth deformations with the human body movements. The generated videos cover a large variety of human actions, poses, and visual appearances, whereas the annotations record accurate human pose dynamics and human body surface information. Our second major contribution is an end-to-end trainable Recurrent Neural Network for full pose mesh recovery from monocular video. Using the proposed video data and LSTM based recurrent structure, our network explicitly learns to model the temporal coherence in videos and imposes geometric consistency over the recovered meshes. We establish the effectiveness of the proposed model with quantitative and qualitative analysis using the proposed and benchmark datasets.',\n",
       "  'Title: a gram gauss newton method learning overparameterized deep neural networks for regression problems\\nAbstract: First-order methods such as stochastic gradient descent (SGD) are currently the standard algorithm for training deep neural networks. Second-order methods, despite their better convergence rate, are rarely used in practice due to the prohibitive computational cost in calculating the second-order information. In this paper, we propose a novel Gram-Gauss-Newton (GGN) algorithm to train deep neural networks for regression problems with square loss. Our method draws inspiration from the connection between neural network optimization and kernel regression of neural tangent kernel (NTK). Different from typical second-order methods that have heavy computational cost in each iteration, GGN only has minor overhead compared to first-order methods such as SGD. We also give theoretical results to show that for sufficiently wide neural networks, the convergence rate of GGN is \\\\emph{quadratic}. Furthermore, we provide convergence guarantee for mini-batch GGN algorithm, which is, to our knowledge, the first convergence result for the mini-batch version of a second-order method on overparameterized neural networks. Preliminary experiments on regression tasks demonstrate that for training standard networks, our GGN algorithm converges much faster and achieves better performance than SGD.',\n",
       "  'Title: coset a benchmark for evaluating neural program embeddings\\nAbstract: Neural program embedding can be helpful in analyzing large software, a task that is challenging for traditional logic-based program analyses due to their limited scalability. A key focus of recent machine-learning advances in this area is on modeling program semantics instead of just syntax. Unfortunately evaluating such advances is not obvious, as program semantics does not lend itself to straightforward metrics. In this paper, we introduce a benchmarking framework called COSET for standardizing the evaluation of neural program embeddings. COSET consists of a diverse dataset of programs in source-code format, labeled by human experts according to a number of program properties of interest. A point of novelty is a suite of program transformations included in COSET. These transformations when applied to the base dataset can simulate natural changes to program code due to optimization and refactoring and can serve as a \"debugging\" tool for classification mistakes. We conducted a pilot study on four prominent models: TreeLSTM, gated graph neural network (GGNN), AST-Path neural network (APNN), and DYPRO. We found that COSET is useful in identifying the strengths and limitations of each model and in pinpointing specific syntactic and semantic characteristics of programs that pose challenges.',\n",
       "  \"Title: inverse boosting pruning trees for depression detection on twitter\\nAbstract: Depression is one of the most common mental health disorders, and a large number of depression people commit suicide each year. Potential depression sufferers do not consult psychological doctors because they feel ashamed or are unaware of any depression, which may result in severe delay of diagnosis and treatment. In the meantime, evidence shows that social media data provides valuable clues about physical and mental health conditions. In this paper, we argue that it is feasible to identify depression at an early stage by mining online social behaviours. Our approach, which is innovative to the practice of depression detection, does not rely on the extraction of numerous or complicated features to achieve accurate depression detection. Instead, we propose a novel classifier, namely, Inverse Boosting Pruning Trees (IBPT), which demonstrates a strong classification ability on a publicly accessible dataset with 7862 Twitter users. To comprehensively evaluate the classification capability of the IBPT, we use three real datasets from the UCI machine learning repository and the IBPT still obtains the best classification results against several state of the arts techniques. The results manifest that our proposed framework is promising for identifying social networks' users with depression.\",\n",
       "  \"Title: quantifying point prediction uncertainty in neural networks via residual estimation with an i o kernel\\nAbstract: Neural Networks (NNs) have been extensively used for a wide spectrum of real-world regression tasks, where the goal is to predict a numerical outcome such as revenue, effectiveness, or a quantitative result. In many such tasks, the point prediction is not enough: the uncertainty (i.e. risk or confidence) of that prediction must also be estimated. Standard NNs, which are most often used in such tasks, do not provide uncertainty information. Existing approaches address this issue by combining Bayesian models with NNs, but these models are hard to implement, more expensive to train, and usually do not predict as accurately as standard NNs. In this paper, a new framework (RIO) is developed that makes it possible to estimate uncertainty in any pretrained standard NN. The behavior of the NN is captured by modeling its prediction residuals with a Gaussian Process, whose kernel includes both the NN's input and its output. The framework is justified theoretically and evaluated in twelve real-world datasets, where it is found to (1) provide reliable estimates of uncertainty, (2) reduce the error of the point predictions, and (3) scale well to large datasets. Given that RIO can be applied to any standard NN without modifications to model architecture or training pipeline, it provides an important ingredient for building real-world NN applications.\",\n",
       "  'Title: understanding overfitting peaks in generalization error analytical risk curves for l 2 and l 1 penalized interpolation\\nAbstract: Traditionally in regression one minimizes the number of fitting parameters or uses smoothing/regularization to trade training (TE) and generalization error (GE). Driving TE to zero by increasing fitting degrees of freedom (dof) is expected to increase GE. However modern big-data approaches, including deep nets, seem to over-parametrize and send TE to zero (data interpolation) without impacting GE. Overparametrization has the benefit that global minima of the empirical loss function proliferate and become easier to find. These phenomena have drawn theoretical attention. Regression and classification algorithms have been shown that interpolate data but also generalize optimally. An interesting related phenomenon has been noted: the existence of non-monotonic risk curves, with a peak in GE with increasing dof. It was suggested that this peak separates a classical regime from a modern regime where over-parametrization improves performance. Similar over-fitting peaks were reported previously (statistical physics approach to learning) and attributed to increased fitting model flexibility. We introduce a generative and fitting model pair (\"Misparametrized Sparse Regression\" or MiSpaR) and show that the overfitting peak can be dissociated from the point at which the fitting function gains enough dof\\'s to match the data generative model and thus provides good generalization. This complicates the interpretation of overfitting peaks as separating a \"classical\" from a \"modern\" regime. Data interpolation itself cannot guarantee good generalization: we need to study the interpolation with different penalty terms. We present analytical formulae for GE curves for MiSpaR with $l_2$ and $l_1$ penalties, in the interpolating limit $\\\\lambda\\\\rightarrow 0$.These risk curves exhibit important differences and help elucidate the underlying phenomena.',\n",
       "  'Title: object agnostic suction grasp affordance detection in dense cluster using self supervised learning docx\\nAbstract: In this paper we study grasp problem in dense cluster, a challenging task in warehouse logistics scenario. By introducing a two-step robust suction affordance detection method, we focus on using vacuum suction pad to clear up a box filled with seen and unseen objects. Two CNN based neural networks are proposed. A Fast Region Estimation Network (FRE-Net) predicts which region contains pickable objects, and a Suction Grasp Point Affordance network (SGPA-Net) determines which point in that region is pickable. So as to enable such two networks, we design a self-supervised learning pipeline to accumulate data, train and test the performance of our method. In both virtual and real environment, within 1500 picks (~5 hours), we reach a picking accuracy of 95% for known objects and 90% for unseen objects with similar geometry features.',\n",
       "  'Title: episodic memory in lifelong language learning\\nAbstract: We introduce a lifelong language learning setup where a model needs to learn from a stream of text examples without any dataset identifier. We propose an episodic memory model that performs sparse experience replay and local adaptation to mitigate catastrophic forgetting in this setup. Experiments on text classification and question answering demonstrate the complementary benefits of sparse experience replay and local adaptation to allow the model to continuously learn from new datasets. We also show that the space complexity of the episodic memory module can be reduced significantly (~50-90%) by randomly choosing which examples to store in memory with a minimal decrease in performance. We consider an episodic memory component as a crucial building block of general linguistic intelligence and see our model as a first step in that direction.',\n",
       "  'Title: fast hierarchical neural network for feature learning on point cloud\\nAbstract: The analyses relying on 3D point clouds are an utterly complex task, often involving million of points, but also requiring computationally efficient algorithms because of many real-time applications; e.g. autonomous vehicle. However, point clouds are intrinsically irregular and the points are sparsely distributed in a non-Euclidean space, which normally requires point-wise processing to achieve high performances. Although shared filter matrices and pooling layers in convolutional neural networks (CNNs) are capable of reducing the dimensionality of the problem and extracting high-level information simultaneously, grids and highly regular data format are required as input. In order to balance model performance and complexity, we introduce a novel neural network architecture exploiting local features from a manually subsampled point set. In our network, a recursive farthest point sampling method is firstly applied to efficiently cover the entire point set. Successively, we employ the k-nearest neighbours (knn) algorithm to gather local neighbourhood for each group of the subsampled points. Finally, a multiple layer perceptron (MLP) is applied on the subsampled points and edges that connect corresponding point and neighbours to extract local features. The architecture has been tested for both shape classification and segmentation using the ModelNet40 and ShapeNet part datasets, in order to show that the network achieves the best trade-off in terms of competitive performance when compared to other state-of-the-art algorithms.',\n",
       "  'Title: reducing the variance in online optimization by transporting past gradients\\nAbstract: Most stochastic optimization methods use gradients once before discarding them. While variance reduction methods have shown that reusing past gradients can be beneficial when there is a finite number of datapoints, they do not easily extend to the online setting. One issue is the staleness due to using past gradients. We propose to correct this staleness using the idea of implicit gradient transport (IGT) which transforms gradients computed at previous iterates into gradients evaluated at the current iterate without using the Hessian explicitly. In addition to reducing the variance and bias of our updates over time, IGT can be used as a drop-in replacement for the gradient estimate in a number of well-understood methods such as heavy ball or Adam. We show experimentally that it achieves state-of-the-art results on a wide range of architectures and benchmarks. Additionally, the IGT gradient estimator yields the optimal asymptotic convergence rate for online stochastic optimization in the restricted setting where the Hessians of all component functions are equal.',\n",
       "  'Title: unsupervised feature learning with k means and an ensemble of deep convolutional neural networks for medical image classification\\nAbstract: Medical image analysis using supervised deep learning methods remains problematic because of the reliance of deep learning methods on large amounts of labelled training data. Although medical imaging data repositories continue to expand there has not been a commensurate increase in the amount of annotated data. Hence, we propose a new unsupervised feature learning method that learns feature representations to then differentiate dissimilar medical images using an ensemble of different convolutional neural networks (CNNs) and K-means clustering. It jointly learns feature representations and clustering assignments in an end-to-end fashion. We tested our approach on a public medical dataset and show its accuracy was better than state-of-the-art unsupervised feature learning methods and comparable to state-of-the-art supervised CNNs. Our findings suggest that our method could be used to tackle the issue of the large volume of unlabelled data in medical imaging repositories.',\n",
       "  \"Title: crypto art a decentralized view\\nAbstract: This is a decentralized position paper on crypto art, which includes viewpoints from different actors of the system: artists, collectors, galleries, art scholars, data scientists. The writing process went as follows: a general definition of the topic was put forward by two of the authors (Franceschet and Colavizza), and used as reference to ask to a set of diverse authors to contribute with their viewpoints asynchronously and independently. No guidelines were offered before the first draft, if not to reach a minimum of words to justify a separate section/contribution. Afterwards, all authors read and commented on each other's work and minimal editing was done. Every author was asked to suggest open questions and future perspectives on the topic of crypto art from their vantage point, while keeping full control of their own sections at all times. While this process does not necessarily guarantee the uniformity expected from, say, a research article, it allows for multiple voices to emerge and provide for a contribution on a common topic. The ending section offers an attempt to pull all these threads together into a perspective on the future of crypto art.\",\n",
       "  'Title: swim a semantic wiki for mathematical knowledge management\\nAbstract: SWiM is a semantic wiki for collaboratively building, editing and browsing mathematical knowledge represented in the domain-specific structural semantic markup language OMDoc. It motivates users to contribute to collections of mathematical knowledge by instantly sharing the benefits of knowledge-powered services with them. SWiM is currently being used for authoring content dictionaries, i. e. collections of uniquely identified mathematical symbols, and prepared for managing a large-scale proof formalisation effort.',\n",
       "  'Title: distant supervision relation extraction with intra bag and inter bag attentions\\nAbstract: This paper presents a neural relation extraction method to deal with the noisy training data generated by distant supervision. Previous studies mainly focus on sentence-level de-noising by designing neural networks with intra-bag attentions. In this paper, both intra-bag and inter-bag attentions are considered in order to deal with the noise at sentence-level and bag-level respectively. First, relation-aware bag representations are calculated by weighting sentence embeddings using intra-bag attentions. Here, each possible relation is utilized as the query for attention calculation instead of only using the target relation in conventional methods. Furthermore, the representation of a group of bags in the training set which share the same relation label is calculated by weighting bag representations using a similarity-based inter-bag attention module. Finally, a bag group is utilized as a training sample when building our relation extractor. Experimental results on the New York Times dataset demonstrate the effectiveness of our proposed intra-bag and inter-bag attention modules. Our method also achieves better relation extraction accuracy than state-of-the-art methods on this dataset.',\n",
       "  'Title: divide and grow capturing huge diversity in crowd images with incrementally growing cnn\\nAbstract: Automated counting of people in crowd images is a challenging task. The major difficulty stems from the large diversity in the way people appear in crowds. In fact, features available for crowd discrimination largely depend on the crowd density to the extent that people are only seen as blobs in a highly dense scene. We tackle this problem with a growing CNN which can progressively increase its capacity to account for the wide variability seen in crowd scenes. Our model starts from a base CNN density regressor, which is trained in equivalence on all types of crowd images. In order to adapt with the huge diversity, we create two child regressors which are exact copies of the base CNN. A differential training procedure divides the dataset into two clusters and fine-tunes the child networks on their respective specialties. Consequently, without any hand-crafted criteria for forming specialties, the child regressors become experts on certain types of crowds. The child networks are again split recursively, creating two experts at every division. This hierarchical training leads to a CNN tree, where the child regressors are more fine experts than any of their parents. The leaf nodes are taken as the final experts and a classifier network is then trained to predict the correct specialty for a given test image patch. The proposed model achieves higher count accuracy on major crowd datasets. Further, we analyse the characteristics of specialties mined automatically by our method.',\n",
       "  'Title: pain intensity estimation by a self taught selection of histograms of topographical features\\nAbstract: Pain assessment through observational pain scales is necessary for special categories of patients such as neonates, patients with dementia, critically ill patients, etc. The recently introduced Prkachin-Solomon score allows pain assessment directly from facial images opening the path for multiple assistive applications. In this paper, we introduce the Histograms of Topographical (HoT) features, which are a generalization of the topographical primal sketch, for the description of the face parts contributing to the mentioned score. We propose a semi-supervised, clustering oriented self--taught learning procedure developed on the emotion oriented Cohn-Kanade database. We use this procedure to improve the discrimination between different pain intensity levels and the generalization with respect to the monitored persons, while testing on the UNBC McMaster Shoulder Pain database.',\n",
       "  'Title: group evolution discovery in social networks\\nAbstract: Group extraction and their evolution are among the topics which arouse the greatest interest in the domain of social network analysis. However, while the grouping methods in social networks are developed very dynamically, the methods of group evolution discovery and analysis are still uncharted territory on the social network analysis map. Therefore the new method for the group evolution discovery called GED is proposed in this paper. Additionally, the results of the first experiments on the email based social network together with comparison with two other methods of group evolution discovery are presented.',\n",
       "  \"Title: the greedy spanner is existentially optimal\\nAbstract: The greedy spanner is arguably the simplest and most well-studied spanner construction. Experimental results demonstrate that it is at least as good as any other spanner construction, in terms of both the size and weight parameters. However, a rigorous proof for this statement has remained elusive. #R##N#In this work we fill in the theoretical gap via a surprisingly simple observation: The greedy spanner is \\\\emph{existentially optimal} (or existentially near-optimal) for several important graph families. Focusing on the weight parameter, the state-of-the-art spanner constructions for both general graphs (due to Chechik and Wulff-Nilsen [SODA'16]) and doubling metrics (due to Gottlieb [FOCS'15]) are complex. Plugging our observation on these results, we conclude that the greedy spanner achieves near-optimal weight guarantees for both general graphs and doubling metrics, thus resolving two longstanding conjectures in the area. #R##N#Further, we observe that approximate-greedy algorithms are existentially near-optimal as well. Consequently, we provide an $O(n \\\\log n)$-time construction of $(1+\\\\epsilon)$-spanners for doubling metrics with constant lightness and degree. Our construction improves Gottlieb's construction, whose runtime is $O(n \\\\log^2 n)$ and whose number of edges and degree are unbounded, and remarkably, it matches the state-of-the-art Euclidean result (due to Gudmundsson et al. [SICOMP'02]) in all the involved parameters (up to dependencies on $\\\\epsilon$ and the dimension).\",\n",
       "  'Title: binary decision diagrams for bin packing with minimum color fragmentation\\nAbstract: Bin Packing with Minimum Color Fragmentation (BPMCF) is an extension of the Bin Packing Problem in which each item has a size and a color and the goal is to minimize the sum of the number of bins containing items of each color. In this work, we introduce BPMCF and present a decomposition strategy to solve the problem, where the assignment of items to bins is formulated as a binary decision diagram and an optimal integrated solutions is identified through a mixed-integer linear programming model. Our computational experiments show that the proposed approach greatly outperforms a direct formulation of BPMCF and that its performance is suitable for large instances of the problem.',\n",
       "  \"Title: base station selections for qos provisioning over distributed multi user mimo links in wireless networks\\nAbstract: We propose the QoS-aware BS-selection and the corresponding resource-allocation schemes for downlink multi-user transmissions over the distributed multiple-input-multiple-output (MIMO) links, where multiple location-independent base-stations (BS), controlled by a central server, cooperatively transmit data to multiple mobile users. Our proposed schemes aim at minimizing the BS usages and reducing the interfering range of the distributed MIMO transmissions, while satisfying diverse statistical delay-QoS requirements for all users, which are characterized by the delay-bound violation probability and the effective capacity technique. Specifically, we propose two BS-usage minimization frameworks to develop the QoS-aware BS-selection schemes and the corresponding wireless resource-allocation algorithms across multiple mobile users. The first framework applies the joint block-diagonalization (BD) and probabilistic transmission (PT) to implement multiple access over multiple mobile users, while the second one employs time-division multiple access (TDMA) approach to control multiple users' links. We then derive the optimal BS-selection schemes for these two frameworks, respectively. In addition, we further discuss the PT-only based BS-selection scheme. Also conducted is a set of simulation evaluations to comparatively study the average BS-usage and interfering range of our proposed schemes and to analyze the impact of QoS constraints on the BS selections for distributed MIMO transmissions.\",\n",
       "  'Title: correlation coefficients and semantic textual similarity\\nAbstract: A large body of research into semantic textual similarity has focused on constructing state-of-the-art embeddings using sophisticated modelling, careful choice of learning signals and many clever tricks. By contrast, little attention has been devoted to similarity measures between these embeddings, with cosine similarity being used unquestionably in the majority of cases. In this work, we illustrate that for all common word vectors, cosine similarity is essentially equivalent to the Pearson correlation coefficient, which provides some justification for its use. We thoroughly characterise cases where Pearson correlation (and thus cosine similarity) is unfit as similarity measure. Importantly, we show that Pearson correlation is appropriate for some word vectors but not others. When it is not appropriate, we illustrate how common non-parametric rank correlation coefficients can be used instead to significantly improve performance. We support our analysis with a series of evaluations on word-level and sentence-level semantic textual similarity benchmarks. On the latter, we show that even the simplest averaged word vectors compared by rank correlation easily rival the strongest deep representations compared by cosine similarity.',\n",
       "  'Title: successive synthesis of latent gaussian trees\\nAbstract: A new synthesis scheme is proposed to effectively generate a random vector with prescribed joint density that induces a (latent) Gaussian tree structure. The quality of synthesis is measured by total variation distance between the synthesized and desired statistics. The proposed layered and successive encoding scheme relies on the learned structure of tree to use minimal number of common random variables to synthesize the desired density. We characterize the achievable rate region for the rate tuples of multi-layer latent Gaussian tree, through which the number of bits needed to simulate such Gaussian joint density are determined. The random sources used in our algorithm are the latent variables at the top layer of tree, the additive independent Gaussian noises, and the Bernoulli sign inputs that capture the ambiguity of correlation signs between the variables.',\n",
       "  'Title: asymptotic mmse analysis under sparse representation modeling\\nAbstract: Compressed sensing is a signal processing technique in which data is acquired directly in a compressed form. There are two modeling approaches that can be considered: the worst-case (Hamming) approach and a statistical mechanism, in which the signals are modeled as random processes rather than as individual sequences. In this paper, the second approach is studied. In particular, we consider a model of the form $\\\\boldsymbol{Y} = \\\\boldsymbol{H}\\\\boldsymbol{X}+\\\\boldsymbol{W}$, where each comportment of $\\\\boldsymbol{X}$ is given by $X_i = S_iU_i$, where $\\\\left\\\\{U_i\\\\right\\\\}$ are i.i.d. Gaussian random variables, and $\\\\left\\\\{S_i\\\\right\\\\}$ are binary random variables independent of $\\\\left\\\\{U_i\\\\right\\\\}$, and not necessarily independent and identically distributed (i.i.d.), $\\\\boldsymbol{H}\\\\in\\\\mathbb{R}^{k\\\\times n}$ is a random matrix with i.i.d. entries, and $\\\\boldsymbol{W}$ is white Gaussian noise. Using a direct relationship between optimum estimation and certain partition functions, and by invoking methods from statistical mechanics and from random matrix theory (RMT), we derive an asymptotic formula for the minimum mean-square error (MMSE) of estimating the input vector $\\\\boldsymbol{X}$ given $\\\\boldsymbol{Y}$ and $\\\\boldsymbol{H}$, as $k,n\\\\to\\\\infty$, keeping the measurement rate, $R = k/n$, fixed. In contrast to previous derivations, which are based on the replica method, the analysis carried out in this paper is rigorous.',\n",
       "  'Title: self organized collective motion with a simulated real robot swarm\\nAbstract: Collective motion is one of the most fascinating phenomena observed in the nature. In the last decade, it aroused so much attention in physics, control and robotics fields. In particular, many studies have been done in swarm robotics related to collective motion, also called flocking. In most of these studies, robots use orientation and proximity of their neighbors to achieve collective motion. In such an approach, one of the biggest problems is to measure orientation information using on-board sensors. In most of the studies, this information is either simulated or implemented using communication. In this paper, to the best of our knowledge, we implemented a fully autonomous coordinated motion without alignment using very simple Mona robots. We used an approach based on Active Elastic Sheet (AES) method. We modified the method and added the capability to enable the swarm to move toward a desired direction and rotate about an arbitrary point. The parameters of the modified method are optimized using TCACS optimization algorithm. We tested our approach in different settings using Matlab and Webots.',\n",
       "  'Title: an algorithmic comparison of the hyper reduction and the discrete empirical interpolation method for a nonlinear thermal problem\\nAbstract: A novel algorithmic discussion of the methodological and numerical differences of competing parametric model reduction techniques for nonlinear problems are presented. First, the Galerkin reduced basis (RB) formulation is presented which fails at providing significant gains with respect to the computational efficiency for nonlinear problems. Renown methods for the reduction of the computing time of nonlinear reduced order models are the Hyper-Reduction and the (Discrete) Empirical Interpolation Method (EIM, DEIM). An algorithmic description and a methodological comparison of both methods are provided. The accuracy of the predictions of the hyper-reduced model and the (D)EIM in comparison to the Galerkin RB is investigated. All three approaches are applied to a simple uncertainty quantification of a planar nonlinear thermal conduction problem. The results are compared to computationally intense finite element simulations.',\n",
       "  \"Title: computationally optimal real resource strategies\\nAbstract: This paper focuses on managing the cost of deliberation before action. In many problems, the overall quality of the solution reflects costs incurred and resources consumed in deliberation as well as the cost and benefit of execution, when both the resource consumption in deliberation phase, and the costs in deliberation and execution are uncertain and may be described by probability distribution functions. A feasible (in terms of resource consumption) strategy that minimizes the expected total cost is termed computationally-optimal. For a situation with several independent, uninterruptible methods to solve the problem, we develop a pseudopolynomial-time algorithm to construct generate-and-test computationally optimal strategy. We show this strategy-construction problem to be NP-complete, and apply Bellman's Optimality Principle to solve it efficiently.\",\n",
       "  'Title: bitcoin meets strong consistency\\nAbstract: The Bitcoin system only provides eventual consistency. For everyday life, the time to confirm a Bitcoin transaction is prohibitively slow. In this paper we propose a new system, built on the Bitcoin blockchain, which enables strong consistency. Our system, PeerCensus, acts as a certification authority, manages peer identities in a peer-to-peer network, and ultimately enhances Bitcoin and similar systems with strong consistency. Our extensive analysis shows that PeerCensus is in a secure state with high probability. We also show how Discoin, a Bitcoin variant that decouples block creation and transaction confirmation, can be built on top of PeerCensus, enabling real-time payments. Unlike Bitcoin, once transactions in Discoin are committed, they stay committed.',\n",
       "  'Title: de fragmenting the cloud\\nAbstract: Existing VM placement schemes have measured their effectiveness solely by looking either Physical Machine\\'s resources(CPU, memory) or network resource. However, real applications use all resource types to varying degrees. The result of applying existing placement schemes to VMs running real applications is a fragmented data center where resources along one dimension become unusable even though they are available because of the unavailability of resources along other dimensions. An example of this fragmentation is unusable CPU because of a bottlenecked network link from the physical machine which has available CPU. To date, evaluations of the efficacy of VM placement schemes has not recognized this fragmentation and it\\'s ill effects, let alone try to measure it and avoid it. In this paper, we first define the notion of what we term \"relative resource fragmentation\" and illustrate how it can be measured in a data center. The metric we put forth for capturing the degree of fragmentation is comprehensive and includes all key data center resource types. We then propose a scheme of minimizing this fragmentation so as to maximize the availability of existing set of data center resources. Results of empirical evaluations of our placement scheme compared to existing network based placement schemes show a reduction of fragmentation by as much as 15% and increase in number of successfully placed applications by upto 20%.',\n",
       "  'Title: lombardi drawings of graphs\\nAbstract: We introduce the notion of Lombardi graph drawings, named after the American abstract artist Mark Lombardi. In these drawings, edges are represented as circular arcs rather than as line segments or polylines, and the vertices have perfect angular resolution: the edges are equally spaced around each vertex. We describe algorithms for finding Lombardi drawings of regular graphs, graphs of bounded degeneracy, and certain families of planar graphs.',\n",
       "  'Title: serverless computing one step forward two steps back\\nAbstract: Serverless computing offers the potential to program the cloud in an autoscaling, pay-as-you go manner. In this paper we address critical gaps in first-generation serverless computing, which place its autoscaling potential at odds with dominant trends in modern computing: notably data-centric and distributed computing, but also open source and custom hardware. Put together, these gaps make current serverless offerings a bad fit for cloud innovation and particularly bad for data systems innovation. In addition to pinpointing some of the main shortfalls of current serverless architectures, we raise a set of challenges we believe must be met to unlock the radical potential that the cloud---with its exabytes of storage and millions of cores---should offer to innovative developers.',\n",
       "  \"Title: self learning scene specific pedestrian detectors using a progressive latent model\\nAbstract: In this paper, a self-learning approach is proposed towards solving scene-specific pedestrian detection problem without any human' annotation involved. The self-learning approach is deployed as progressive steps of object discovery, object enforcement, and label propagation. In the learning procedure, object locations in each frame are treated as latent variables that are solved with a progressive latent model (PLM). Compared with conventional latent models, the proposed PLM incorporates a spatial regularization term to reduce ambiguities in object proposals and to enforce object localization, and also a graph-based label propagation to discover harder instances in adjacent frames. With the difference of convex (DC) objective functions, PLM can be efficiently optimized with a concave-convex programming and thus guaranteeing the stability of self-learning. Extensive experiments demonstrate that even without annotation the proposed self-learning approach outperforms weakly supervised learning approaches, while achieving comparable performance with transfer learning and fully supervised approaches.\",\n",
       "  'Title: breaking value symmetry\\nAbstract: Symmetry is an important factor in solving many constraint satisfaction problems. One common type of symmetry is when we have symmetric values. In a recent series of papers, we have studied methods to break value symmetries. Our results identify computational limits on eliminating value symmetry. For instance, we prove that pruning all symmetric values is NP-hard in general. Nevertheless, experiments show that much value symmetry can be broken in practice. These results may be useful to researchers in planning, scheduling and other areas as value symmetry occurs in many different domains.',\n",
       "  'Title: correlated resource models of internet end hosts\\nAbstract: Understanding and modelling resources of Internet end hosts is essential for the design of desktop software and Internet-distributed applications. In this paper we develop a correlated resource model of Internet end hosts based on real trace data taken from the SETI@home project. This data covers a 5-year period with statistics for 2.7 million hosts. The resource model is based on statistical analysis of host computational power, memory, and storage as well as how these resources change over time and the correlations between them. We find that resources with few discrete values (core count, memory) are well modeled by exponential laws governing the change of relative resource quantities over time. Resources with a continuous range of values are well modeled with either correlated normal distributions (processor speed for integer operations and floating point operations) or log-normal distributions (available disk space). We validate and show the utility of the models by applying them to a resource allocation problem for Internet-distributed applications, and demonstrate their value over other models. We also make our trace data and tool for automatically generating realistic Internet end hosts publicly available.',\n",
       "  'Title: algebraic geometry codes with complementary duals exceed the asymptotic gilbert varshamov bound\\nAbstract: It was shown by Massey that linear complementary dual (LCD for short) codes are asymptotically good. In 2004, Sendrier proved that LCD codes meet the asymptotic Gilbert-Varshamov (GV for short) bound. Until now, the GV bound still remains to be the best asymptotical lower bound for LCD codes. In this paper, we show that an algebraic geometry code over a finite field of even characteristic is equivalent to an LCD code and consequently there exists a family of LCD codes that are equivalent to algebraic geometry codes and exceed the asymptotical GV bound.',\n",
       "  'Title: minimum weight dynamo and fast opinion spreading\\nAbstract: We consider the following multi--level opinion spreading model on networks. Initially, each node gets a weight from the set [0..k-1], where such a weight stands for the individuals conviction of a new idea or product. Then, by proceeding to rounds, each node updates its weight according to the weights of its neighbors. We are interested in the initial assignments of weights leading each node to get the value k-1 --e.g. unanimous maximum level acceptance-- within a given number of rounds. We determine lower bounds on the sum of the initial weights of the nodes under the irreversible simple majority rules, where a node increases its weight if and only if the majority of its neighbors have a weight that is higher than its own one. Moreover, we provide constructive tight upper bounds for some class of regular topologies: rings, tori, and cliques.',\n",
       "  'Title: the closer the better similarity of publication pairs at different co citation levels\\nAbstract: We investigate the similarities of pairs of articles which are co-cited at the different co-citation levels of the journal, article, section, paragraph, sentence and bracket. Our results indicate that textual similarity, intellectual overlap (shared references), author overlap (shared authors), proximity in publication time all rise monotonically as the co-citation level gets lower (from journal to bracket). While the main gain in similarity happens when moving from journal to article co-citation, all level changes entail an increase in similarity, especially section to paragraph and paragraph to sentence/bracket levels. We compare results from four journals over the years 2010-2015: Cell, the European Journal of Operational Research, Physics Letters B and Research Policy, with consistent general outcomes and some interesting differences. Our findings motivate the use of granular co-citation information as defined by meaningful units of text, with implications for, among others, the elaboration of maps of science and the retrieval of scholarly literature.',\n",
       "  \"Title: learning the information divergence\\nAbstract: Information divergence that measures the difference between two nonnegative matrices or tensors has found its use in a variety of machine learning problems. Examples are Nonnegative Matrix/Tensor Factorization, Stochastic Neighbor Embedding, topic models, and Bayesian network optimization. The success of such a learning task depends heavily on a suitable divergence. A large variety of divergences have been suggested and analyzed, but very few results are available for an objective choice of the optimal divergence for a given task. Here we present a framework that facilitates automatic selection of the best divergence among a given family, based on standard maximum likelihood estimation. We first propose an approximated Tweedie distribution for the beta-divergence family. Selecting the best beta then becomes a machine learning problem solved by maximum likelihood. Next, we reformulate alpha-divergence in terms of beta-divergence, which enables automatic selection of alpha by maximum likelihood with reuse of the learning principle for beta-divergence. Furthermore, we show the connections between gamma and beta-divergences as well as R\\\\'enyi and alpha-divergences, such that our automatic selection framework is extended to non-separable divergences. Experiments on both synthetic and real-world data demonstrate that our method can quite accurately select information divergence across different learning problems and various divergence families.\",\n",
       "  'Title: effects of high order co occurrences on word semantic similarities\\nAbstract: A computational model of the construction of word meaning through exposure to texts is built in order to simulate the effects of co-occurrence values on word semantic similarities, paragraph by paragraph. Semantic similarity is here viewed as association. It turns out that the similarity between two words W1 and W2 strongly increases with a co-occurrence, decreases with the occurrence of W1 without W2 or W2 without W1, and slightly increases with high-order co-occurrences. Therefore, operationalizing similarity as a frequency of co-occurrence probably introduces a bias: first, there are cases in which there is similarity without co-occurrence and, second, the frequency of co-occurrence overestimates similarity.',\n",
       "  'Title: misspelling oblivious word embeddings\\nAbstract: In this paper we present a method to learn word embeddings that are resilient to misspellings. Existing word embeddings have limited applicability to malformed texts, which contain a non-negligible amount of out-of-vocabulary words. We propose a method combining FastText with subwords and a supervised task of learning misspelling patterns. In our method, misspellings of each word are embedded close to their correct variants. We train these embeddings on a new dataset we are releasing publicly. Finally, we experimentally show the advantages of this approach on both intrinsic and extrinsic NLP tasks using public test sets.',\n",
       "  'Title: efficient and accurate estimation of lipschitz constants for deep neural networks\\nAbstract: Tight estimation of the Lipschitz constant for deep neural networks (DNNs) is useful in many applications ranging from robustness certification of classifiers to stability analysis of closed-loop systems with reinforcement learning controllers. Existing methods in the literature for estimating the Lipschitz constant suffer from either lack of accuracy or poor scalability. In this paper, we present a convex optimization framework to compute guaranteed upper bounds on the Lipschitz constant of DNNs both accurately and efficiently. Our main idea is to interpret activation functions as gradients of convex potential functions. Hence, they satisfy certain properties that can be described by quadratic constraints. This particular description allows us to pose the Lipschitz constant estimation problem as a semidefinite program (SDP). The resulting SDP can be adapted to increase either the estimation accuracy (by capturing the interaction between activation functions of different layers) or scalability (by decomposition and parallel implementation). We illustrate the utility of our approach with a variety of experiments on randomly generated networks and on classifiers trained on the MNIST and Iris datasets. In particular, we experimentally demonstrate that our Lipschitz bounds are the most accurate compared to those in the literature. We also study the impact of adversarial training methods on the Lipschitz bounds of the resulting classifiers and show that our bounds can be used to efficiently provide robustness guarantees.',\n",
       "  'Title: distributed analysis and load balancing system for grid enabled analysis on hand held devices using multi agents systems\\nAbstract: Handheld devices, while growing rapidly, are inherently constrained and lack the capability of executing resource hungry applications. This paper presents the design and implementation of distributed analysis and load-balancing system for hand-held devices using multi-agents system. This system enables low resource mobile handheld devices to act as potential clients for Grid enabled applications and analysis environments. We propose a system, in which mobile agents will transport, schedule, execute and return results for heavy computational jobs submitted by handheld devices. Moreover, in this way, our system provides high throughput computing environment for hand-held devices.',\n",
       "  'Title: provably efficient rl with rich observations via latent state decoding\\nAbstract: We study the exploration problem in episodic MDPs with rich observations generated from a small number of latent states. Under certain identifiability assumptions, we demonstrate how to estimate a mapping from the observations to latent states inductively through a sequence of regression and clustering steps---where previously decoded latent states provide labels for later regression problems---and use it to construct good exploration policies. We provide finite-sample guarantees on the quality of the learned state decoding function and exploration policies, and complement our theory with an empirical evaluation on a class of hard exploration problems. Our method exponentially improves over $Q$-learning with naive exploration, even when $Q$-learning has cheating access to latent states.',\n",
       "  \"Title: learning without concentration\\nAbstract: We obtain sharp bounds on the performance of Empirical Risk Minimization performed in a convex class and with respect to the squared loss, without assuming that class members and the target are bounded functions or have rapidly decaying tails. #R##N#Rather than resorting to a concentration-based argument, the method used here relies on a `small-ball' assumption and thus holds for classes consisting of heavy-tailed functions and for heavy-tailed targets. #R##N#The resulting estimates scale correctly with the `noise level' of the problem, and when applied to the classical, bounded scenario, always improve the known bounds.\",\n",
       "  'Title: online voltage stability assessment for load areas based on the holomorphic embedding method\\nAbstract: This paper proposes an online steady-state voltage stability assessment scheme to evaluate the proximity to voltage collapse at each bus of a load area. Using a non-iterative holomorphic embedding method (HEM) with a proposed physical germ solution, an accurate loading limit at each load bus can be calculated based on online state estimation on the entire load area and a measurement-based equivalent for the external system. The HEM employs a power series to calculate an accurate Power-Voltage (P-V) curve at each load bus and accordingly evaluates the voltage stability margin considering load variations in the next period. An adaptive two-stage Pade approximants method is proposed to improve the convergence of the power series for accurate determination of the nose point on the P-V curve with moderate computational burden. The proposed method is illustrated in detail on a 4-bus test system and then demonstrated on a load area of the Northeast Power Coordinating Council (NPCC) 48-geneartor, 140-bus power system.',\n",
       "  'Title: modanet a large scale street fashion dataset with polygon annotations\\nAbstract: Understanding clothes from a single image has strong commercial and cultural impacts on modern societies. However, this task remains a challenging computer vision problem due to wide variations in the appearance, style, brand and layering of clothing items. We present a new database called ModaNet, a large-scale collection of images based on Paperdoll dataset. Our dataset provides 55,176 street images, fully annotated with polygons on top of the 1 million weakly annotated street images in Paperdoll. ModaNet aims to provide a technical benchmark to fairly evaluate the progress of applying the latest computer vision techniques that rely on large data for fashion understanding. The rich annotation of the dataset allows to measure the performance of state-of-the-art algorithms for object detection, semantic segmentation and polygon prediction on street fashion images in detail. The polygon-based annotation dataset has been released this https URL, we also host the leaderboard at EvalAI: this https URL.',\n",
       "  \"Title: deep recurrent electricity theft detection in ami networks with random tuning of hyper parameters\\nAbstract: Modern smart grids rely on advanced metering infrastructure (AMI) networks for monitoring and billing purposes. However, such an approach suffers from electricity theft cyberattacks. Different from the existing research that utilizes shallow, static, and customer-specific-based electricity theft detectors, this paper proposes a generalized deep recurrent neural network (RNN)-based electricity theft detector that can effectively thwart these cyberattacks. The proposed model exploits the time series nature of the customers' electricity consumption to implement a gated recurrent unit (GRU)-RNN, hence, improving the detection performance. In addition, the proposed RNN-based detector adopts a random search analysis in its learning stage to appropriately fine-tune its hyper-parameters. Extensive test studies are carried out to investigate the detector's performance using publicly available real data of 107,200 energy consumption days from 200 customers. Simulation results demonstrate the superior performance of the proposed detector compared with state-of-the-art electricity theft detectors.\",\n",
       "  'Title: an efficient algorithm for factoring polynomials over algebraic extension field\\nAbstract: A new efficient algorithm is proposed for factoring polynomials over an algebraic extension field. The extension field is defined by a polynomial ring modulo a maximal ideal. If the maximal ideal is given by its Groebner basis, no extra Groebner basis computation is needed for factoring a polynomial over this extension field. Nothing more than linear algebraic technique is used to get a polynomial over the ground field by a generic linear map. Then this polynomial is factorized over the ground field. From these factors, the factorization of the polynomial over the extension field is obtained. The new algorithm has been implemented and computer experiments indicate that the new algorithm is very efficient, particularly in complicated examples.',\n",
       "  'Title: sophistication vs logical depth\\nAbstract: Sophistication and logical depth are two measures that express how complicated the structure in a string is. Sophistication is defined as the minimal complexity of a computable function that defines a two-part description for the string that is shortest within some precision; the second can be defined as the minimal computation time of a program that is shortest within some precision. We show that the Busy Beaver function of the sophistication of a string exceeds its logical depth with logarithmically bigger precision, and that logical depth exceeds the Busy Beaver function of sophistication with logarithmically bigger precision. We also show that this is not true if the precision is only increased by a constant (when the notions are defined with plain Kolmogorov complexity). Finally we show that sophistication is unstable in its precision: constant variations can change its value by a linear term in the length of the string.',\n",
       "  \"Title: probability and asset updating using bayesian networks for combinatorial prediction markets\\nAbstract: A market-maker-based prediction market lets forecasters aggregate information by editing a consensus probability distribution either directly or by trading securities that pay off contingent on an event of interest. Combinatorial prediction markets allow trading on any event that can be specified as a combination of a base set of events. However, explicitly representing the full joint distribution is infeasible for markets with more than a few base events. A factored representation such as a Bayesian network (BN) can achieve tractable computation for problems with many related variables. Standard BN inference algorithms, such as the junction tree algorithm, can be used to update a representation of the entire joint distribution given a change to any local conditional probability. However, in order to let traders reuse assets from prior trades while never allowing assets to become negative, a BN based prediction market also needs to update a representation of each user's assets and find the conditional state in which a user has minimum assets. Users also find it useful to see their expected assets given an edit outcome. We show how to generalize the junction tree algorithm to perform all these computations.\",\n",
       "  'Title: learning multi level hierarchies with hindsight\\nAbstract: Hierarchical agents have the potential to solve sequential decision making tasks with greater sample efficiency than their non-hierarchical counterparts because hierarchical agents can break down tasks into sets of subtasks that only require short sequences of decisions. In order to realize this potential of faster learning, hierarchical agents need to be able to learn their multiple levels of policies in parallel so these simpler subproblems can be solved simultaneously. Yet, learning multiple levels of policies in parallel is hard because it is inherently unstable: changes in a policy at one level of the hierarchy may cause changes in the transition and reward functions at higher levels in the hierarchy, making it difficult to jointly learn multiple levels of policies. In this paper, we introduce a new Hierarchical Reinforcement Learning (HRL) framework, Hierarchical Actor-Critic (HAC), that can overcome the instability issues that arise when agents try to jointly learn multiple levels of policies. The main idea behind HAC is to train each level of the hierarchy independently of the lower levels by training each level as if the lower level policies are already optimal. We demonstrate experimentally in both grid world and simulated robotics domains that our approach can significantly accelerate learning relative to other non-hierarchical and hierarchical methods. Indeed, our framework is the first to successfully learn 3-level hierarchies in parallel in tasks with continuous state and action spaces.',\n",
       "  'Title: modular networks learning to decompose neural computation\\nAbstract: Scaling model capacity has been vital in the success of deep learning. For a typical network, necessary compute resources and training time grow dramatically with model size. Conditional computation is a promising way to increase the number of parameters with a relatively small increase in resources. We propose a training algorithm that flexibly chooses neural modules based on the data to be processed. Both the decomposition and modules are learned end-to-end. In contrast to existing approaches, training does not rely on regularization to enforce diversity in module use. We apply modular networks both to image recognition and language modeling tasks, where we achieve superior performance compared to several baselines. Introspection reveals that modules specialize in interpretable contexts.',\n",
       "  \"Title: evolving symbolic controllers\\nAbstract: The idea of symbolic controllers tries to bridge the gap between the top-down manual design of the controller architecture, as advocated in Brooks' subsumption architecture, and the bottom-up designer-free approach that is now standard within the Evolutionary Robotics community. The designer provides a set of elementary behavior, and evolution is given the goal of assembling them to solve complex tasks. Two experiments are presented, demonstrating the efficiency and showing the recursiveness of this approach. In particular, the sensitivity with respect to the proposed elementary behaviors, and the robustness w.r.t. generalization of the resulting controllers are studied in detail.\",\n",
       "  'Title: neobility at semeval 2017 task 1 an attention based sentence similarity model\\nAbstract: This paper describes a neural-network model which performed competitively (top 6) at the SemEval 2017 cross-lingual Semantic Textual Similarity (STS) task. Our system employs an attention-based recurrent neural network model that optimizes the sentence similarity. In this paper, we describe our participation in the multilingual STS task which measures similarity across English, Spanish, and Arabic.',\n",
       "  \"Title: dempster s rule for evidence ordered in a complete directed acyclic graph\\nAbstract: For the case of evidence ordered in a complete directed acyclic graph this paper presents a new algorithm with lower computational complexity for Dempster's rule than that of step-by-step application of Dempster's rule. In this problem, every original pair of evidences, has a corresponding evidence against the simultaneous belief in both propositions. In this case, it is uncertain whether the propositions of any two evidences are in logical conflict. The original evidences are associated with the vertices and the additional evidences are associated with the edges. The original evidences are ordered, i.e., for every pair of evidences it is determinable which of the two evidences is the earlier one. We are interested in finding the most probable completely specified path through the graph, where transitions are possible only from lower- to higher-ranked vertices. The path is here a representation for a sequence of states, for instance a sequence of snapshots of a physical object's track. A completely specified path means that the path includes no other vertices than those stated in the path representation, as opposed to an incompletely specified path that may also include other vertices than those stated. In a hierarchical network of all subsets of the frame, i.e., of all incompletely specified paths, the original and additional evidences support subsets that are not disjoint, thus it is not possible to prune the network to a tree. Instead of propagating belief, the new algorithm reasons about the logical conditions of a completely specified path through the graph. The new algorithm is O(|THETA| log |THETA|), compared to O(|THETA| ** log |THETA|) of the classic brute force algorithm.\",\n",
       "  'Title: conditions under which conditional independence and scoring methods lead to identical selection of bayesian network models\\nAbstract: It is often stated in papers tackling the task of inferring Bayesian network structures from data that there are these two distinct approaches: (i) Apply conditional independence tests when testing for the presence or otherwise of edges; (ii) Search the model space using a scoring metric. Here I argue that for complete data and a given node ordering this division is a myth, by showing that cross entropy methods for checking conditional independence are mathematically identical to methods based upon discriminating between models by their overall goodness-of-fit logarithmic scores.',\n",
       "  'Title: cd phy physical layer security in wireless networks through constellation diversity\\nAbstract: A common approach for introducing security at the physical layer is to rely on the channel variations of the wireless environment. This type of approach is not always suitable for wireless networks where the channel remains static for most of the network lifetime. For these scenarios, a channel independent physical layer security measure is more appropriate which will rely on a secret known to the sender and the receiver but not to the eavesdropper. In this paper, we propose CD-PHY, a physical layer security technique that exploits the constellation diversity of wireless networks which is independent of the channel variations. The sender and the receiver use a custom bit sequence to constellation symbol mapping to secure the physical layer communication which is not known a priori to the eavesdropper. Through theoretical modeling and experimental simulation, we show that this information theoretic construct can achieve Shannon secrecy and any brute force attack from the eavesdropper incurs high overhead and minuscule probability of success. Our results also show that the high bit error rate also makes decoding practically infeasible for the eavesdropper, thus securing the communication between the sender and receiver.',\n",
       "  'Title: on board communication based relative localization for collision avoidance in micro air vehicle teams\\nAbstract: Micro Air Vehicles (MAVs) will unlock their true potential once they can operate in groups. To this end, it is essential for them to estimate on-board the relative location of their neighbors. The challenge lies in limiting the mass and processing burden needed to enable this. We developed a relative localization method that only requires the MAVs to communicate via their wireless transceiver. Communication allows the exchange of on-board states (velocity, height, and orientation), while the signal-strength provides range data. These quantities are fused to provide a full relative location estimate. We used our method to tackle the problem of collision avoidance in tight areas. The system was tested with a team of AR.Drones flying in a 4mx4m area and with miniature drones of ~50g in a 2mx2m area. The MAVs were able to track their relative positions and fly several minutes without collisions. Our implementation used Bluetooth to communicate between the drones. This featured significant noise and disturbances in signal-strength, which worsened as more drones were added. Simulation analysis suggests that results can improve with a more suitable transceiver module.',\n",
       "  \"Title: even delta matroids and the complexity of planar boolean csps\\nAbstract: The main result of our paper is a generalization of the classical blossom algorithm for finding perfect matchings that can efficiently solve Boolean CSPs where each variable appears in exactly two constraints and all constraints are even $\\\\Delta$-matroid relations (represented by lists of tuples). As a consequence of this, we settle the complexity classification of planar Boolean CSPs started by Dvo\\\\v{r}\\\\'ak and Kupec.\",\n",
       "  'Title: generalized spatial modulation in indoor wireless visible light communication\\nAbstract: In this paper, we investigate the performance of generalized spatial modulation (GSM) in indoor wireless visible light communication (VLC) systems. GSM uses $N_t$ light emitting diodes (LED), but activates only $N_a$ of them at a given time. Spatial modulation and spatial multiplexing are special cases of GSM with $N_{a}=1$ and $N_{a}=N_t$, respectively. We first derive an analytical upper bound on the bit error rate (BER) for maximum likelihood (ML) detection of GSM in VLC systems. Analysis and simulation results show that the derived upper bound is very tight at medium to high signal-to-noise ratios (SNR). The channel gains and channel correlations influence the GSM performance such that the best BER is achieved at an optimum LED spacing. Also, for a fixed transmission efficiency, the performance of GSM in VLC improves as the half-power semi-angle of the LEDs is decreased. We then compare the performance of GSM in VLC systems with those of other MIMO schemes such as spatial multiplexing (SMP), space shift keying (SSK), generalized space shift keying (GSSK), and spatial modulation (SM). Analysis and simulation results show that GSM in VLC outperforms the other considered MIMO schemes at moderate to high SNRs; for example, for 8 bits per channel use, GSM outperforms SMP and GSSK by about 21 dB, and SM by about 10 dB at $10^{-4}$ BER.',\n",
       "  \"Title: fast optical flow using dense inverse search\\nAbstract: Most recent works in optical flow extraction focus on the accuracy and neglect the time complexity. However, in real-life visual applications, such as tracking, activity detection and recognition, the time complexity is critical. #R##N#We propose a solution with very low time complexity and competitive accuracy for the computation of dense optical flow. It consists of three parts: 1) inverse search for patch correspondences; 2) dense displacement field creation through patch aggregation along multiple scales; 3) variational refinement. At the core of our Dense Inverse Search-based method (DIS) is the efficient search of correspondences inspired by the inverse compositional image alignment proposed by Baker and Matthews in 2001. #R##N#DIS is competitive on standard optical flow benchmarks with large displacements. DIS runs at 300Hz up to 600Hz on a single CPU core, reaching the temporal resolution of human's biological vision system. It is order(s) of magnitude faster than state-of-the-art methods in the same range of accuracy, making DIS ideal for visual applications.\",\n",
       "  'Title: improved complexity results on k coloring p_t free graphs\\nAbstract: A graph is $H$-free if it does not contain an induced subgraph isomorphic to $H$. We denote by $P_k$ and $C_k$ the path and the cycle on $k$ vertices, respectively. In this paper, we prove that 4-COLORING is NP-complete for $P_7$-free graphs, and that 5-COLORING is NP-complete for $P_6$-free graphs. These two results improve all previous results on $k$-coloring $P_t$-free graphs, and almost complete the classification of complexity of $k$-COLORING $P_t$-free graphs for $k\\\\ge 4$ and $t\\\\ge 1$, leaving as the only missing case 4-COLORING $P_6$-free graphs. We expect that 4-COLORING is polynomial time solvable for $P_6$-free graphs; in support of this, we describe a polynomial time algorithm for 4-COLORING $P_6$-free graphs which are also $P$-free, where $P$ is the graph obtained from $C_4$ by adding a new vertex and making it adjacent to exactly one vertex on the $C_4$.',\n",
       "  \"Title: parallel recursive state compression for free\\nAbstract: This paper focuses on reducing memory usage in enumerative model checking, while maintaining the multi-core scalability obtained in earlier work. We present a tree-based multi-core compression method, which works by leveraging sharing among sub-vectors of state vectors. #R##N#An algorithmic analysis of both worst-case and optimal compression ratios shows the potential to compress even large states to a small constant on average (8 bytes). Our experiments demonstrate that this holds up in practice: the median compression ratio of 279 measured experiments is within 17% of the optimum for tree compression, and five times better than the median compression ratio of SPIN's COLLAPSE compression. #R##N#Our algorithms are implemented in the LTSmin tool, and our experiments show that for model checking, multi-core tree compression pays its own way: it comes virtually without overhead compared to the fastest hash table-based methods.\",\n",
       "  'Title: learning shape abstractions by assembling volumetric primitives\\nAbstract: We present a learning framework for abstracting complex shapes by learning to assemble objects using 3D volumetric primitives. In addition to generating simple and geometrically interpretable explanations of 3D objects, our framework also allows us to automatically discover and exploit consistent structure in the data. We demonstrate that using our method allows predicting shape representations which can be leveraged for obtaining a consistent parsing across the instances of a shape collection and constructing an interpretable shape similarity measure. We also examine applications for image-based prediction as well as shape manipulation.',\n",
       "  'Title: losing weight by gaining edges\\nAbstract: We present a new way to encode weighted sums into unweighted pairwise constraints, obtaining the following results. #R##N#- Define the k-SUM problem to be: given n integers in [-n^2k, n^2k] are there k which sum to zero? (It is well known that the same problem over arbitrary integers is equivalent to the above definition, by linear-time randomized reductions.) We prove that this definition of k-SUM remains W[1]-hard, and is in fact W[1]-complete: k-SUM can be reduced to f(k) * n^o(1) instances of k-Clique. #R##N#- The maximum node-weighted k-Clique and node-weighted k-dominating set problems can be reduced to n^o(1) instances of the unweighted k-Clique and k-dominating set problems, respectively. This implies a strong equivalence between the time complexities of the node weighted problems and the unweighted problems: any polynomial improvement on one would imply an improvement for the other. #R##N#- A triangle of weight 0 in a node weighted graph with m edges can be deterministically found in m^1.41 time.',\n",
       "  'Title: understanding deep image representations by inverting them\\nAbstract: Image representations, from SIFT and Bag of Visual Words to Convolutional Neural Networks (CNNs), are a crucial component of almost any image understanding system. Nevertheless, our understanding of them remains limited. In this paper we conduct a direct analysis of the visual information contained in representations by asking the following question: given an encoding of an image, to which extent is it possible to reconstruct the image itself? To answer this question we contribute a general framework to invert representations. We show that this method can invert representations such as HOG and SIFT more accurately than recent alternatives while being applicable to CNNs too. We then use this technique to study the inverse of recent state-of-the-art CNN image representations for the first time. Among our findings, we show that several layers in CNNs retain photographically accurate information about the image, with different degrees of geometric and photometric invariance.',\n",
       "  'Title: plug and play decentralized model predictive control\\nAbstract: In this paper we consider a linear system structured into physically coupled subsystems and propose a decentralized control scheme capable to guarantee asymptotic stability and satisfaction of constraints on system inputs and states. The design procedure is totally decentralized, since the synthesis of a local controller uses only information on a subsystem and its neighbors, i.e. subsystems coupled to it. We first derive tests for checking if a subsystem can be plugged into (or unplugged from) an existing plant without spoiling overall stability and constraint satisfaction. When this is possible, we show how to automatize the design of local controllers so that it can be carried out in parallel by smart actuators equipped with computational resources and capable to exchange information with neighboring subsystems. In particular, local controllers exploit tube-based Model Predictive Control (MPC) in order to guarantee robustness with respect to physical coupling among subsystems. Finally, an application of the proposed control design procedure to frequency control in power networks is presented.',\n",
       "  'Title: multi view machines\\nAbstract: For a learning task, data can usually be collected from different sources or be represented from multiple views. For example, laboratory results from different medical examinations are available for disease diagnosis, and each of them can only reflect the health state of a person from a particular aspect/view. Therefore, different views provide complementary information for learning tasks. An effective integration of the multi-view information is expected to facilitate the learning performance. In this paper, we propose a general predictor, named multi-view machines (MVMs), that can effectively include all the possible interactions between features from multiple views. A joint factorization is embedded for the full-order interaction parameters which allows parameter estimation under sparsity. Moreover, MVMs can work in conjunction with different loss functions for a variety of machine learning tasks. A stochastic gradient descent method is presented to learn the MVM model. We further illustrate the advantages of MVMs through comparison with other methods for multi-view classification, including support vector machines (SVMs), support tensor machines (STMs) and factorization machines (FMs).',\n",
       "  'Title: space constrained interval selection\\nAbstract: We study streaming algorithms for the interval selection problem: finding a maximum cardinality subset of disjoint intervals on the line. A deterministic 2-approximation streaming algorithm for this problem is developed, together with an algorithm for the special case of proper intervals, achieving improved approximation ratio of 3/2. We complement these upper bounds by proving that they are essentially best possible in the streaming setting: it is shown that an approximation ratio of $2 - \\\\epsilon$ (or $3 / 2 - \\\\epsilon$ for proper intervals) cannot be achieved unless the space is linear in the input size. In passing, we also answer an open question of Adler and Azar \\\\cite{AdlerAzar03} regarding the space complexity of constant-competitive randomized preemptive online algorithms for the same problem.',\n",
       "  'Title: geocoding without geotags a text based approach for reddit\\nAbstract: In this paper, we introduce the first geolocation inference approach for reddit, a social media platform where user pseudonymity has thus far made supervised demographic inference difficult to implement and validate. In particular, we design a text-based heuristic schema to generate ground truth location labels for reddit users in the absence of explicitly geotagged data. After evaluating the accuracy of our labeling procedure, we train and test several geolocation inference models across our reddit data set and three benchmark Twitter geolocation data sets. Ultimately, we show that geolocation models trained and applied on the same domain substantially outperform models attempting to transfer training data across domains, even more so on reddit where platform-specific interest-group metadata can be used to improve inferences.',\n",
       "  'Title: improved bounds on restricted isometry constants for gaussian matrices\\nAbstract: The Restricted Isometry Constants (RIC) of a matrix $A$ measures how close to an isometry is the action of $A$ on vectors with few nonzero entries, measured in the $\\\\ell^2$ norm. Specifically, the upper and lower RIC of a matrix $A$ of size $n\\\\times N$ is the maximum and the minimum deviation from unity (one) of the largest and smallest, respectively, square of singular values of all ${N\\\\choose k}$ matrices formed by taking $k$ columns from $A$. Calculation of the RIC is intractable for most matrices due to its combinatorial nature; however, many random matrices typically have bounded RIC in some range of problem sizes $(k,n,N)$. We provide the best known bound on the RIC for Gaussian matrices, which is also the smallest known bound on the RIC for any large rectangular matrix. Improvements over prior bounds are achieved by exploiting similarity of singular values for matrices which share a substantial number of columns.',\n",
       "  'Title: a survey of blockchain frameworks and applications\\nAbstract: The applications of the blockchain technology are still being discov-ered. When a new potential disruptive technology emerges, there is a tendency to try to solve every problem with that technology. However, it is still necessary to determine what approach is the best for each type of application. To find how distributed ledgers solve existing problems, this study looks for blockchain frameworks in the academic world. Identifying the existing frameworks can demonstrate where the interest in the technology exists and where it can be miss-ing. This study encountered several blockchain frameworks in development. However, there are few references to operational needs, testing, and deploy of the technology. With the widespread use of the technology, either integrating with pre-existing solutions, replacing legacy systems, or new implementations, the need for testing, deploying, exploration, and maintenance is expected to in-tensify.',\n",
       "  'Title: the dilworth number of auto chordal bipartite graphs\\nAbstract: The mirror (or bipartite complement) mir(B) of a bipartite graph B=(X,Y,E) has the same color classes X and Y as B, and two vertices x in X and y in Y are adjacent in mir(B) if and only if xy is not in E. A bipartite graph is chordal bipartite if none of its induced subgraphs is a chordless cycle with at least six vertices. In this paper, we deal with chordal bipartite graphs whose mirror is chordal bipartite as well; we call these graphs auto-chordal bipartite graphs (ACB graphs for short). We describe the relationship to some known graph classes such as interval and strongly chordal graphs and we present several characterizations of ACB graphs. We show that ACB graphs have unbounded Dilworth number, and we characterize ACB graphs with Dilworth number k.',\n",
       "  'Title: nearly optimal exploration exploitation decision thresholds\\nAbstract: While in general trading off exploration and exploitation in reinforcement learning is hard, under some formulations relatively simple solutions exist. In this paper, we first derive upper bounds for the utility of selecting different actions in the multi-armed bandit setting. Unlike the common statistical upper confidence bounds, these explicitly link the planning horizon, uncertainty and the need for exploration explicit. The resulting algorithm can be seen as a generalisation of the classical Thompson sampling algorithm. We experimentally test these algorithms, as well as $\\\\epsilon$-greedy and the value of perfect information heuristics. Finally, we also introduce the idea of bagging for reinforcement learning. By employing a version of online bootstrapping, we can efficiently sample from an approximate posterior distribution.',\n",
       "  'Title: leveraging disease progression learning for medical image recognition\\nAbstract: Unlike natural images, medical images often have intrinsic characteristics that can be leveraged for neural network learning. For example, images that belong to different stages of a disease may continuously follow a certain progression pattern. In this paper, we propose a novel method that leverages disease progression learning for medical image recognition. In our method, sequences of images ordered by disease stages are learned by a neural network that consists of a shared vision model for feature extraction and a long short-term memory network for the learning of stage sequences. Auxiliary vision outputs are also included to capture stage features that tend to be discrete along the disease progression. Our proposed method is evaluated on a public diabetic retinopathy dataset, and achieves about 3.3% improvement in disease staging accuracy, compared to the baseline method that does not use disease progression learning.',\n",
       "  'Title: quality attributes in practice contemporary data\\nAbstract: It is well known that the software process in place impacts the quality of the resulting product. However, the specific way in which this effect occurs is still mostly unknown and reported through anecdotes. To gather a better understanding of such relationship, a very large survey has been conducted during the last year and has been completed by more than 100 software developers and engineers from 21 countries. We have used the percentage of satisfied customers estimated by the software developers and engineers as the main dependent variable. The results evidence some interesting patterns, like that quality attribute of which customers are more satisfied appears functionality, architectural styles may not have a significant influence on quality, agile methodologies might result in happier customers, larger companies and shorter projects seems to produce better products.',\n",
       "  'Title: factorizing lambdamart for cold start recommendations\\nAbstract: Recommendation systems often rely on point-wise loss metrics such as the mean squared error. However, in real recommendation settings only few items are presented to a user. This observation has recently encouraged the use of rankbased metrics. LambdaMART is the state-of-the-art algorithm in learning to rank which relies on such a metric. Despite its success it does not have a principled regularization mechanism relying in empirical approaches to control model complexity leaving it thus prone to overfitting. Motivated by the fact that very often the users’ and items’ descriptions as well as the preference behavior can be well summarized by a small number of hidden factors, we propose a novel algorithm, LambdaMARTMatrix Factorization (LambdaMART-MF), that learns a low rank latent representation of users and items using gradient boosted trees. The algorithm factorizes lambdaMART by defining relevance scores as the inner product of the learned representations of the users and items. The low rank is essentially a model complexity controller; on top of it we propose additional regularizers to constraint the learned latent representations that reflect the user and item manifolds as these are defined by their original feature based descriptors and the preference behavior. Finally we also propose to use a weighted variant of NDCG to reduce the penalty for similar items with large rating discrepancy. We experiment on two very different recommendation datasets, meta-mining and movies-users, and evaluate the performance of LambdaMART-MF, with and without regularization, in the cold start setting as well as in the simpler matrix completion setting. In both cases it outperforms in a significant manner current state of the art algorithms.',\n",
       "  'Title: fuzzy feedback scheduling of resource constrained embedded control systems\\nAbstract: The quality of control (QoC) of a resource-constrained embedded control system may be jeopardized in dynamic environments with variable workload. This gives rise to the increasing demand of co-design of control and scheduling. To deal with uncertainties in resource availability, a fuzzy feedback scheduling (FFS) scheme is proposed in this paper. Within the framework of feedback scheduling, the sampling periods of control loops are dynamically adjusted using the fuzzy control technique. The feedback scheduler provides QoC guarantees in dynamic environments through maintaining the CPU utilization at a desired level. The framework and design methodology of the proposed FFS scheme are described in detail. A simplified mobile robot target tracking system is investigated as a case study to demonstrate the effectiveness of the proposed FFS scheme. The scheme is independent of task execution times, robust to measurement noises, and easy to implement, while incurring only a small overhead.',\n",
       "  'Title: topsig topology preserving document signatures\\nAbstract: Performance comparisons between File Signatures and Inverted Files for text retrieval have previously shown several significant shortcomings of file signatures relative to inverted files. The inverted file approach underpins most state-of-the-art search engine algorithms, such as Language and Probabilistic models. It has been widely accepted that traditional file signatures are inferior alternatives to inverted files. This paper describes TopSig, a new approach to the construction of file signatures. Many advances in semantic hashing and dimensionality reduction have been made in recent times, but these were not so far linked to general purpose, signature file based, search engines. This paper introduces a different signature file approach that builds upon and extends these recent advances. We are able to demonstrate significant improvements in the performance of signature file based indexing and retrieval, performance that is comparable to that of state of the art inverted file based systems, including Language models and BM25. These findings suggest that file signatures offer a viable alternative to inverted files in suitable settings and from the theoretical perspective it positions the file signatures model in the class of Vector Space retrieval models.',\n",
       "  'Title: finding small patterns in permutations in linear time\\nAbstract: Given two permutations $\\\\sigma$ and $\\\\pi$, the \\\\textsc{Permutation Pattern} problem asks if $\\\\sigma$ is a subpattern of $\\\\pi$. We show that the problem can be solved in time $2^{O(\\\\ell^2\\\\log \\\\ell)}\\\\cdot n$, where $\\\\ell=|\\\\sigma|$ and $n=|\\\\pi|$. In other words, the problem is fixed-parameter tractable parameterized by the size of the subpattern to be found. #R##N#We introduce a novel type of decompositions for permutations and a corresponding width measure. We present a linear-time algorithm that either finds $\\\\sigma$ as a subpattern of $\\\\pi$, or finds a decomposition of $\\\\pi$ whose width is bounded by a function of $|\\\\sigma|$. Then we show how to solve the \\\\textsc{Permutation Pattern} problem in linear time if a bounded-width decomposition is given in the input.',\n",
       "  'Title: wireless energy and information transfer in networks with hybrid arq\\nAbstract: In this paper, we consider a class of wireless powered communication devices using hybrid automatic repeat request (HARQ) protocol to ensure reliable communications. In particular, we analyze the trade-off between accumulating mutual information and harvesting RF energy at the receiver of a point-to-point link over a time-varying independent and identically distributed (i.i.d.) channel. The transmitter is assumed to have a constant energy source while the receiver relies, solely, on the RF energy harvested from the received signal. At each time slot, the incoming RF signal is split between information accumulation and energy accumulation with the objective of minimizing the expected number of re-transmissions. A major finding of this work is that the optimal policy minimizing the expected number of re-transmissions utilizes the incoming RF signal to either exclusively harvest energy or to accumulate mutual information. This finding enables achieving an optimal solution in feasible time by converting a two dimensional uncountable state Markov decision process (MDP) with continuous action space into a countable state MDP with binary decision space.',\n",
       "  'Title: tight bounds for lp samplers finding duplicates in streams and related problems\\nAbstract: In this paper, we present near-optimal space bounds for Lp-samplers. Given a stream of updates (additions and subtraction) to the coordinates of an underlying vector x \\\\in R^n, a perfect Lp sampler outputs the i-th coordinate with probability |x_i|^p/||x||_p^p. In SODA 2010, Monemizadeh and Woodruff showed polylog space upper bounds for approximate Lp-samplers and demonstrated various applications of them. Very recently, Andoni, Krauthgamer and Onak improved the upper bounds and gave a O(\\\\epsilon^{-p} log^3 n) space \\\\epsilon relative error and constant failure rate Lp-sampler for p \\\\in [1,2]. In this work, we give another such algorithm requiring only O(\\\\epsilon^{-p} log^2 n) space for p \\\\in (1,2). For p \\\\in (0,1), our space bound is O(\\\\epsilon^{-1} log^2 n), while for the $p=1$ case we have an O(log(1/\\\\epsilon)\\\\epsilon^{-1} log^2 n) space algorithm. We also give a O(log^2 n) bits zero relative error L0-sampler, improving the O(log^3 n) bits algorithm due to Frahling, Indyk and Sohler. #R##N#As an application of our samplers, we give better upper bounds for the problem of finding duplicates in data streams. In case the length of the stream is longer than the alphabet size, L1 sampling gives us an O(log^2 n) space algorithm, thus improving the previous O(log^3 n) bound due to Gopalan and Radhakrishnan. #R##N#In the second part of our work, we prove an Omega(log^2 n) lower bound for sampling from 0, \\\\pm 1 vectors (in this special case, the parameter p is not relevant for Lp sampling). This matches the space of our sampling algorithms for constant \\\\epsilon > 0. We also prove tight space lower bounds for the finding duplicates and heavy hitters problems. We obtain these lower bounds using reductions from the communication complexity problem augmented indexing.',\n",
       "  'Title: fairness aware ranking in search recommendation systems with application to linkedin talent search\\nAbstract: We present a framework for quantifying and mitigating algorithmic bias in mechanisms designed for ranking individuals, typically used as part of web-scale search and recommendation systems. We first propose complementary measures to quantify bias with respect to protected attributes such as gender and age. We then present algorithms for computing fairness-aware re-ranking of results. For a given search or recommendation task, our algorithms seek to achieve a desired distribution of top ranked results with respect to one or more protected attributes. We show that such a framework can be tailored to achieve fairness criteria such as equality of opportunity and demographic parity depending on the choice of the desired distribution. We evaluate the proposed algorithms via extensive simulations over different parameter choices, and study the effect of fairness-aware ranking on both bias and utility measures. We finally present the online A/B testing results from applying our framework towards representative ranking in LinkedIn Talent Search, and discuss the lessons learned in practice. Our approach resulted in tremendous improvement in the fairness metrics (nearly three fold increase in the number of search queries with representative results) without affecting the business metrics, which paved the way for deployment to 100% of LinkedIn Recruiter users worldwide. Ours is the first large-scale deployed framework for ensuring fairness in the hiring domain, with the potential positive impact for more than 630M LinkedIn members.',\n",
       "  'Title: distributed interference alignment with low overhead\\nAbstract: Based on closed-form interference alignment (IA) solutions, a low overhead distributed interference alignment (LOIA) scheme is proposed in this paper for the $K$-user SISO interference channel, and extension to multiple antenna scenario is also considered. Compared with the iterative interference alignment (IIA) algorithm proposed by Gomadam et al., the overhead is greatly reduced. Simulation results show that the IIA algorithm is strictly suboptimal compared with our LOIA algorithm in the overhead-limited scenario.',\n",
       "  'Title: detecting redundant css rules in html5 applications a tree rewriting approach\\nAbstract: HTML5 applications normally have a large set of CSS (Cascading Style Sheets) rules for data display. Each CSS rule consists of a node selector (given in an XPath-like query language) and a declaration block (assigning values to selected nodes\\' display attributes). As web applications evolve, maintaining CSS files can easily become problematic. Some CSS rules will be replaced by new ones, but these obsolete (hence redundant) CSS rules often remain in the applications. Not only does this \"bloat\" the applications, but it also significantly increases web browsers\\' processing time. Most works on detecting redundant CSS rules in HTML5 applications do not consider the dynamic behaviors of HTML5 (specified in JavaScript); in fact, the only proposed method that takes these into account is dynamic analysis (a.k.a. testing), which cannot soundly prove redundancy of CSS rules. In this paper, we introduce an abstraction of HTML5 applications based on monotonic tree-rewriting and study its \"redundancy problem\". We establish the precise complexity of the problem and various subproblems of practical importance (ranging from P to EXP). In particular, our algorithm relies on an efficient reduction to an analysis of symbolic pushdown systems (for which highly optimised solvers are available), which yields a fast method for checking redundancy in practice. We implemented our algorithm and demonstrated its efficacy in detecting redundant CSS rules in HTML5 applications.',\n",
       "  'Title: diverse palindromic factorization is np complete\\nAbstract: We prove that it is NP-complete to decide whether a given string can be factored into palindromes that are each unique in the factorization.',\n",
       "  'Title: temporal graph offset reconstruction towards temporally robust graph representation learning\\nAbstract: Graphs are a commonly used construct for representing relationships between elements in complex high dimensional datasets. Many real-world phenomenon are dynamic in nature, meaning that any graph used to represent them is inherently temporal. However, many of the machine learning models designed to capture knowledge about the structure of these graphs ignore this rich temporal information when creating representations of the graph. This results in models which do not perform well when used to make predictions about the future state of the graph -- especially when the delta between time stamps is not small. In this work, we explore a novel training procedure and an associated unsupervised model which creates graph representations optimised to predict the future state of the graph. We make use of graph convolutional neural networks to encode the graph into a latent representation, which we then use to train our temporal offset reconstruction method, inspired by auto-encoders, to predict a later time point -- multiple time steps into the future. Using our method, we demonstrate superior performance for the task of future link prediction compared with none-temporal state-of-the-art baselines. We show our approach to be capable of outperforming non-temporal baselines by 38% on a real world dataset.',\n",
       "  'Title: growth and duplication of public source code over time provenance tracking at scale\\nAbstract: We study the evolution of the largest known corpus of publicly available source code, i.e., the Software Heritage archive (4B unique source code files, 1B commits capturing their development histories across 50M software projects). On such corpus we quantify the growth rate of original, never-seen-before source code files and commits. We find the growth rates to be exponential over a period of more than 40 years.#R##N#We then estimate the multiplication factor, i.e., how much the same artifacts (e.g., files or commits) appear in different contexts (e.g., commits or source code distribution places). We observe a combinatorial explosion in the multiplication of identical source code files across different commits.#R##N#We discuss the implication of these findings for the problem of tracking the provenance of source code artifacts (e.g., where and when a given source code file or commit has been observed in the wild) for the entire body of publicly available source code. To that end we benchmark different data models for capturing software provenance information at this scale and growth rate. We identify a viable solution that is deployable on commodity hardware and appears to be maintainable for the foreseeable future.',\n",
       "  'Title: an improved separation of regular resolution from pool resolution and clause learning\\nAbstract: We prove that the graph tautology principles of Alekhnovich, Johannsen, Pitassi and Urquhart have polynomial size pool resolution refutations that use only input lemmas as learned clauses and without degenerate resolution inferences. We also prove that these graph tautology principles can be refuted by polynomial size DPLL proofs with clause learning, even when restricted to greedy, unit-propagating DPLL search.',\n",
       "  'Title: unsupervised bilingual lexicon induction from mono lingual multimodal data\\nAbstract: Bilingual lexicon induction, translating words from the source language to the target language, is a long-standing natural language processing task. Recent endeavors prove that it is promising to employ images as pivot to learn the lexicon induction without reliance on parallel corpora. However, these vision-based approaches simply associate words with entire images, which are constrained to translate concrete words and require object-centered images. We humans can understand words better when they are within a sentence with context. Therefore, in this paper, we propose to utilize images and their associated captions to address the limitations of previous approaches. We propose a multi-lingual caption model trained with different mono-lingual multimodal data to map words in different languages into joint spaces. Two types of word representation are induced from the multi-lingual caption model: linguistic features and localized visual features. The linguistic feature is learned from the sentence contexts with visual semantic constraints, which is beneficial to learn translation for words that are less visual-relevant. The localized visual feature is attended to the region in the image that correlates to the word, so that it alleviates the image restriction for salient visual representation. The two types of features are complementary for word translation. Experimental results on multiple language pairs demonstrate the effectiveness of our proposed method, which substantially outperforms previous vision-based approaches without using any parallel sentences or supervision of seed word pairs.',\n",
       "  \"Title: measuring personalization of web search\\nAbstract: Web search is an integral part of our daily lives. Recently, there has been a trend of personalization in Web search, where different users receive different results for the same search query. The increasing level of personalization is leading to concerns about Filter Bubble effects, where certain users are simply unable to access information that the search engines' algorithm decides is irrelevant. Despite these concerns, there has been little quantification of the extent of personalization in Web search today, or the user attributes that cause it. #R##N#In light of this situation, we make three contributions. First, we develop a methodology for measuring personalization in Web search results. While conceptually simple, there are numerous details that our methodology must handle in order to accurately attribute differences in search results to personalization. Second, we apply our methodology to 200 users on Google Web Search and 100 users on Bing. We find that, on average, 11.7% of results show differences due to personalization on Google, while 15.8% of results are personalized on Bing, but that this varies widely by search query and by result ranking. Third, we investigate the user features used to personalize on Google Web Search and Bing. Surprisingly, we only find measurable personalization as a result of searching with a logged in account and the IP address of the searching user. Our results are a first step towards understanding the extent and effects of personalization on Web search engines today.\",\n",
       "  'Title: importance weighted active learning\\nAbstract: We present a practical and statistically consistent scheme for actively learning binary classifiers under general loss functions. Our algorithm uses importance weighting to correct sampling bias, and by controlling the variance, we are able to give rigorous label complexity bounds for the learning process. Experiments on passively labeled data show that this approach reduces the label complexity required to achieve good predictive performance on many learning problems.',\n",
       "  'Title: global hashing system for fast image search\\nAbstract: Hashing methods have been widely investigated for fast approximate nearest neighbor searching in large data sets. Most existing methods use binary vectors in lower dimensional spaces to represent data points that are usually real vectors of higher dimensionality. We divide the hashing process into two steps. Data points are first embedded in a low-dimensional space, and the global positioning system method is subsequently introduced but modified for binary embedding. We devise dataindependent and data-dependent methods to distribute the satellites at appropriate locations. Our methods are based on finding the tradeoff between the information losses in these two steps. Experiments show that our data-dependent method outperforms other methods in different-sized data sets from 100k to 10M. By incorporating the orthogonality of the code matrix, both our data-independent and data-dependent methods are particularly impressive in experiments on longer bits.',\n",
       "  'Title: optimization over geodesics for exact principal geodesic analysis\\nAbstract: In fields ranging from computer vision to signal processing and statistics, increasing computational power allows a move from classical linear models to models that incorporate non-linear phenomena. This shift has created interest in computational aspects of differential geometry, and solving optimization problems that incorporate non-linear geometry constitutes an important computational task. In this paper, we develop methods for numerically solving optimization problems over spaces of geodesics using numerical integration of Jacobi fields and second order derivatives of geodesic families. As an important application of this optimization strategy, we compute exact Principal Geodesic Analysis (PGA), a non-linear version of the PCA dimensionality reduction procedure. By applying the exact PGA algorithm to synthetic data, we exemplify the differences between the linearized and exact algorithms caused by the non-linear geometry. In addition, we use the numerically integrated Jacobi fields to determine sectional curvatures and provide upper bounds for injectivity radii.',\n",
       "  'Title: low complexity qos aware coordinated scheduling for heterogenous networks\\nAbstract: In this paper, we consider a heterogenous network (HetNet), where low-power indoor femtocells are deployed in the coverage area of the existing macro base station (MBS). This paper proposes a novel coordinated random beamforming and user scheduling strategy to improve the throughput of users served by the femtocell access point (FAP) while satisfying the quality-of-service (QoS) requirements of users served by both MBS and FAP. The strategy, termed as QoS-Aware Coodinated Scheduling (QACS), requires limited coordination between the MBS and FAP, i.e., only the indexes of the qualified beams are shared. Exact statistical analysis for the ergodic achievable rate of both FAP and MBS with the proposed strategy are presented. Scheduling fairness is also addressed for the proposed QACS.',\n",
       "  'Title: dynamic index coding for wireless broadcast networks\\nAbstract: We consider a wireless broadcast station that transmits packets to multiple users. The packet requests for each user may overlap, and some users may already have certain packets. This presents a problem of broadcasting in the presence of side information, and is a generalization of the well known (and unsolved) index coding problem of information theory. Rather than achieving the full capacity region, we develop a code-constrained capacity region, which restricts attention to a pre-specified set of coding actions. We develop a dynamic max-weight algorithm that allows for random packet arrivals and supports any traffic inside the code-constrained capacity region. Further, we provide a simple set of codes based on cycles in the underlying demand graph. We show these codes are optimal for a class of broadcast relay problems.',\n",
       "  'Title: optimal space time codes for the mimo amplify and forward cooperative channel\\nAbstract: In this work, we extend the non-orthogonal amplify-and-forward (NAF) cooperative diversity scheme to the MIMO channel. A family of space-time block codes for a half-duplex MIMO NAF fading cooperative channel with N relays is constructed. The code construction is based on the non-vanishing determinant criterion (NVD) and is shown to achieve the optimal diversity-multiplexing tradeoff (DMT) of the channel. We provide a general explicit algebraic construction, followed by some examples. In particular, in the single relay case, it is proved that the Golden code and the 4x4 Perfect code are optimal for the single-antenna and two-antenna case, respectively. Simulation results reveal that a significant gain (up to 10dB) can be obtained with the proposed codes, especially in the single-antenna case.',\n",
       "  'Title: the price of anarchy in large games\\nAbstract: Game-theoretic models relevant for computer science applications usually feature a large number of players. The goal of this paper is to develop an analytical framework for bounding the price of anarchy in such models. We demonstrate the wide applicability of our framework through instantiations for several well-studied models, including simultaneous single-item auctions, greedy combinatorial auctions, and routing games. In all cases, we identify conditions under which the POA of large games is much better than that of worst-case instances. Our results also give new senses in which simple auctions can perform almost as well as optimal ones in realistic settings.',\n",
       "  'Title: expanded combinatorial designs as tool to model network slicing in 5g\\nAbstract: The network slice management function (NSMF) in 5G has a task to configure the network slice instances and to combine network slice subnet instances from the new-generation radio access network and the core network into an end-to-end network slice instance. In this paper, we propose a mathematical model for network slicing based on combinatorial designs such as Latin squares and rectangles and their conjugate forms. We extend those designs with attributes that offer different levels of abstraction. For one set of attributes we prove a stability Lemma for the necessary conditions to reach a stationary ergodic stage. We also introduce a definition of utilization ratio function and offer an algorithm for its maximization. Moreover, we provide algorithms that simulate the work of NSMF with randomized or optimized strategies, and we report the results of our implementation, experiments and simulations for one set of attributes.',\n",
       "  'Title: blackbox identity testing for bounded top fanin depth 3 circuits the field doesn t matter\\nAbstract: Let C be a depth-3 circuit with n variables, degree d and top fanin k (called sps(k,d,n) circuits) over base field F. It is a major open problem to design a deterministic polynomial time blackbox algorithm that tests if C is identically zero. Klivans & Spielman (STOC 2001) observed that the problem is open even when k is a constant. This case has been subjected to a serious study over the past few years, starting from the work of Dvir & Shpilka (STOC 2005). #R##N#We give the first polynomial time blackbox algorithm for this problem. Our algorithm runs in time poly(nd^k), regardless of the base field. The only field for which polynomial time algorithms were previously known is F=Q (Kayal & Saraf, FOCS 2009, and Saxena & Seshadhri, FOCS 2010). This is the first blackbox algorithm for depth-3 circuits that does not use the rank based approaches of Karnin & Shpilka (CCC 2008). #R##N#We prove an important tool for the study of depth-3 identities. We design a blackbox polynomial time transformation that reduces the number of variables in a sps(k,d,n) circuit to k variables, but preserves the identity structure.',\n",
       "  'Title: syndrome decoding of reed solomon codes beyond half the minimum distance based on shift register synthesis\\nAbstract: In this paper, a new approach for decoding low-rate Reed-Solomon codes beyond half the minimum distance is considered and analyzed. Unlike the Sudan algorithm published in 1997, this new approach is based on multi-sequence shift-register synthesis, which makes it easy to understand and simple to implement. The computational complexity of this shift-register based algorithm is of the same order as the complexity of the well-known Berlekamp-Massey algorithm. Moreover, the error correcting radius coincides with the error correcting radius of the original Sudan algorithm, and the practical decoding performance observed on a q-ary symmetric channel (QSC) is virtually identical to the decoding performance of the Sudan algorithm. Bounds for the failure and error probability as well as for the QSC decoding performance of the new algorithm are derived, and the performance is illustrated by means of examples.',\n",
       "  'Title: metrics for finite markov decision processes\\nAbstract: We present metrics for measuring the similarity of states in a finite Markov decision process (MDP). The formulation of our metrics is based on the notion of bisimulation for MDPs, with an aim towards solving discounted infinite horizon reinforcement learning tasks. Such metrics can be used to aggregate states, as well as to better structure other value function approximators (e.g., memory-based or nearest-neighbor approximators). We provide bounds that relate our metric distances to the optimal values of states in the given MDP.',\n",
       "  'Title: diffusion and superposition distances for signals supported on networks\\nAbstract: We introduce the diffusion and superposition distances as two metrics to compare signals supported in the nodes of a network. Both metrics consider the given vectors as initial temperature distributions and diffuse heat trough the edges of the graph. The similarity between the given vectors is determined by the similarity of the respective diffusion profiles. The superposition distance computes the instantaneous difference between the diffused signals and integrates the difference over time. The diffusion distance determines a distance between the integrals of the diffused signals. We prove that both distances define valid metrics and that they are stable to perturbations in the underlying network. We utilize numerical experiments to illustrate their utility in classifying signals in a synthetic network as well as in classifying ovarian cancer histologies using gene mutation profiles of different patients. We also reinterpret diffusion as a transformation of interrelated feature spaces and use it as preprocessing tool for learning. We use diffusion to increase the accuracy of handwritten digit classification.',\n",
       "  \"Title: algorithmic discrepancy beyond partial coloring\\nAbstract: The partial coloring method is one of the most powerful and widely used method in combinatorial discrepancy problems. However, in many cases it leads to sub-optimal bounds as the partial coloring step must be iterated a logarithmic number of times, and the errors can add up in an adversarial way. We give a new and general algorithmic framework that overcomes the limitations of the partial coloring method and can be applied in a black-box manner to various problems. Using this framework, we give new improved bounds and algorithms for several classic problems in discrepancy. In particular, for Tusnady's problem, we give an improved $O(\\\\log^2 n)$ bound for discrepancy of axis-parallel rectangles and more generally an $O_d(\\\\log^dn)$ bound for $d$-dimensional boxes in $\\\\mathbb{R}^d$. Previously, even non-constructively, the best bounds were $O(\\\\log^{2.5} n)$ and $O_d(\\\\log^{d+0.5}n)$ respectively. Similarly, for the Steinitz problem we give the first algorithm that matches the best known non-constructive bounds due to Banaszczyk \\\\cite{Bana12} in the $\\\\ell_\\\\infty$ case, and improves the previous algorithmic bounds substantially in the $\\\\ell_2$ case. Our framework is based upon a substantial generalization of the techniques developed recently in the context of the Koml\\\\'{o}s discrepancy problem [BDG16].\",\n",
       "  'Title: distributed detection of a non cooperative target via generalized locally optimum approaches\\nAbstract: In this paper we tackle distributed detection of a non-cooperative target with a Wireless Sensor Network (WSN). When the target is present, sensors observe an unknown random signal with amplitude attenuation depending on the distance between the sensor and the target (unknown) positions, embedded in white Gaussian noise. The Fusion Center (FC) receives sensors decisions through error-prone Binary Symmetric Channels (BSCs) and is in charge of performing a (potentially) more-accurate global decision. The resulting problem is a one-sided testing with nuisance parameters present only under the target-present hypothesis. We first focus on fusion rules based on Generalized Likelihood Ratio Test (GLRT), Bayesian and hybrid approaches. Then, aimed at reducing the computational complexity, we develop fusion rules based on generalizations of the well-known Locally-Optimum Detection (LOD) framework. Finally, all the proposed rules are compared in terms of performance and complexity.',\n",
       "  'Title: a message passing approach for joint channel estimation interference mitigation and decoding\\nAbstract: Channel uncertainty and co-channel interference are two major challenges in the design of wireless systems such as future generation cellular networks. This paper studies receiver design for a wireless channel model with both time-varying Rayleigh fading and strong co-channel interference of similar form as the desired signal. It is assumed that the channel coefficients of the desired signal can be estimated through the use of pilots, whereas no pilot for the interference signal is available, as is the case in many practical wireless systems. Because the interference process is non-Gaussian, treating it as Gaussian noise generally often leads to unacceptable performance. In order to exploit the statistics of the interference and correlated fading in time, an iterative message-passing architecture is proposed for joint channel estimation, interference mitigation and decoding. Each message takes the form of a mixture of Gaussian densities where the number of components is limited so that the overall complexity of the receiver is constant per symbol regardless of the frame and code lengths. Simulation of both coded and uncoded systems shows that the receiver performs significantly better than conventional receivers with linear channel estimation, and is robust with respect to mismatch in the assumed fading model.',\n",
       "  \"Title: signature generation for sensitive information leakage in android applications\\nAbstract: In recent years, there has been rapid growth in mobile devices such as smartphones, and a number of applications are developed specifically for the smartphone market. In particular, there are many applications that are ``free'' to the user, but depend on advertisement services for their revenue. Such applications include an advertisement module - a library provided by the advertisement service - that can collect a user's sensitive information and transmit it across the network. Users accept this business model, but in most cases the applications do not require the user's acknowledgment in order to transmit sensitive information. Therefore, such applications' behavior becomes an invasion of privacy. In our analysis of 1,188 Android applications' network traffic and permissions, 93% of the applications we analyzed connected to multiple destinations when using the network. 61% required a permission combination that included both access to sensitive information and use of networking services. These applications have the potential to leak the user's sensitive information. In an effort to enable users to control the transmission of their private information, we propose a system which, using a novel clustering method based on the HTTP packet destination and content distances, generates signatures from the clustering result and uses them to detect sensitive information leakage from Android applications. Our system does not require an Android framework modification or any special privileges. Thus users can easily introduce our system to their devices, and manage suspicious applications' network behavior in a fine grained manner. Our system accurately detected 94% of the sensitive information leakage from the applications evaluated and produced only 5% false negative results, and less than 3% false positive results.\",\n",
       "  'Title: latent distribution assumption for unbiased and consistent consensus modelling\\nAbstract: We study the problem of aggregation noisy labels. Usually, it is solved by proposing a stochastic model for the process of generating noisy labels and then estimating the model parameters using the observed noisy labels. A traditional assumption underlying previously introduced generative models is that each object has one latent true label. In contrast, we introduce a novel latent distribution assumption, implying that a unique true label for an object might not exist, but rather each object might have a specific distribution generating a latent subjective label each time the object is observed. Our experiments showed that the novel assumption is more suitable for difficult tasks, when there is an ambiguity in choosing a \"true\" label for certain objects.',\n",
       "  'Title: perspectroscope a window to the world of diverse perspectives\\nAbstract: This work presents PerspectroScope, a web-based system which lets users query a discussion-worthy natural language claim, and extract and visualize various perspectives in support or against the claim, along with evidence supporting each perspective. The system thus lets users explore various perspectives that could touch upon aspects of the issue at hand.The system is built as a combination of retrieval engines and learned textual-entailment-like classifiers built using a few recent developments in natural language understanding. To make the system more adaptive, expand its coverage, and improve its decisions over time, our platform employs various mechanisms to get corrections from the users. #R##N#PerspectroScope is available at this http URL.',\n",
       "  'Title: conflict anticipation in the search for graph automorphisms\\nAbstract: Effective search for graph automorphisms allows identifying symmetries in many discrete structures, ranging from chemical molecules to microprocessor circuits. Using this type of structure can enhance visualization as well as speed up computational optimization and verification. Competitive algorithms for the graph automorphism problem are based on efficient partition refinement augmented with group-theoretic pruning techniques. In this paper, we improve prior algorithms for the graph automorphism problem by introducing simultaneous refinement of multiple partitions, which enables the anticipation of future conflicts in search and leads to significant pruning, reducing overall runtimes. Empirically, we observe an exponential speedup for the family of Miyazaki graphs, which have been shown to impede leading graph-automorphism algorithms.',\n",
       "  'Title: controller synthesis for discrete time hybrid polynomial systems via occupation measures\\nAbstract: In this paper, we design nonlinear state feedback controllers for discrete-time polynomial dynamical systems via the occupation measure approach. We propose the discrete-time controlled Liouville equation, and use it to formulate the controller synthesis problem as an infinite-dimensional linear programming problem on measures, which is then relaxed as finite-dimensional semidefinite programming problems on moments of measures and their duals on sums-of-squares polynomials. Nonlinear controllers can be extracted from the solutions to the relaxed problems. The advantage of the occupation measure approach is that we solve convex problems instead of generally non-convex problems, and the computational complexity is polynomial in the state and input dimensions, and hence the approach is more scalable. In addition, we show that the approach can be applied to over-approximating the backward reachable set of discrete-time autonomous polynomial systems and the controllable set of discrete-time polynomial systems under known state feedback control laws. We illustrate our approach on several dynamical systems.',\n",
       "  \"Title: the power of local information in social networks\\nAbstract: We study the power of \\\\textit{local information algorithms} for optimization problems on social networks. We focus on sequential algorithms for which the network topology is initially unknown and is revealed only within a local neighborhood of vertices that have been irrevocably added to the output set. The distinguishing feature of this setting is that locality is necessitated by constraints on the network information visible to the algorithm, rather than being desirable for reasons of efficiency or parallelizability. In this sense, changes to the level of network visibility can have a significant impact on algorithm design. #R##N#We study a range of problems under this model of algorithms with local information. We first consider the case in which the underlying graph is a preferential attachment network. We show that one can find the node of maximum degree in the network in a polylogarithmic number of steps, using an opportunistic algorithm that repeatedly queries the visible node of maximum degree. This addresses an open question of Bollob{\\\\'a}s and Riordan. In contrast, local information algorithms require a linear number of queries to solve the problem on arbitrary networks. #R##N#Motivated by problems faced by recruiters in online networks, we also consider network coverage problems such as finding a minimum dominating set. For this optimization problem we show that, if each node added to the output set reveals sufficient information about the set's neighborhood, then it is possible to design randomized algorithms for general networks that nearly match the best approximations possible even with full access to the graph structure. We show that this level of visibility is necessary. #R##N#We conclude that a network provider's decision of how much structure to make visible to its users can have a significant effect on a user's ability to interact strategically with the network.\",\n",
       "  'Title: provable algorithms for inference in topic models\\nAbstract: Recently, there has been considerable progress on designing algorithms with provable guarantees -- typically using linear algebraic methods -- for parameter learning in latent variable models. But designing provable algorithms for inference has proven to be more challenging. Here we take a first step towards provable inference in topic models. We leverage a property of topic models that enables us to construct simple linear estimators for the unknown topic proportions that have small variance, and consequently can work with short documents. Our estimators also correspond to finding an estimate around which the posterior is well-concentrated. We show lower bounds that for shorter documents it can be information theoretically impossible to find the hidden topics. Finally, we give empirical results that demonstrate that our algorithm works on realistic topic models. It yields good solutions on synthetic data and runs in time comparable to a {\\\\em single} iteration of Gibbs sampling.',\n",
       "  \"Title: multi task classification hypothesis space with improved generalization bounds\\nAbstract: This paper presents a RKHS, in general, of vector-valued functions intended to be used as hypothesis space for multi-task classification. It extends similar hypothesis spaces that have previously considered in the literature. Assuming this space, an improved Empirical Rademacher Complexity-based generalization bound is derived. The analysis is itself extended to an MKL setting. The connection between the proposed hypothesis space and a Group-Lasso type regularizer is discussed. Finally, experimental results, with some SVM-based Multi-Task Learning problems, underline the quality of the derived bounds and validate the paper's analysis.\",\n",
       "  'Title: cp decomposition with tensor power method for convolutional neural networks compression\\nAbstract: Convolutional Neural Networks (CNNs) has shown a great success in many areas including complex image classification tasks. However, they need a lot of memory and computational cost, which hinders them from running in relatively low-end smart devices such as smart phones. We propose a CNN compression method based on CP-decomposition and Tensor Power Method. We also propose an iterative fine tuning, with which we fine-tune the whole network after decomposing each layer, but before decomposing the next layer. Significant reduction in memory and computation cost is achieved compared to state-of-the-art previous work with no more accuracy loss.',\n",
       "  'Title: canonical models and the complexity of modal team logic\\nAbstract: We study modal team logic MTL, the team-semantical extension of modal logic ML closed under Boolean negation. Its fragments, such as modal dependence, independence, and inclusion logic, are well-understood. However, due to the unrestricted Boolean negation, the satisfiability problem of full MTL has been notoriously resistant to a complexity theoretical classification. #R##N#In our approach, we introduce the notion of canonical models into the team-semantical setting. By construction of such a model, we reduce the satisfiability problem of MTL to simple model checking. Afterwards, we show that this approach is optimal in the sense that MTL-formulas can efficiently enforce canonicity. #R##N#Furthermore, to capture these results in terms of complexity, we introduce a non-elementary complexity class, TOWER(poly), and prove that it contains satisfiability and validity of MTL as complete problems. We also prove that the fragments of MTL with bounded modal depth are complete for the levels of the elementary hierarchy (with polynomially many alternations). The respective hardness results hold for both strict or lax semantics of the modal operators and the splitting disjunction, and also over the class of reflexive and transitive frames.',\n",
       "  'Title: query adaptive hash code ranking for large scale multi view visual search\\nAbstract: Hash based nearest neighbor search has become attractive in many applications. However, the quantization in hashing usually degenerates the discriminative power when using Hamming distance ranking. Besides, for large-scale visual search, existing hashing methods cannot directly support the efficient search over the data with multiple sources, and while the literature has shown that adaptively incorporating complementary information from diverse sources or views can significantly boost the search performance. To address the problems, this paper proposes a novel and generic approach to building multiple hash tables with multiple views and generating fine-grained ranking results at bitwise and tablewise levels. For each hash table, a query-adaptive bitwise weighting is introduced to alleviate the quantization loss by simultaneously exploiting the quality of hash functions and their complement for nearest neighbor search. From the tablewise aspect, multiple hash tables are built for different data views as a joint index, over which a query-specific rank fusion is proposed to rerank all results from the bitwise ranking by diffusing in a graph. Comprehensive experiments on image search over three well-known benchmarks show that the proposed method achieves up to 17.11% and 20.28% performance gains on single and multiple table search over state-of-the-art methods.',\n",
       "  'Title: how clever is the film model and how clever can it be\\nAbstract: The FiLM model achieves close-to-perfect performance on the diagnostic CLEVR dataset and is distinguished from other such models by having a comparatively simple and easily transferable architecture. In this paper, we investigate in more detail the ability of FiLM to learn various linguistic constructions. Our main results show that (a) FiLM is not able to learn relational statements straight away except for very simple instances, (b) training on a broader set of instances as well as pretraining on simpler instance types can help alleviate these learning difficulties, (c) mixing is less robust than pretraining and very sensitive to the compositional structure of the dataset. Overall, our results suggest that the approach of big all-encompassing datasets and the paradigm of \"the effectiveness of data\" may have fundamental limitations.',\n",
       "  'Title: strong equivalence of qualitative optimization problems\\nAbstract: We introduce the framework of qualitative optimization problems (or, simply, optimization problems) to represent preference theories. The formalism uses separate modules to describe the space of outcomes to be compared (the generator) and the preferences on outcomes (the selector). We consider two types of optimization problems. They differ in the way the generator, which we model by a propositional theory, is interpreted: by the standard propositional logic semantics, and by the equilibrium-model (answer-set) semantics. Under the latter interpretation of generators, optimization problems directly generalize answer-set optimization programs proposed previously. We study strong equivalence of optimization problems, which guarantees their interchangeability within any larger context. We characterize several versions of strong equivalence obtained by restricting the class of optimization problems that can be used as extensions and establish the complexity of associated reasoning tasks. Understanding strong equivalence is essential for modular representation of optimization problems and rewriting techniques to simplify them without changing their inherent properties.',\n",
       "  'Title: sembed semantic embedding of egocentric action videos\\nAbstract: We present SEMBED, an approach for embedding an egocentric object interaction video in a semantic-visual graph to estimate the probability distribution over its potential semantic labels. When object interactions are annotated using unbounded choice of verbs, we embrace the wealth and ambiguity of these labels by capturing the semantic relationships as well as the visual similarities over motion and appearance features. We show how SEMBED can interpret a challenging dataset of 1225 freely annotated egocentric videos, outperforming SVM classification by more than 5%.',\n",
       "  'Title: new approaches for almost sure termination of probabilistic programs\\nAbstract: We study the almost-sure termination problem for probabilistic programs. First, we show that supermartingales with lower bounds on conditional absolute difference provide a sound approach for the almost-sure termination problem. Moreover, using this approach we can obtain explicit optimal bounds on tail probabilities of non-termination within a given number of steps. Second, we present a new approach based on Central Limit Theorem for the almost-sure termination problem, and show that this approach can establish almost-sure termination of programs which none of the existing approaches can handle. Finally, we discuss algorithmic approaches for the two above methods that lead to automated analysis techniques for almost-sure termination of probabilistic programs.',\n",
       "  \"Title: decomposition of multi agent planning under distributed motion and task ltl specifications\\nAbstract: The aim of this work is to introduce an efficient procedure for discrete multi-agent planning under local complex temporal logic behavior specifications. While the first part of an agent's behavior specification constraints the agent's trace and is independent, the second part of the specification expresses the agent's tasks in terms of the services to be provided along the trace and may impose requests for the other agents' collaborations. To fight the extreme computational complexity of centralized multi-agent planning, we propose a two-phase automata-based solution, where we systematically decouple the planning procedure for the two types of specifications. At first, we only consider the former specifications in a fully decentralized way and we compactly represent each agents' admissible traces by abstracting away the states that are insignificant for the satisfaction of their latter specifications. Second, the synchronized planning procedure uses only the compact representations. The satisfaction of the overall specification is guaranteed by construction for each agent. An illustrative example demonstrating the practical benefits of the solution is included.\",\n",
       "  'Title: indexing schemes for similarity search in datasets of short protein fragments\\nAbstract: We propose a family of very efficient hierarchical indexing schemes for ungapped, score matrix-based similarity search in large datasets of short (4-12 amino acid) protein fragments. This type of similarity search has importance in both providing a building block to more complex algorithms and for possible use in direct biological investigations where datasets are of the order of 60 million objects. Our scheme is based on the internal geometry of the amino acid alphabet and performs exceptionally well, for example outputting 100 nearest neighbours to any possible fragment of length 10 after scanning on average less than one per cent of the entire dataset.',\n",
       "  'Title: 3d convolutional neural networks for tumor segmentation using long range 2d context\\nAbstract: We present an efficient deep learning approach for the challenging task of tumor segmentation in multisequence MR images. In recent years, Convolutional Neural Networks (CNN) have achieved state-of-the-art performances in a large variety of recognition tasks in medical imaging. Because of the considerable computational cost of CNNs, large volumes such as MRI are typically processed by subvolumes, for instance slices (axial, coronal, sagittal) or small 3D patches. In this paper we introduce a CNN-based model which efficiently combines the advantages of the short-range 3D context and the long-range 2D context. To overcome the limitations of specific choices of neural network architectures, we also propose to merge outputs of several cascaded 2D-3D models by a voxelwise voting strategy. Furthermore, we propose a network architecture in which the different MR sequences are processed by separate subnetworks in order to be more robust to the problem of missing MR sequences. Finally, a simple and efficient algorithm for training large CNN models is introduced. We evaluate our method on the public benchmark of the BRATS 2017 challenge on the task of multiclass segmentation of malignant brain tumors. Our method achieves good performances and produces accurate segmentations with median Dice scores of 0.918 (whole tumor), 0.883 (tumor core) and 0.854 (enhancing core). Our approach can be naturally applied to various tasks involving segmentation of lesions or organs.',\n",
       "  'Title: two hop interference channels impact of linear time varying schemes\\nAbstract: We consider the two-hop interference channel (IC) with constant real channel coefficients, which consists of two source-destination pairs, separated by two relays. We analyze the achievable degrees of freedom (DoF) of such network when relays are restricted to perform scalar amplify-forward (AF) operations, with possibly time-varying coefficients. We show that, somewhat surprisingly, by providing the flexibility of choosing time-varying AF coefficients at the relays, it is possible to achieve 4/3 sum-DoF. We also develop a novel outer bound that matches our achievability, hence characterizing the sum-DoF of two-hop interference channels with time-varying AF relaying strategies.',\n",
       "  \"Title: the range of topological effects on communication\\nAbstract: We continue the study of communication cost of computing functions when inputs are distributed among $k$ processors, each of which is located at one vertex of a network/graph called a terminal. Every other node of the network also has a processor, with no input. The communication is point-to-point and the cost is the total number of bits exchanged by the protocol, in the worst case, on all edges. #R##N#Chattopadhyay, Radhakrishnan and Rudra (FOCS'14) recently initiated a study of the effect of topology of the network on the total communication cost using tools from $L_1$ embeddings. Their techniques provided tight bounds for simple functions like Element-Distinctness (ED), which depend on the 1-median of the graph. This work addresses two other kinds of natural functions. We show that for a large class of natural functions like Set-Disjointness the communication cost is essentially $n$ times the cost of the optimal Steiner tree connecting the terminals. Further, we show for natural composed functions like $\\\\text{ED} \\\\circ \\\\text{XOR}$ and $\\\\text{XOR} \\\\circ \\\\text{ED}$, the naive protocols suggested by their definition is optimal for general networks. Interestingly, the bounds for these functions depend on more involved topological parameters that are a combination of Steiner tree and 1-median costs. #R##N#To obtain our results, we use some new tools in addition to ones used in Chattopadhyay et. al. These include (i) viewing the communication constraints via a linear program; (ii) using tools from the theory of tree embeddings to prove topology sensitive direct sum results that handle the case of composed functions and (iii) representing the communication constraints of certain problems as a family of collection of multiway cuts, where each multiway cut simulates the hardness of computing the function on the star topology.\",\n",
       "  \"Title: open answer set programming with guarded programs\\nAbstract: Open answer set programming (OASP) is an extension of answer set programming where one may ground a program with an arbitrary superset of the program's constants. We define a fixed point logic (FPL) extension of Clark's completion such that open answer sets correspond to models of FPL formulas and identify a syntactic subclass of programs, called (loosely) guarded programs. Whereas reasoning with general programs in OASP is undecidable, the FPL translation of (loosely) guarded programs falls in the decidable (loosely) guarded fixed point logic (mu(L)GF). Moreover, we reduce normal closed ASP to loosely guarded OASP, enabling for the first time, a characterization of an answer set semantics by muLGF formulas. We further extend the open answer set semantics for programs with generalized literals. Such generalized programs (gPs) have interesting properties, e.g., the ability to express infinity axioms. We restrict the syntax of gPs such that both rules and generalized literals are guarded. Via a translation to guarded fixed point logic, we deduce 2-exptime-completeness of satisfiability checking in such guarded gPs (GgPs). Bound GgPs are restricted GgPs with exptime-complete satisfiability checking, but still sufficiently expressive to optimally simulate computation tree logic (CTL). We translate Datalog lite programs to GgPs, establishing equivalence of GgPs under an open answer set semantics, alternation-free muGF, and Datalog lite.\",\n",
       "  'Title: polymorphic types in acl2\\nAbstract: This paper describes a tool suite for the ACL2 programming language which incorporates certain ideas from the Hindley-Milner paradigm of functional programming (as exemplified in popular languages like ML and Haskell), including a \"typed\" style of programming with the ability to define polymorphic types. These ideas are introduced via macros into the language of ACL2, taking advantage of ACL2\\'s guard-checking mechanism to perform type checking on both function definitions and theorems. Finally, we discuss how these macros were used to implement features of Specware, a software specification and implementation system.',\n",
       "  'Title: on counting untyped lambda terms\\nAbstract: We present several results on counting untyped lambda terms, i.e., on telling how many terms belong to such or such class, according to the size of the terms and/or to the number of free variables.',\n",
       "  'Title: an introduction to deep learning for the physical layer\\nAbstract: We present and discuss several novel applications of deep learning for the physical layer. By interpreting a communications system as an autoencoder, we develop a fundamental new way to think about communications system design as an end-to-end reconstruction task that seeks to jointly optimize transmitter and receiver components in a single process. We show how this idea can be extended to networks of multiple transmitters and receivers and present the concept of radio transformer networks as a means to incorporate expert domain knowledge in the machine learning model. Lastly, we demonstrate the application of convolutional neural networks on raw IQ samples for modulation classification which achieves competitive accuracy with respect to traditional schemes relying on expert features. The paper is concluded with a discussion of open challenges and areas for future investigation.',\n",
       "  'Title: output polynomial enumeration on graphs of bounded local linear mim width\\nAbstract: The linear induced matching width (LMIM-width) of a graph is a width parameter defined by using the notion of branch-decompositions of a set function on ternary trees. In this paper we study output-polynomial enumeration algorithms on graphs of bounded LMIM-width and graphs of bounded local LMIM-width. In particular, we show that all 1-minimal and all 1-maximal (\\\\sigma,\\\\rho)-dominating sets, and hence all minimal dominating sets, of graphs of bounded LMIM-width can be enumerated with polynomial (linear) delay using polynomial space. Furthermore, we show that all minimal dominating sets of a unit square graph can be enumerated in incremental polynomial time.',\n",
       "  \"Title: automatic 3d bi ventricular segmentation of cardiac images by a shape constrained multi task deep learning approach\\nAbstract: Deep learning approaches have achieved state-of-the-art performance in cardiac magnetic resonance (CMR) image segmentation. However, most approaches have focused on learning image intensity features for segmentation, whereas the incorporation of anatomical shape priors has received less attention. In this paper, we combine a multi-task deep learning approach with atlas propagation to develop a shape-constrained bi-ventricular segmentation pipeline for short-axis CMR volumetric images. The pipeline first employs a fully convolutional network (FCN) that learns segmentation and landmark localisation tasks simultaneously. The architecture of the proposed FCN uses a 2.5D representation, thus combining the computational advantage of 2D FCNs networks and the capability of addressing 3D spatial consistency without compromising segmentation accuracy. Moreover, the refinement step is designed to explicitly enforce a shape constraint and improve segmentation quality. This step is effective for overcoming image artefacts (e.g. due to different breath-hold positions and large slice thickness), which preclude the creation of anatomically meaningful 3D cardiac shapes. The proposed pipeline is fully automated, due to network's ability to infer landmarks, which are then used downstream in the pipeline to initialise atlas propagation. We validate the pipeline on 1831 healthy subjects and 649 subjects with pulmonary hypertension. Extensive numerical experiments on the two datasets demonstrate that our proposed method is robust and capable of producing accurate, high-resolution and anatomically smooth bi-ventricular 3D models, despite the artefacts in input CMR volumes.\",\n",
       "  \"Title: from artifacts to aggregations modeling scientific life cycles on the semantic web\\nAbstract: In the process of scientific research, many information objects are generated, all of which may remain valuable indefinitely. However, artifacts such as instrument data and associated calibration information may have little value in isolation; their meaning is derived from their relationships to each other. Individual artifacts are best represented as components of a life cycle that is specific to a scientific research domain or project. Current cataloging practices do not describe objects at a sufficient level of granularity nor do they offer the globally persistent identifiers necessary to discover and manage scholarly products with World Wide Web standards. The Open Archives Initiative's Object Reuse and Exchange data model (OAI-ORE) meets these requirements. We demonstrate a conceptual implementation of OAI-ORE to represent the scientific life cycles of embedded networked sensor applications in seismology and environmental sciences. By establishing relationships between publications, data, and contextual research information, we illustrate how to obtain a richer and more realistic view of scientific practices. That view can facilitate new forms of scientific research and learning. Our analysis is framed by studies of scientific practices in a large, multi-disciplinary, multi-university science and engineering research center, the Center for Embedded Networked Sensing (CENS).\",\n",
       "  'Title: tracking large scale video remix in real world events\\nAbstract: Social information networks, such as YouTube, contains traces of both explicit online interaction (such as \"like\", leaving a comment, or subscribing to video feed), and latent interactions (such as quoting, or remixing parts of a video). We propose visual memes, or frequently re-posted short video segments, for tracking such latent video interactions at scale. Visual memes are extracted by scalable detection algorithms that we develop, with high accuracy. We further augment visual memes with text, via a statistical model of latent topics. We model content interactions on YouTube with visual memes, defining several measures of influence and building predictive models for meme popularity. Experiments are carried out on with over 2 million video shots from more than 40,000 videos on two prominent news events in 2009: the election in Iran and the swine flu epidemic. In these two events, a high percentage of videos contain remixed content, and it is apparent that traditional news media and citizen journalists have different roles in disseminating remixed content. We perform two quantitative evaluations for annotating visual memes and predicting their popularity. The joint statistical model of visual memes and words outperform a concurrence model, and the average error is ~2% for predicting meme volume and ~17% for their lifespan.',\n",
       "  \"Title: randomly removing g handles at once\\nAbstract: Indyk and Sidiropoulos (2007) proved that any orientable graph of genus $g$ can be probabilistically embedded into a graph of genus $g-1$ with constant distortion. Viewing a graph of genus $g$ as embedded on the surface of a sphere with $g$ handles attached, Indyk and Sidiropoulos' method gives an embedding into a distribution over planar graphs with distortion $2^{O(g)}$, by iteratively removing the handles. By removing all $g$ handles at once, we present a probabilistic embedding with distortion $O(g^2)$ for both orientable and non-orientable graphs. Our result is obtained by showing that the nimum-cut graph of Erickson and Har Peled (2004) has low dilation, and then randomly cutting this graph out of the surface using the Peeling Lemma of Lee and Sidiropoulos (2009).\",\n",
       "  'Title: parameterized approximation schemes using graph widths\\nAbstract: Combining the techniques of approximation algorithms and parameterized complexity has long been considered a promising research area, but relatively few results are currently known. In this paper we study the parameterized approximability of a number of problems which are known to be hard to solve exactly when parameterized by treewidth or clique-width. Our main contribution is to present a natural randomized rounding technique that extends well-known ideas and can be used for both of these widths. Applying this very generic technique we obtain approximation schemes for a number of problems, evading both polynomial-time inapproximability and parameterized intractability bounds.',\n",
       "  'Title: learnable histogram statistical context features for deep neural networks\\nAbstract: Statistical features, such as histogram, Bag-of-Words (BoW) and Fisher Vector, were commonly used with hand-crafted features in conventional classification methods, but attract less attention since the popularity of deep learning methods. In this paper, we propose a learnable histogram layer, which learns histogram features within deep neural networks in end-to-end training. Such a layer is able to back-propagate (BP) errors, learn optimal bin centers and bin widths, and be jointly optimized with other layers in deep networks during training. Two vision problems, semantic segmentation and object detection, are explored by integrating the learnable histogram layer into deep networks, which show that the proposed layer could be well generalized to different applications. In-depth investigations are conducted to provide insights on the newly introduced layer.',\n",
       "  'Title: dependent hierarchical normalized random measures for dynamic topic modeling\\nAbstract: We develop dependent hierarchical normalized random measures and apply them to dynamic topic modeling. The dependency arises via superposition, subsampling and point transition on the underlying Poisson processes of these measures. The measures used include normalised generalised Gamma processes that demonstrate power law properties, unlike Dirichlet processes used previously in dynamic topic modeling. Inference for the model includes adapting a recently developed slice sampler to directly manipulate the underlying Poisson process. Experiments performed on news, blogs, academic and Twitter collections demonstrate the technique gives superior perplexity over a number of previous models.',\n",
       "  \"Title: compute and forward on a multiaccess relay channel coding and symmetric rate optimization\\nAbstract: We consider a system in which two users communicate with a destination with the help of a half-duplex relay. Based on the compute-and-forward scheme, we develop and evaluate the performance of coding strategies that are of network coding spirit. In this framework, instead of decoding the users' information messages, the destination decodes two integer-valued linear combinations that relate the transmitted codewords. Two decoding schemes are considered. In the first one, the relay computes one of the linear combinations and then forwards it to the destination. The destination computes the other linear combination based on the direct transmissions. In the second one, accounting for the side information available at the destination through the direct links, the relay compresses what it gets using Wyner-Ziv compression and conveys it to the destination. The destination then computes the two linear combinations, locally. For both coding schemes, we discuss the design criteria, and derive the allowed symmetric-rate. Next, we address the power allocation and the selection of the integer-valued coefficients to maximize the offered symmetric-rate; an iterative coordinate descent method is proposed. The analysis shows that the first scheme can outperform standard relaying techniques in certain regimes, and the second scheme, while relying on feasible structured lattice codes, can at best achieve the same performance as regular compress-and-forward for the multiaccess relay network model that we study. The results are illustrated through some numerical examples.\",\n",
       "  'Title: sparse recovery with graph constraints fundamental limits and measurement construction\\nAbstract: This paper addresses the problem of sparse recovery with graph constraints in the sense that we can take additive measurements over nodes only if they induce a connected subgraph. We provide explicit measurement constructions for several special graphs. A general measurement construction algorithm is also proposed and evaluated. For any given graph $G$ with $n$ nodes, we derive order optimal upper bounds of the minimum number of measurements needed to recover any $k$-sparse vector over $G$ ($M^G_{k,n}$). Our study suggests that $M^G_{k,n}$ may serve as a graph connectivity metric.',\n",
       "  'Title: the polysemy of the words that children learn over time\\nAbstract: Here we study polysemy as a potential learning bias in vocabulary learning in children. Words of low polysemy could be preferred as they reduce the disambiguation effort for the listener. However, such preference could be a side-effect of another bias: the preference of children for nouns in combination with the lower polysemy of nouns with respect to other part-of-speech categories. Our results show that mean polysemy in children increases over time in two phases, i.e. a fast growth till the 31st month followed by a slower tendency towards adult speech. In contrast, this evolution is not found in adults interacting with children. This suggests that children have a preference for non-polysemous words in their early stages of vocabulary acquisition. Interestingly, the evolutionary pattern described above weakens when controlling for syntactic category (noun, verb, adjective or adverb) but it does not disappear completely, suggesting that it could result from acombination of a standalone bias for low polysemy and a preference for nouns.',\n",
       "  'Title: basis collapse for holographic algorithms over all domain sizes\\nAbstract: The theory of holographic algorithms introduced by Valiant represents a novel approach to achieving polynomial-time algorithms for seemingly intractable counting problems via a reduction to counting planar perfect matchings and a linear change of basis. Two fundamental parameters in holographic algorithms are the \\\\emph{domain size} and the \\\\emph{basis size}. Roughly, the domain size is the range of colors involved in the counting problem at hand (e.g. counting graph $k$-colorings is a problem over domain size $k$), while the basis size $\\\\ell$ captures the dimensionality of the representation of those colors. A major open problem has been: for a given $k$, what is the smallest $\\\\ell$ for which any holographic algorithm for a problem over domain size $k$ \"collapses to\" (can be simulated by) a holographic algorithm with basis size $\\\\ell$? Cai and Lu showed in 2008 that over domain size 2, basis size 1 suffices, opening the door to an extensive line of work on the structural theory of holographic algorithms over the Boolean domain. Cai and Fu later showed for signatures of full rank that over domain sizes 3 and 4, basis sizes 1 and 2, respectively, suffice, and they conjectured that over domain size $k$ there is a collapse to basis size $\\\\lfloor\\\\log_2 k\\\\rfloor$. In this work, we resolve this conjecture in the affirmative for signatures of full rank for all $k$.',\n",
       "  \"Title: partial conway and iteration semirings\\nAbstract: A Conway semiring is a semiring $S$ equipped with a unary operation $^*:S \\\\to S$, always called 'star', satisfying the sum star and product star identities. It is known that these identities imply a Kleene type theorem. Some computationally important semirings, such as $N$ or $N^{\\\\rat}\\\\llangle \\\\Sigma^* \\\\rrangle$ of rational power series of words on $\\\\Sigma$ with coefficients in $N$, cannot have a total star operation satisfying the Conway identities. We introduce here partial Conway semirings, which are semirings $S$ which have a star operation defined only on an ideal of $S$; when the arguments are appropriate, the operation satisfies the above identities. We develop the general theory of partial Conway semirings and prove a Kleene theorem for this generalization.\",\n",
       "  \"Title: succinctness of two way probabilistic and quantum finite automata\\nAbstract: We prove that two-way probabilistic and quantum finite automata (2PFA's and 2QFA's) can be considerably more concise than both their one-way versions (1PFA's and 1QFA's), and two-way nondeterministic finite automata (2NFA's). For this purpose, we demonstrate several infinite families of regular languages which can be recognized with some fixed probability greater than $ {1/2} $ by just tuning the transition amplitudes of a 2QFA (and, in one case, a 2PFA) with a constant number of states, whereas the sizes of the corresponding 1PFA's, 1QFA's and 2NFA's grow without bound. We also show that 2QFA's with mixed states can support highly efficient probability amplification. The weakest known model of computation where quantum computers recognize more languages with bounded error than their classical counterparts is introduced.\",\n",
       "  'Title: anaphora and discourse structure\\nAbstract: We argue in this paper that many common adverbial phrases generally taken to signal a discourse relation between syntactically connected units within discourse structure, instead work anaphorically to contribute relational meaning, with only indirect dependence on discourse structure. This allows a simpler discourse structure to provide scaffolding for compositional semantics, and reveals multiple ways in which the relational meaning conveyed by adverbial connectives can interact with that associated with discourse structure. We conclude by sketching out a lexicalised grammar for discourse that facilitates discourse interpretation as a product of compositional rules, anaphor resolution and inference.',\n",
       "  'Title: benchmarking declarative approximate selection predicates\\nAbstract: Declarative data quality has been an active research topic. The fundamental principle behind a declarative approach to data quality is the use of declarative statements to realize data quality primitives on top of any relational data source. A primary advantage of such an approach is the ease of use and integration with existing applications. Several similarity predicates have been proposed in the past for common quality primitives (approximate selections, joins, etc.) and have been fully expressed using declarative SQL statements. In this thesis, new similarity predicates are proposed along with their declarative realization, based on notions of probabilistic information retrieval. Then, full declarative specifications of previously proposed similarity predicates in the literature are presented, grouped into classes according to their primary characteristics. Finally, a thorough performance and accuracy study comparing a large number of similarity predicates for data cleaning operations is performed.',\n",
       "  \"Title: verifying a platform for digital imaging a multi tool strategy\\nAbstract: Fiji is a Java platform widely used by biologists and other experimental scientists to process digital images. In particular, in our research - made together with a biologists team; we use Fiji in some pre-processing steps before undertaking a homological digital processing of images. In a previous work, we have formalised the correctness of the programs which use homological techniques to analyse digital images. However, the verification of Fiji's pre-processing step was missed. In this paper, we present a multi-tool approach filling this gap, based on the combination of Why/Krakatoa, Coq and ACL2.\",\n",
       "  'Title: image question answer synergistic network for visual dialog\\nAbstract: The image, question (combined with the history for de-referencing), and the corresponding answer are three vital components of visual dialog. Classical visual dialog systems integrate the image, question, and history to search for or generate the best matched answer, and so, this approach significantly ignores the role of the answer. In this paper, we devise a novel image-question-answer synergistic network to value the role of the answer for precise visual dialog. We extend the traditional one-stage solution to a two-stage solution. In the first stage, candidate answers are coarsely scored according to their relevance to the image and question pair. Afterward, in the second stage, answers with high probability of being correct are re-ranked by synergizing with image and question. On the Visual Dialog v1.0 dataset, the proposed synergistic network boosts the discriminative visual dialog model to achieve a new state-of-the-art of 57.88\\\\% normalized discounted cumulative gain. A generative visual dialog model equipped with the proposed technique also shows promising improvements.',\n",
       "  \"Title: data driven identification of a thermal network in multi zone building\\nAbstract: System identification of smart buildings is necessary for their optimal control and application in demand response. The thermal response of a building around an operating point can be modeled using a network of interconnected resistors with capacitors at each node/zone called RC network. The development of the RC network involves two phases: obtaining the network topology, and estimating thermal resistances and capacitance's. In this article, we present a provable method to reconstruct the interaction topology of thermal zones of a building solely from temperature measurements. We demonstrate that our learning algorithm accurately reconstructs the interaction topology for a $5$ zone office building in EnergyPlus with real-world conditions. We show that our learning algorithm is able to recover the network structure in scenarios where prior research prove insufficient.\",\n",
       "  'Title: analytical evaluation of fractional frequency reuse for heterogeneous cellular networks\\nAbstract: Interference management techniques are critical to the performance of heterogeneous cellular networks, which will have dense and overlapping coverage areas, and experience high levels of interference. Fractional frequency reuse (FFR) is an attractive interference management technique due to its low complexity and overhead, and significant coverage improvement for low-percentile (cell-edge) users. Instead of relying on system simulations based on deterministic access point locations, this paper instead proposes an analytical model for evaluating Strict FFR and Soft Frequency Reuse (SFR) deployments based on the spatial Poisson point process. Our results both capture the non-uniformity of heterogeneous deployments and produce tractable expressions which can be used for system design with Strict FFR and SFR. We observe that the use of Strict FFR bands reserved for the users of each tier with the lowest average SINR provides the highest gains in terms of coverage and rate, while the use of SFR allows for more efficient use of shared spectrum between the tiers, while still mitigating much of the interference. Additionally, in the context of multi-tier networks with closed access in some tiers, the proposed framework shows the impact of cross-tier interference on closed access FFR, and informs the selection of key FFR parameters in open access.',\n",
       "  'Title: a scalable asynchronous distributed algorithm for topic modeling\\nAbstract: Learning meaningful topic models with massive document collections which contain millions of documents and billions of tokens is challenging because of two reasons: First, one needs to deal with a large number of topics (typically in the order of thousands). Second, one needs a scalable and efficient way of distributing the computation across multiple machines. In this paper we present a novel algorithm F+Nomad LDA which simultaneously tackles both these problems. In order to handle large number of topics we use an appropriately modified Fenwick tree. This data structure allows us to sample from a multinomial distribution over $T$ items in $O(\\\\log T)$ time. Moreover, when topic counts change the data structure can be updated in $O(\\\\log T)$ time. In order to distribute the computation across multiple processor we present a novel asynchronous framework inspired by the Nomad algorithm of \\\\cite{YunYuHsietal13}. We show that F+Nomad LDA significantly outperform state-of-the-art on massive problems which involve millions of documents, billions of words, and thousands of topics.',\n",
       "  'Title: phased lstm accelerating recurrent network training for long or event based sequences\\nAbstract: Recurrent Neural Networks (RNNs) have become the state-of-the-art choice for extracting patterns from temporal sequences. However, current RNN models are ill-suited to process irregularly sampled data triggered by events generated in continuous time by sensors or other neurons. Such data can occur, for example, when the input comes from novel event-driven artificial sensors that generate sparse, asynchronous streams of events or from multiple conventional sensors with different update intervals. In this work, we introduce the Phased LSTM model, which extends the LSTM unit by adding a new time gate. This gate is controlled by a parametrized oscillation with a frequency range that produces updates of the memory cell only during a small percentage of the cycle. Even with the sparse updates imposed by the oscillation, the Phased LSTM network achieves faster convergence than regular LSTMs on tasks which require learning of long sequences. The model naturally integrates inputs from sensors of arbitrary sampling rates, thereby opening new areas of investigation for processing asynchronous sensory events that carry timing information. It also greatly improves the performance of LSTMs in standard RNN applications, and does so with an order-of-magnitude fewer computes at runtime.',\n",
       "  'Title: unsupervised anomaly detection with generative adversarial networks to guide marker discovery\\nAbstract: Obtaining models that capture imaging markers relevant for disease progression and treatment monitoring is challenging. Models are typically based on large amounts of data with annotated examples of known markers aiming at automating detection. High annotation effort and the limitation to a vocabulary of known markers limit the power of such approaches. Here, we perform unsupervised learning to identify anomalies in imaging data as candidates for markers. We propose AnoGAN, a deep convolutional generative adversarial network to learn a manifold of normal anatomical variability, accompanying a novel anomaly scoring scheme based on the mapping from image space to a latent space. Applied to new data, the model labels anomalies, and scores image patches indicating their fit into the learned distribution. Results on optical coherence tomography images of the retina demonstrate that the approach correctly identifies anomalous images, such as images containing retinal fluid or hyperreflective foci.',\n",
       "  'Title: semantic autoencoder for zero shot learning\\nAbstract: Existing zero-shot learning (ZSL) models typically learn a projection function from a feature space to a semantic embedding space (e.g.~attribute space). However, such a projection function is only concerned with predicting the training seen class semantic representation (e.g.~attribute prediction) or classification. When applied to test data, which in the context of ZSL contains different (unseen) classes without training data, a ZSL model typically suffers from the project domain shift problem. In this work, we present a novel solution to ZSL based on learning a Semantic AutoEncoder (SAE). Taking the encoder-decoder paradigm, an encoder aims to project a visual feature vector into the semantic space as in the existing ZSL models. However, the decoder exerts an additional constraint, that is, the projection/code must be able to reconstruct the original visual feature. We show that with this additional reconstruction constraint, the learned projection function from the seen classes is able to generalise better to the new unseen classes. Importantly, the encoder and decoder are linear and symmetric which enable us to develop an extremely efficient learning algorithm. Extensive experiments on six benchmark datasets demonstrate that the proposed SAE outperforms significantly the existing ZSL models with the additional benefit of lower computational cost. Furthermore, when the SAE is applied to supervised clustering problem, it also beats the state-of-the-art.',\n",
       "  \"Title: unified analysis and optimization of d2d communications in cellular networks over fading channels\\nAbstract: This paper develops an innovative approach to the modeling and analysis of downlink cellular networks with device-to-device (D$2$D) transmissions. The analytical embodiment of the signal-to-noise and-interference ratio (SINR) analysis in general fading channels is unified due to the H-transform theory, a taxonomy never considered before in stochastic geometry-based cellular network modeling and analysis. The proposed framework has the potential, due to versatility of the Fox's H functions, of significantly simplifying the cumbersome analysis procedure and representation of D$2$D and cellular coverage, while subsuming those previously derived for all the known simple and composite fading models. By harnessing its tractability, the developed statistical machinery is employed to launch an investigation into the optimal design of coexisting D$2$D and cellular communications. We propose novel coverage-aware power control combined with opportunistic access control to maximize the area spectral efficiency (ASE) of D$2$D communications. Simulation results substantiate performance gains achieved by the proposed optimization framework in terms of cellular communication coverage probability, average D$2$D transmit power, and the ASE of D$2$D communications under different fading models and link- and network-level dynamics.\",\n",
       "  'Title: neos server 4 0 administrative guide\\nAbstract: The NEOS Server 4.0 provides a general Internet-based client/server as a link between users and software applications. The administrative guide covers the fundamental principals behind the operation of the NEOS Server, installation and trouble-shooting of the Server software, and implementation details of potential interest to a NEOS Server administrator. The guide also discusses making new software applications available through the Server, including areas of concern to remote solver administrators such as maintaining security, providing usage instructions, and enforcing reasonable restrictions on jobs. The administrative guide is intended both as an introduction to the NEOS Server and as a reference for use when running the Server.',\n",
       "  'Title: semantic stability in social tagging streams\\nAbstract: One potential disadvantage of social tagging systems is that due to the lack of a centralized vocabulary, a crowd of users may never manage to reach a consensus on the description of resources (e.g., books, users or songs) on the Web. Yet, previous research has provided interesting evidence that the tag distributions of resources may become semantically stable over time as more and more users tag them. At the same time, previous work has raised an array of new questions such as: (i) How can we assess the semantic stability of social tagging systems in a robust and methodical way? (ii) Does semantic stabilization of tags vary across different social tagging systems and ultimately, (iii) what are the factors that can explain semantic stabilization in such systems? In this work we tackle these questions by (i) presenting a novel and robust method which overcomes a number of limitations in existing methods, (ii) empirically investigating semantic stabilization processes in a wide range of social tagging systems with distinct domains and properties and (iii) detecting potential causes for semantic stabilization, specifically imitation behavior, shared background knowledge and intrinsic properties of natural language. Our results show that tagging streams which are generated by a combination of imitation dynamics and shared background knowledge exhibit faster and higher semantic stability than tagging streams which are generated via imitation dynamics or natural language streams alone.',\n",
       "  'Title: feasibility of in band full duplex radio transceivers with imperfect rf components analysis and enhanced cancellation algorithms\\nAbstract: In this paper we provide an overview regarding the feasibility of in-band full-duplex transceivers under imperfect RF components. We utilize results and findings from the recent research on full-duplex communications, while introducing also transmitter-induced thermal noise into the analysis. This means that the model of the RF impairments used in this paper is the most comprehensive thus far. By assuming realistic parameter values for the different transceiver components, it is shown that IQ imaging and transmitter-induced nonlinearities are the most significant sources of distortion in in-band full-duplex transceivers, in addition to linear self-interference. Motivated by this, we propose a novel augmented nonlinear digital self-interference canceller that is able to model and hence suppress all the essential transmitter imperfections jointly. This is also verified and demonstrated by extensive waveform simulations.',\n",
       "  'Title: bootstrapping structure using similarity\\nAbstract: In this paper a new similarity-based learning algorithm, inspired by string edit-distance (Wagner and Fischer, 1974), is applied to the problem of bootstrapping structure from scratch. The algorithm takes a corpus of unannotated sentences as input and returns a corpus of bracketed sentences. The method works on pairs of unstructured sentences or sentences partially bracketed by the algorithm that have one or more words in common. It finds parts of sentences that are interchangeable (i.e. the parts of the sentences that are different in both sentences). These parts are taken as possible constituents of the same type. While this corresponds to the basic bootstrapping step of the algorithm, further structure may be learned from comparison with other (similar) sentences. #R##N#We used this method for bootstrapping structure from the flat sentences of the Penn Treebank ATIS corpus, and compared the resulting structured sentences to the structured sentences in the ATIS corpus. Similarly, the algorithm was tested on the OVIS corpus. We obtained 86.04 % non-crossing brackets precision on the ATIS corpus and 89.39 % non-crossing brackets precision on the OVIS corpus.',\n",
       "  \"Title: learning where to attend like a human driver\\nAbstract: Despite the advent of autonomous cars, it's likely - at least in the near future - that human attention will still maintain a central role as a guarantee in terms of legal responsibility during the driving task. In this paper we study the dynamics of the driver's gaze and use it as a proxy to understand related attentional mechanisms. First, we build our analysis upon two questions: where and what the driver is looking at? Second, we model the driver's gaze by training a coarse-to-fine convolutional network on short sequences extracted from the DR(eye)VE dataset. Experimental comparison against different baselines reveal that the driver's gaze can indeed be learnt to some extent, despite i) being highly subjective and ii) having only one driver's gaze available for each sequence due to the irreproducibility of the scene. Eventually, we advocate for a new assisted driving paradigm which suggests to the driver, with no intervention, where she should focus her attention.\",\n",
       "  \"Title: the burden of risk aversion in mean risk selfish routing\\nAbstract: Considering congestion games with uncertain delays, we compute the inefficiency introduced in network routing by risk-averse agents. At equilibrium, agents may select paths that do not minimize the expected latency so as to obtain lower variability. A social planner, who is likely to be more risk neutral than agents because it operates at a longer time-scale, quantifies social cost with the total expected delay along routes. From that perspective, agents may make suboptimal decisions that degrade long-term quality. We define the {\\\\em price of risk aversion} (PRA) as the worst-case ratio of the social cost at a risk-averse Wardrop equilibrium to that where agents are risk-neutral. For networks with general delay functions and a single source-sink pair, we show that the PRA depends linearly on the agents' risk tolerance and on the degree of variability present in the network. In contrast to the {\\\\em price of anarchy}, in general the PRA increases when the network gets larger but it does not depend on the shape of the delay functions. To get this result we rely on a combinatorial proof that employs alternating paths that are reminiscent of those used in max-flow algorithms. For {\\\\em series-parallel} (SP) graphs, the PRA becomes independent of the network topology and its size. As a result of independent interest, we prove that for SP networks with deterministic delays, Wardrop equilibria {\\\\em maximize} the shortest-path objective among all feasible flows.\",\n",
       "  'Title: unified heat kernel regression for diffusion kernel smoothing and wavelets on manifolds and its application to mandible growth modeling in ct images\\nAbstract: We present a novel kernel regression framework for smoothing scalar surface data using the Laplace-Beltrami eigenfunctions. Starting with the heat kernel constructed from the eigenfunctions, we formulate a new bivariate kernel regression framework as a weighted eigenfunction expansion with the heat kernel as the weights. The new kernel regression is mathematically equivalent to isotropic heat diffusion, kernel smoothing and recently popular diffusion wavelets. Unlike many previous partial differential equation based approaches involving diffusion, our approach represents the solution of diffusion analytically, reducing numerical inaccuracy and slow convergence. The numerical implementation is validated on a unit sphere using spherical harmonics. As an illustration, we have applied the method in characterizing the localized growth pattern of mandible surfaces obtained in CT images from subjects between ages 0 and 20 years by regressing the length of displacement vectors with respect to the template surface.',\n",
       "  \"Title: deep feature based face detection on mobile devices\\nAbstract: We propose a deep feature-based face detector for mobile devices to detect user's face acquired by the front facing camera. The proposed method is able to detect faces in images containing extreme pose and illumination variations as well as partial faces. The main challenge in developing deep feature-based algorithms for mobile devices is the constrained nature of the mobile platform and the non-availability of CUDA enabled GPUs on such devices. Our implementation takes into account the special nature of the images captured by the front-facing camera of mobile devices and exploits the GPUs present in mobile devices without CUDA-based frameorks, to meet these challenges.\",\n",
       "  'Title: a sytematic piggybacking design for minimum storage regenerating codes\\nAbstract: Piggybacking is an efficient method to decrease the repair bandwidth of Maximum Distance Separable (MDS) codes or Minimum Storage Regenerating (MSR) codes. In this paper, for minimizing the repair bandwidth of parity nodes of the known MSR codes with high rate, which is usually the whole size of the original data, i.e., the maximal, a new systematic piggybacking design is proposed through an in-depth analysis of the design of piggybacking. As a result, new MSR codes are obtained with almost optimal repair bandwidth of parity nodes while retaining the optimal repair bandwidth of systematic nodes. Furthermore, MSR codes with balanced download during node repair process are presented based on the new piggybacking design.',\n",
       "  \"Title: local approximation schemes for topology control\\nAbstract: This paper presents a distributed algorithm on wireless ad-hoc networks that runs in polylogarithmic number of rounds in the size of the network and constructs a linear size, lightweight, (1+\\\\epsilon)-spanner for any given \\\\epsilon > 0. A wireless network is modeled by a d-dimensional \\\\alpha-quasi unit ball graph (\\\\alpha-UBG), which is a higher dimensional generalization of the standard unit disk graph (UDG) model. The d-dimensional \\\\alpha-UBG model goes beyond the unrealistic ``flat world'' assumption of UDGs and also takes into account transmission errors, fading signal strength, and physical obstructions. The main result in the paper is this: for any fixed \\\\epsilon > 0, 0 < \\\\alpha \\\\le 1, and d \\\\ge 2, there is a distributed algorithm running in O(\\\\log n \\\\log^* n) communication rounds on an n-node, d-dimensional \\\\alpha-UBG G that computes a (1+\\\\epsilon)-spanner G' of G with maximum degree \\\\Delta(G') = O(1) and total weight w(G') = O(w(MST(G)). This result is motivated by the topology control problem in wireless ad-hoc networks and improves on existing topology control algorithms along several dimensions. The technical contributions of the paper include a new, sequential, greedy algorithm with relaxed edge ordering and lazy updating, and clustering techniques for filtering out unnecessary edges.\",\n",
       "  'Title: on the intersection of additive perfect codes\\nAbstract: The intersection problem for additive (extended and non-extended) perfect codes, i.e. which are the possibilities for the number of codewords in the intersection of two additive codes C1 and C2 of the same length, is investigated. Lower and upper bounds for the intersection number are computed and, for any value between these bounds, codes which have this given intersection value are constructed. #R##N#For all these codes the abelian group structure of the intersection is characterized. The parameters of this abelian group structure corresponding to the intersection codes are computed and lower and upper bounds for these parameters are established. Finally, constructions of codes the intersection of which fits any parameters between these bounds are given.',\n",
       "  'Title: throughput delay analysis of random linear network coding for wireless broadcasting\\nAbstract: In an unreliable single-hop broadcast network setting, we investigate the throughput and decoding-delay performance of random linear network coding as a function of the coding window size and the network size. Our model consists of a source transmitting packets of a single flow to a set of $n$ users over independent erasure channels. The source performs random linear network coding (RLNC) over $k$ (coding window size) packets and broadcasts them to the users. We note that the broadcast throughput of RLNC must vanish with increasing $n$, for any fixed $k.$ Hence, in contrast to other works in the literature, we investigate how the coding window size $k$ must scale for increasing $n$. Our analysis reveals that the coding window size of $\\\\Theta(\\\\ln(n))$ represents a phase transition rate, below which the throughput converges to zero, and above which it converges to the broadcast capacity. Further, we characterize the asymptotic distribution of decoding delay and provide approximate expressions for the mean and variance of decoding delay for the scaling regime of $k=\\\\omega(\\\\ln(n)).$ These asymptotic expressions reveal the impact of channel correlations on the throughput and delay performance of RLNC. We also show how our analysis can be extended to other rateless block coding schemes such as the LT codes. Finally, we comment on the extension of our results to the cases of dependent channels across users and asymmetric channel model.',\n",
       "  'Title: convex color image segmentation with optimal transport distances\\nAbstract: This work is about the use of regularized optimal-transport distances for convex, histogram-based image segmentation. In the considered framework, fixed exemplar histograms define a prior on the statistical features of the two regions in competition. In this paper, we investigate the use of various transport-based cost functions as discrepancy measures and rely on a primal-dual algorithm to solve the obtained convex optimization problem.',\n",
       "  'Title: provable bounds for learning some deep representations\\nAbstract: We give algorithms with provable guarantees that learn a class of deep nets in the generative model view popularized by Hinton and others. Our generative model is an $n$ node multilayer neural net that has degree at most $n^{\\\\gamma}$ for some $\\\\gamma <1$ and each edge has a random edge weight in $[-1,1]$. Our algorithm learns {\\\\em almost all} networks in this class with polynomial running time. The sample complexity is quadratic or cubic depending upon the details of the model. #R##N#The algorithm uses layerwise learning. It is based upon a novel idea of observing correlations among features and using these to infer the underlying edge structure via a global graph recovery procedure. The analysis of the algorithm reveals interesting structure of neural networks with random edge weights.',\n",
       "  \"Title: planarity of streamed graphs\\nAbstract: In this paper we introduce a notion of planarity for graphs that are presented in a streaming fashion. A $\\\\textit{streamed graph}$ is a stream of edges $e_1,e_2,...,e_m$ on a vertex set $V$. A streamed graph is $\\\\omega$-$\\\\textit{stream planar}$ with respect to a positive integer window size $\\\\omega$ if there exists a sequence of planar topological drawings $\\\\Gamma_i$ of the graphs $G_i=(V,\\\\{e_j \\\\mid i\\\\leq j < i+\\\\omega\\\\})$ such that the common graph $G^{i}_\\\\cap=G_i\\\\cap G_{i+1}$ is drawn the same in $\\\\Gamma_i$ and in $\\\\Gamma_{i+1}$, for $1\\\\leq i < m-\\\\omega$. The $\\\\textit{Stream Planarity}$ Problem with window size $\\\\omega$ asks whether a given streamed graph is $\\\\omega$-stream planar. We also consider a generalization, where there is an additional $\\\\textit{backbone graph}$ whose edges have to be present during each time step. These problems are related to several well-studied planarity problems. #R##N#We show that the $\\\\textit{Stream Planarity}$ Problem is NP-complete even when the window size is a constant and that the variant with a backbone graph is NP-complete for all $\\\\omega \\\\ge 2$. On the positive side, we provide $O(n+\\\\omega{}m)$-time algorithms for (i) the case $\\\\omega = 1$ and (ii) all values of $\\\\omega$ provided the backbone graph consists of one $2$-connected component plus isolated vertices and no stream edge connects two isolated vertices. Our results improve on the Hanani-Tutte-style $O((nm)^3)$-time algorithm proposed by Schaefer [GD'14] for $\\\\omega=1$.\",\n",
       "  'Title: kinetic voronoi diagrams and delaunay triangulations under polygonal distance functions\\nAbstract: Let $P$ be a set of $n$ points and $Q$ a convex $k$-gon in ${\\\\mathbb R}^2$. We analyze in detail the topological (or discrete) changes in the structure of the Voronoi diagram and the Delaunay triangulation of $P$, under the convex distance function defined by $Q$, as the points of $P$ move along prespecified continuous trajectories. Assuming that each point of $P$ moves along an algebraic trajectory of bounded degree, we establish an upper bound of $O(k^4n\\\\lambda_r(n))$ on the number of topological changes experienced by the diagrams throughout the motion; here $\\\\lambda_r(n)$ is the maximum length of an $(n,r)$-Davenport-Schinzel sequence, and $r$ is a constant depending on the algebraic degree of the motion of the points. Finally, we describe an algorithm for efficiently maintaining the above structures, using the kinetic data structure (KDS) framework.',\n",
       "  'Title: on the accuracy of the wyner model in cellular networks\\nAbstract: The Wyner model has been widely used to model and analyze cellular networks due to its simplicity and analytical tractability. Its key aspects include fixed user locations and the deterministic and homogeneous interference intensity. While clearly a significant simplification of a real cellular system, which has random user locations and interference levels that vary by several orders of magnitude over a cell, a common presumption by theorists is that the Wyner model nevertheless captures the essential aspects of cellular interactions. But is this true? To answer this question, we consider both uplink and downlink transmissions, and both outage-based and average-based metrics. For the uplink, for both metrics, we conclude that the Wyner model is in fact quite accurate for systems with a sufficient number of simultaneous users, e.g. CDMA. Conversely, it is broadly inaccurate otherwise. With multicell processing, intracell TDMA is shown to be suboptimal in terms of average throughput, in sharp contrast to predictions using the Wyner model. Turning to the downlink, the Wyner model is highly inaccurate for outage since it depends largely on the user locations. However, for average or sum throughput, the Wyner model serves as an acceptable simplification in certain special cases if the interference parameter is set appropriately.',\n",
       "  'Title: bayesian uncertainty matching for unsupervised domain adaptation\\nAbstract: Domain adaptation is an important technique to alleviate performance degradation caused by domain shift, e.g., when training and test data come from different domains. Most existing deep adaptation methods focus on reducing domain shift by matching marginal feature distributions through deep transformations on the input features, due to the unavailability of target domain labels. We show that domain shift may still exist via label distribution shift at the classifier, thus deteriorating model performances. To alleviate this issue, we propose an approximate joint distribution matching scheme by exploiting prediction uncertainty. Specifically, we use a Bayesian neural network to quantify prediction uncertainty of a classifier. By imposing distribution matching on both features and labels (via uncertainty), label distribution mismatching in source and target data is effectively alleviated, encouraging the classifier to produce consistent predictions across domains. We also propose a few techniques to improve our method by adaptively reweighting domain adaptation loss to achieve nontrivial distribution matching and stable training. Comparisons with state of the art unsupervised domain adaptation methods on three popular benchmark datasets demonstrate the superiority of our approach, especially on the effectiveness of alleviating negative transfer.',\n",
       "  \"Title: timeline a dynamic hierarchical dirichlet process model for recovering birth death and evolution of topics in text stream\\nAbstract: Topic models have proven to be a useful tool for discovering latent structures in document collections. However, most document collections often come as temporal streams and thus several aspects of the latent structure such as the number of topics, the topics' distribution and popularity are time-evolving. Several models exist that model the evolution of some but not all of the above aspects. In this paper we introduce infinite dynamic topic models, iDTM, that can accommodate the evolution of all the aforementioned aspects. Our model assumes that documents are organized into epochs, where the documents within each epoch are exchangeable but the order between the documents is maintained across epochs. iDTM allows for unbounded number of topics: topics can die or be born at any epoch, and the representation of each topic can evolve according to a Markovian dynamics. We use iDTM to analyze the birth and evolution of topics in the NIPS community and evaluated the efficacy of our model on both simulated and real datasets with favorable outcome.\",\n",
       "  'Title: three generalizations of the focus constraint\\nAbstract: The FOCUS constraint expresses the notion that solutions are concentrated. In practice, this constraint suffers from the rigidity of its semantics. To tackle this issue, we propose three generalizations of the FOCUS constraint. We provide for each one a complete filtering algorithm as well as discussing decompositions.',\n",
       "  'Title: parallel processing of large graphs\\nAbstract: More and more large data collections are gathered worldwide in various IT systems. Many of them possess the networked nature and need to be processed and analysed as graph structures. Due to their size they require very often usage of parallel paradigm for efficient computation. Three parallel techniques have been compared in the paper: MapReduce, its map-side join extension and Bulk Synchronous Parallel (BSP). They are implemented for two different graph problems: calculation of single source shortest paths (SSSP) and collective classification of graph nodes by means of relational influence propagation (RIP). The methods and algorithms are applied to several network datasets differing in size and structural profile, originating from three domains: telecommunication, multimedia and microblog. The results revealed that iterative graph processing with the BSP implementation always and significantly, even up to 10 times outperforms MapReduce, especially for algorithms with many iterations and sparse communication. Also MapReduce extension based on map-side join usually noticeably presents better efficiency, although not as much as BSP. Nevertheless, MapReduce still remains the good alternative for enormous networks, whose data structures do not fit in local memories.',\n",
       "  'Title: fast deep matting for portrait animation on mobile phone\\nAbstract: Image matting plays an important role in image and video editing. However, the formulation of image matting is inherently ill-posed. Traditional methods usually employ interaction to deal with the image matting problem with trimaps and strokes, and cannot run on the mobile phone in real-time. In this paper, we propose a real-time automatic deep matting approach for mobile devices. By leveraging the densely connected blocks and the dilated convolution, a light full convolutional network is designed to predict a coarse binary mask for portrait images. And a feathering block, which is edge-preserving and matting adaptive, is further developed to learn the guided filter and transform the binary mask into alpha matte. Finally, an automatic portrait animation system based on fast deep matting is built on mobile devices, which does not need any interaction and can realize real-time matting with 15 fps. The experiments show that the proposed approach achieves comparable results with the state-of-the-art matting solvers.',\n",
       "  'Title: imitation learning with a value based prior\\nAbstract: The goal of imitation learning is for an apprentice to learn how to behave in a stochastic environment by observing a mentor demonstrating the correct behavior. Accurate prior knowledge about the correct behavior can reduce the need for demonstrations from the mentor. We present a novel approach to encoding prior knowledge about the correct behavior, where we assume that this prior knowledge takes the form of a Markov Decision Process (MDP) that is used by the apprentice as a rough and imperfect model of the mentor\\'s behavior. Specifically, taking a Bayesian approach, we treat the value of a policy in this modeling MDP as the log prior probability of the policy. In other words, we assume a priori that the mentor\\'s behavior is likely to be a high value policy in the modeling MDP, though quite possibly different from the optimal policy. We describe an efficient algorithm that, given a modeling MDP and a set of demonstrations by a mentor, provably converges to a stationary point of the log posterior of the mentor\\'s policy, where the posterior is computed with respect to the \"value based\" prior. We also present empirical evidence that this prior does in fact speed learning of the mentor\\'s policy, and is an improvement in our experiments over similar previous methods.',\n",
       "  'Title: coalition formation game for cooperative cognitive radio using gibbs sampling\\nAbstract: This paper considers a cognitive radio network in which each secondary user selects a primary user to assist in order to get a chance of accessing the primary user channel. Thus, each group of secondary users assisting the same primary user forms a coaltion. Within each coalition, sequential relaying is employed, and a relay ordering algorithm is used to make use of the relays in an efficient manner. It is required then to find the optimal sets of secondary users assisting each primary user such that the sum of their rates is maximized. The problem is formulated as a coalition formation game, and a Gibbs Sampling based algorithm is used to find the optimal coalition structure.',\n",
       "  'Title: reconstruction of integers from pairwise distances\\nAbstract: Given a set of integers, one can easily construct the set of their pairwise distances. We consider the inverse problem: given a set of pairwise distances, find the integer set which realizes the pairwise distance set. This problem arises in a lot of fields in engineering and applied physics, and has confounded researchers for over 60 years. It is one of the few fundamental problems that are neither known to be NP-hard nor solvable by polynomial-time algorithms. Whether unique recovery is possible also remains an open question. #R##N#In many practical applications where this problem occurs, the integer set is naturally sparse (i.e., the integers are sufficiently spaced), a property which has not been explored. In this work, we exploit the sparse nature of the integer set and develop a polynomial-time algorithm which provably recovers the set of integers (up to linear shift and reversal) from the set of their pairwise distances with arbitrarily high probability if the sparsity is $O(n^{1/2-\\\\eps})$. Numerical simulations verify the effectiveness of the proposed algorithm.',\n",
       "  'Title: jointly modeling embedding and translation to bridge video and language\\nAbstract: Automatically describing video content with natural language is a fundamental challenge of multimedia. Recurrent Neural Networks (RNN), which models sequence dynamics, has attracted increasing attention on visual interpretation. However, most existing approaches generate a word locally with given previous words and the visual content, while the relationship between sentence semantics and visual content is not holistically exploited. As a result, the generated sentences may be contextually correct but the semantics (e.g., subjects, verbs or objects) are not true. #R##N#This paper presents a novel unified framework, named Long Short-Term Memory with visual-semantic Embedding (LSTM-E), which can simultaneously explore the learning of LSTM and visual-semantic embedding. The former aims to locally maximize the probability of generating the next word given previous words and visual content, while the latter is to create a visual-semantic embedding space for enforcing the relationship between the semantics of the entire sentence and visual content. Our proposed LSTM-E consists of three components: a 2-D and/or 3-D deep convolutional neural networks for learning powerful video representation, a deep RNN for generating sentences, and a joint embedding model for exploring the relationships between visual content and sentence semantics. The experiments on YouTube2Text dataset show that our proposed LSTM-E achieves to-date the best reported performance in generating natural sentences: 45.3% and 31.0% in terms of BLEU@4 and METEOR, respectively. We also demonstrate that LSTM-E is superior in predicting Subject-Verb-Object (SVO) triplets to several state-of-the-art techniques.',\n",
       "  'Title: approximation algorithms for generalized mst and tsp in grid clusters\\nAbstract: We consider a special case of the generalized minimum spanning tree problem (GMST) and the generalized travelling salesman problem (GTSP) where we are given a set of points inside the integer grid (in Euclidean plane) where each grid cell is $1 \\\\times 1$. In the MST version of the problem, the goal is to find a minimum tree that contains exactly one point from each non-empty grid cell (cluster). Similarly, in the TSP version of the problem, the goal is to find a minimum weight cycle containing one point from each non-empty grid cell. We give a $(1+4\\\\sqrt{2}+\\\\epsilon)$ and $(1.5+8\\\\sqrt{2}+\\\\epsilon)$-approximation algorithm for these two problems in the described setting, respectively. #R##N#Our motivation is based on the problem posed in [7] for a constant approximation algorithm. The authors designed a PTAS for the more special case of the GMST where non-empty cells are connected end dense enough. However, their algorithm heavily relies on this connectivity restriction and is unpractical. Our results develop the topic further.',\n",
       "  'Title: service oriented communities visions and contributions towards social organizations\\nAbstract: With the increase of the populations, resources are becoming scarcer, and a smarter way to make use of them becomes a vital necessity of our societies. On the other hand, resource management is traditionally carried out through well established organizations, policies, and regulations that are often considered as impossible to restructure. Our position is that merely expanding the traditional approaches might not be enough. Systems must be radically rethought in order to achieve a truly effective and rational use of the available resources. Classical concepts such as demand and supply need to be rethought as well, as they operate artificial classifications that limit the true potential of systems and organizations. In what follows we propose our vision to future, \"smarter\" systems able to overcome the limitations of the status quo. Such systems require what Boulding called \"gestalts,\" namely concepts able to \"directing research towards the gaps which they reveal\". In this paper we elaborate on this and show how such gestalts can pave the way towards novel reformulations of traditional services able to reach a better and more sensible management of the available resources and cope with their scarcity. Our vision of a Service-oriented Community is also introduced. We believe that such communities---in heterarchical coexistence with traditional systems---provide the necessary diversity and innovation orientation to prevent societal lock-ins such as the ones we are experiencing in assisting our elderly ones.',\n",
       "  'Title: capacity with causal and non causal side information a unified view\\nAbstract: We identify the common underlying form of the capacity expression that is applicable to both cases where causal or non-causal side information is made available to the transmitter. Using this common form we find that for the single user channel, the multiple access channel, the degraded broadcast channel, and the degraded relay channel, the sum capacity with causal and non-causal side information are identical when all the transmitter side information is also made available to all the receivers. A genie-aided outerbound is developed that states that when a genie provides $n$ bits of side information to a receiver the resulting capacity improvement can not be more than $n$ bits. Combining these two results we are able to bound the relative capacity advantage of non-causal side information over causal side information for both single user as well as various multiple user communication scenarios. Applications of these capacity bounds are demonstrated through examples of random access channels. Interestingly, the capacity results indicate that the excessive MAC layer overheads common in present wireless systems may be avoided through coding across multiple access blocks. It is also shown that even one bit of side information at the transmitter can result in unbounded capacity improvement. As a side, we obtain the sum capacity for a multiple access channel when the side information available to the transmitter is causal and possibly correlated to the side information available to the receiver.',\n",
       "  'Title: node overlap removal by growing a tree\\nAbstract: Node overlap removal is a necessary step in many scenarios including laying out a graph, or visualizing a tag cloud. Our contribution is a new overlap removal algorithm that iteratively builds a Minimum Spanning Tree on a Delaunay triangulation of the node centers and removes the node overlaps by \"growing\" the tree. The algorithm is simple to implement yet produces high quality layouts. According to our experiments it runs several times faster than the current state-of-the-art methods.',\n",
       "  'Title: using short synchronous wom codes to make wom codes decodable\\nAbstract: In the framework of write-once memory (WOM) codes, it is important to distinguish between codes that can be decoded directly and those that require that the decoder knows the current generation to successfully decode the state of the memory. A widely used approach to construct WOM codes is to design first nondecodable codes that approach the boundaries of the capacity region, and then make them decodable by appending additional cells that store the current generation, at an expense of a rate loss. In this paper, we propose an alternative method to make nondecodable WOM codes decodable by appending cells that also store some additional data. The key idea is to append to the original (nondecodable) code a short synchronous WOM code and write generations of the original code and of the synchronous code simultaneously. We consider both the binary and the nonbinary case. Furthermore, we propose a construction of synchronous WOM codes, which are then used to make nondecodable codes decodable. For short-to-moderate block lengths, the proposed method significantly reduces the rate loss as compared to the standard method.',\n",
       "  'Title: models of visually grounded speech signal pay attention to nouns a bilingual experiment on english and japanese\\nAbstract: We investigate the behaviour of attention in neural models of visually grounded speech trained on two languages: English and Japanese. Experimental results show that attention focuses on nouns and this behaviour holds true for two very typologically different languages. We also draw parallels between artificial neural attention and human attention and show that neural attention focuses on word endings as it has been theorised for human attention. Finally, we investigate how two visually grounded monolingual models can be used to perform cross-lingual speech-to-speech retrieval. For both languages, the enriched bilingual (speech-image) corpora with part-of-speech tags and forced alignments are distributed to the community for reproducible research.',\n",
       "  'Title: alchemy a quantum chemistry dataset for benchmarking ai models\\nAbstract: We introduce a new molecular dataset, named Alchemy, for developing machine learning models useful in chemistry and material science. As of June 20th 2019, the dataset comprises of 12 quantum mechanical properties of 119,487 organic molecules with up to 14 heavy atoms, sampled from the GDB MedChem database. The Alchemy dataset expands the volume and diversity of existing molecular datasets. Our extensive benchmarks of the state-of-the-art graph neural network models on Alchemy clearly manifest the usefulness of new data in validating and developing machine learning models for chemistry and material science. We further launch a contest to attract attentions from researchers in the related fields. More details can be found on the contest website \\\\footnote{this https URL}. At the time of benchamrking experiment, we have generated 119,487 molecules in our Alchemy dataset. More molecular samples are generated since then. Hence, we provide a list of molecules used in the reported benchmarks.',\n",
       "  \"Title: learning sparse neural networks via ell_0 and t ell_1 by a relaxed variable splitting method with application to multi scale curve classification\\nAbstract: We study sparsification of convolutional neural networks (CNN) by a relaxed variable splitting method of $\\\\ell_0$ and transformed-$\\\\ell_1$ (T$\\\\ell_1$) penalties, with application to complex curves such as texts written in different fonts, and words written with trembling hands simulating those of Parkinson's disease patients. The CNN contains 3 convolutional layers, each followed by a maximum pooling, and finally a fully connected layer which contains the largest number of network weights. With $\\\\ell_0$ penalty, we achieved over 99 \\\\% test accuracy in distinguishing shaky vs. regular fonts or hand writings with above 86 \\\\% of the weights in the fully connected layer being zero. Comparable sparsity and test accuracy are also reached with a proper choice of T$\\\\ell_1$ penalty.\",\n",
       "  \"Title: development of a sensory neural network for medical diagnosing\\nAbstract: Performance of a sensory-neural network developed for diagnosing of diseases is described. Information about patient's condition is provided by answers to the questionnaire. Questions correspond to sensors generating signals when patients acknowledge symptoms. These signals excite neurons in which characteristics of the diseases are represented by synaptic weights associated with indicators of symptoms. The disease corresponding to the most excited neuron is proposed as the result of diagnosing. Its reliability is estimated by the likelihood defined by the ratio of excitation of the most excited neuron and the complete neural network.\",\n",
       "  'Title: functions definable by numerical set expressions\\nAbstract: A \"numerical set-expression\" is a term specifying a cascade of arithmetic and logical operations to be performed on sets of non-negative integers. If these operations are confined to the usual Boolean operations together with the result of lifting addition to the level of sets, we speak of \"additive circuits\". If they are confined to the usual Boolean operations together with the result of lifting addition and multiplication to the level of sets, we speak of \"arithmetic circuits\". In this paper, we investigate the definability of sets and functions by means of additive and arithmetic circuits, occasionally augmented with additional operations.',\n",
       "  'Title: free deconvolution for signal processing applications\\nAbstract: Situations in many fields of research, such as digital communications, nuclear physics and mathematical finance, can be modelled with random matrices. When the matrices get large, free probability theory is an invaluable tool for describing the asymptotic behaviour of many systems. It will be shown how free probability can be used to aid in source detection for certain systems. Sample covariance matrices for systems with noise are the starting point in our source detection problem. Multiplicative free deconvolution is shown to be a method which can aid in expressing limit eigenvalue distributions for sample covariance matrices, and to simplify estimators for eigenvalue distributions of covariance matrices.',\n",
       "  'Title: secure network in business to business application by using access control list acl and service level agreement sla\\nAbstract: The motivation behind this paper is to dissecting the secure network for Business to Business (B2B) application by Implementing Access Control List (ACL) and Service Level Agreement (SLA). This data provides the nature of attacks reported as external or internal attacks. This paper presents the initial finding of attacks, types of attacks and their ratio within specific time. It demonstrates the advance technique and methodology to reduce the attacks and vulnerabilities and minimize the ratio of attacks to the networks and application and keep the network secure and runs application smoothly regarding that. It also identifies the location of attacks, the reason behind the attack and the technique used in attacking. The whole field of system security is limitless and in an evolutionary stage. To comprehend the exploration being performed today, foundation learning of the web and assaults, the security is vital and in this way they are investigated. It provides the statistical analytics about various attacks and nature of attacks for acquiring the results through simulation to prove the hypothesis.',\n",
       "  'Title: online and stochastic gradient methods for non decomposable loss functions\\nAbstract: Modern applications in sensitive domains such as biometrics and medicine frequently require the use of non-decomposable loss functions such as precision@k, F-measure etc. Compared to point loss functions such as hinge-loss, these offer much more fine grained control over prediction, but at the same time present novel challenges in terms of algorithm design and analysis. In this work we initiate a study of online learning techniques for such non-decomposable loss functions with an aim to enable incremental learning as well as design scalable solvers for batch problems. To this end, we propose an online learning framework for such loss functions. Our model enjoys several nice properties, chief amongst them being the existence of efficient online learning algorithms with sublinear regret and online to batch conversion bounds. Our model is a provable extension of existing online learning models for point loss functions. We instantiate two popular losses, prec@k and pAUC, in our model and prove sublinear regret bounds for both of them. Our proofs require a novel structural lemma over ranked lists which may be of independent interest. We then develop scalable stochastic gradient descent solvers for non-decomposable loss functions. We show that for a large family of loss functions satisfying a certain uniform convergence property (that includes prec@k, pAUC, and F-measure), our methods provably converge to the empirical risk minimizer. Such uniform convergence results were not known for these losses and we establish these using novel proof techniques. We then use extensive experimentation on real life and benchmark datasets to establish that our method can be orders of magnitude faster than a recently proposed cutting plane method.',\n",
       "  'Title: bicameral structuring and synthetic imagery for jointly predicting instance boundaries and nearby occlusions from a single image\\nAbstract: Oriented boundary detection is a challenging task aimed at both delineating category-agnostic object instances and inferring their spatial layout from a single RGB image. State-of-the-art deep convolutional networks for this task rely on two independent streams that predict boundaries and occlusions respectively, although both require similar local and global cues, and occlusions cause boundaries. We therefore propose a fully convolutional bicameral structuring, composed of two cascaded decoders sharing one deep encoder, linked altogether by skip connections to combine local and global features, for jointly predicting instance boundaries and their unoccluded side. Furthermore, state-of-the-art datasets contain real images with few instances and occlusions mostly due to objects occluding the background, thereby missing meaningful occlusions between instances. For evaluating the missing scenario of dense piles of objects as well, we introduce synthetic data (Mikado), which extensibly contains more instances and inter-instance occlusions per image than the PASCAL Instance Occlusion Dataset (PIOD), the COCO Amodal dataset (COCOA), and the Densely Segmented Supermarket Amodal dataset (D2SA). We show that the proposed network design outperforms the two-stream baseline and alternative archiectures for oriented boundary detection on both PIOD and Mikado, and the amodal segmentation approach on COCOA as well. Our experiments on D2SA also show that Mikado is plausible in the sense that it enables the learning of performance-enhancing representations transferable to real data, while drastically reducing the need of hand-made annotations for finetuning.',\n",
       "  \"Title: successive integer forcing and its sum rate optimality\\nAbstract: Integer-forcing receivers generalize traditional linear receivers for the multiple-input multiple-output channel by decoding integer-linear combinations of the transmitted streams, rather then the streams themselves. Previous works have shown that the additional degree of freedom in choosing the integer coefficients enables this receiver to approach the performance of maximum-likelihood decoding in various scenarios. Nonetheless, even for the optimal choice of integer coefficients, the additive noise at the equalizer's output is still correlated. In this work we study a variant of integer-forcing, termed successive integer-forcing, that exploits these noise correlations to improve performance. This scheme is the integer-forcing counterpart of successive interference cancellation for traditional linear receivers. Similarly to the latter, we show that successive integer-forcing is capacity achieving when it is possible to optimize the rate allocation to the different streams. In comparison to standard successive interference cancellation receivers, the successive integer-forcing receiver offers more possibilities for capacity achieving rate tuples, and in particular, ones that are more balanced.\",\n",
       "  'Title: broadcast channels with delayed finite rate feedback predict or observe\\nAbstract: Most multiuser precoding techniques require accurate transmitter channel state information (CSIT) to maintain orthogonality between the users. Such techniques have proven quite fragile in time-varying channels because the CSIT is inherently imperfect due to estimation and feedback delay, as well quantization noise. An alternative approach recently proposed by Maddah-Ali and Tse (MAT) allows for significant multiplexing gain in the multi-input single-output (MISO) broadcast channel (BC) even with transmit CSIT that is completely stale, i.e. uncorrelated with the current channel state. With $K$ users, their scheme claims to lose only a $\\\\log(K)$ factor relative to the full $K$ degrees of freedom (DoF) attainable in the MISO BC with perfect CSIT for large $K$. However, their result does not consider the cost of the feedback, which is potentially very large in high mobility (short channel coherence time). In this paper, we more closely examine the MAT scheme and compare its DoF gain to single user transmission (which always achieves 1 DoF) and partial CSIT linear precoding (which achieves up to $K$). In particular, assuming the channel coherence time is $N$ symbol periods and the feedback delay is $N_{\\\\rm fd}$ we show that when $N   (1+o(1)) (N_{\\\\rm fd}+ K / \\\\log K)(1-\\\\log^{-1}K)^{-1}$ (long coherence time), zero-forcing precoding outperforms the other two. The MAT scheme is optimal for intermediate coherence times, which for practical parameter choices is indeed quite a large and significant range, even accounting for the feedback cost.',\n",
       "  \"Title: completeness of randomized kinodynamic planners with state based steering\\nAbstract: Probabilistic completeness is an important property in motion planning. Although it has been established with clear assumptions for geometric planners, the panorama of completeness results for kinodynamic planners is still incomplete, as most existing proofs rely on strong assumptions that are difficult, if not impossible, to verify on practical systems. In this paper, we focus on an important class of kinodynamic planners, namely those that interpolate trajectories in the state space. We provide a proof of probabilistic completeness for these planners under assumptions that can be readily verified from the system's equations of motion and the user-defined interpolation function. Our proof relies crucially on a property of interpolated trajectories, termed second-order continuity (SOC), which we show is tightly related to the ability of a planner to benefit from denser sampling. We analyze the impact of this property in simulations on a low-torque pendulum. Our results show that a simple RRT using a second-order continuous interpolation swiftly finds solution, while it is impossible for the same planner using standard Bezier curves (which are not SOC) to find any solution.\",\n",
       "  \"Title: activis visual exploration of industry scale deep neural network models\\nAbstract: While deep learning models have achieved state-of-the-art accuracies for many prediction tasks, understanding these models remains a challenge. Despite the recent interest in developing visual tools to help users interpret deep learning models, the complexity and wide variety of models deployed in industry, and the large-scale datasets that they used, pose unique design challenges that are inadequately addressed by existing work. Through participatory design sessions with over 15 researchers and engineers at Facebook, we have developed, deployed, and iteratively improved ActiVis, an interactive visualization system for interpreting large-scale deep learning models and results. By tightly integrating multiple coordinated views, such as a computation graph overview of the model architecture, and a neuron activation view for pattern discovery and comparison, users can explore complex deep neural network models at both the instance- and subset-level. ActiVis has been deployed on Facebook's machine learning platform. We present case studies with Facebook researchers and engineers, and usage scenarios of how ActiVis may work with different models.\",\n",
       "  'Title: learning to segment and represent motion primitives from driving data for motion planning applications\\nAbstract: Developing an intelligent vehicle which can perform human-like actions requires the ability to learn basic driving skills from a large amount of naturalistic driving data. The algorithms will become efficient if we could decompose the complex driving tasks into motion primitives which represent the elementary compositions of driving skills. Therefore, the purpose of this paper is to segment unlabeled trajectory data into a library of motion primitives. By applying a probabilistic inference based on an iterative Expectation-Maximization algorithm, our method segments the collected trajectories while learning a set of motion primitives represented by the dynamic movement primitives. The proposed method utilizes the mutual dependencies between the segmentation and representation of motion primitives and the driving-specific based initial segmentation. By utilizing this mutual dependency and the initial condition, this paper presents how we can enhance the performance of both the segmentation and the motion primitive library establishment. We also evaluate the applicability of the primitive representation method to imitation learning and motion planning algorithms. The model is trained and validated by using the driving data collected from the Beijing Institute of Technology intelligent vehicle platform. The results show that the proposed approach can find the proper segmentation and establish the motion primitive library simultaneously.',\n",
       "  'Title: a deterministic equivalent for the analysis of non gaussian correlated mimo multiple access channels\\nAbstract: Large dimensional random matrix theory (RMT) has provided an efficient analytical tool to understand multiple-input multiple-output (MIMO) channels and to aid the design of MIMO wireless communication systems. However, previous studies based on large dimensional RMT rely on the assumption that the transmit correlation matrix is diagonal or the propagation channel matrix is Gaussian. There is an increasing interest in the channels where the transmit correlation matrices are generally nonnegative definite and the channel entries are non-Gaussian. This class of channel models appears in several applications in MIMO multiple access systems, such as small cell networks (SCNs). To address these problems, we use the generalized Lindeberg principle to show that the Stieltjes transforms of this class of random matrices with Gaussian or non-Gaussian independent entries coincide in the large dimensional regime. This result permits to derive the deterministic equivalents (e.g., the Stieltjes transform and the ergodic mutual information) for non-Gaussian MIMO channels from the known results developed for Gaussian MIMO channels, and is of great importance in characterizing the spectral efficiency of SCNs.',\n",
       "  'Title: balanced allocations and double hashing\\nAbstract: Double hashing has recently found more common usage in schemes that use multiple hash functions. In double hashing, for an item $x$, one generates two hash values $f(x)$ and $g(x)$, and then uses combinations $(f(x) +k g(x)) \\\\bmod n$ for $k=0,1,2,...$ to generate multiple hash values from the initial two. We first perform an empirical study showing that, surprisingly, the performance difference between double hashing and fully random hashing appears negligible in the standard balanced allocation paradigm, where each item is placed in the least loaded of $d$ choices, as well as several related variants. We then provide theoretical results that explain the behavior of double hashing in this context.',\n",
       "  \"Title: good friends bad news affect and virality in twitter\\nAbstract: The link between affect, defined as the capacity for sentimental arousal on the part of a message, and virality, defined as the probability that it be sent along, is of significant theoretical and practical importance, e.g. for viral marketing. A quantitative study of emailing of articles from the NY Times finds a strong link between positive affect and virality, and, based on psychological theories it is concluded that this relation is universally valid. The conclusion appears to be in contrast with classic theory of diffusion in news media emphasizing negative affect as promoting propagation. In this paper we explore the apparent paradox in a quantitative analysis of information diffusion on Twitter. Twitter is interesting in this context as it has been shown to present both the characteristics social and news media. The basic measure of virality in Twitter is the probability of retweet. Twitter is different from email in that retweeting does not depend on pre-existing social relations, but often occur among strangers, thus in this respect Twitter may be more similar to traditional news media. We therefore hypothesize that negative news content is more likely to be retweeted, while for non-news tweets positive sentiments support virality. To test the hypothesis we analyze three corpora: A complete sample of tweets about the COP15 climate summit, a random sample of tweets, and a general text corpus including news. The latter allows us to train a classifier that can distinguish tweets that carry news and non-news information. We present evidence that negative sentiment enhances virality in the news segment, but not in the non-news segment. We conclude that the relation between affect and virality is more complex than expected based on the findings of Berger and Milkman (2010), in short 'if you want to be cited: Sweet talk your friends or serve bad news to the public'.\",\n",
       "  'Title: a logic framework for p2p deductive databases\\nAbstract: This paper presents a logic framework for modeling the interaction among deductive databases in a peer-to-peer (P2P) environment. Each peer joining a P2P system  provides or imports data  from its neighbors by using a set of  mapping rules , that is, a set of semantic correspondences to a set of peers belonging to the same environment. By using mapping rules, as soon as it enters the system, a peer can participate and access all data available in its neighborhood, and through its neighborhood it becomes accessible to all the other peers in the system. A query can be posed to any peer in the system and the answer is computed by using locally stored data and all the information that can be consistently imported from the neighborhood. Two different types of mapping rules are defined: mapping rules allowing to import a maximal set of atoms not leading to inconsistency (called  maximal mapping rules ) and mapping rules allowing to import a minimal set of atoms needed to restore consistency (called  minimal mapping rules ). Implicitly, the use of maximal mapping rules states  it is preferable to import as long as no inconsistencies arise ; whereas the use of minimal mapping rules states that  it is preferable not to import unless a inconsistency exists . The paper presents three different declarative semantics of a P2P system: (i) the  Max Weak Model Semantics , in which mapping rules are used to import  as much knowledge as possible  from a peer’s neighborhood without violating local integrity constraints; (ii) the  Min Weak Model Semantics , in which the P2P system can be locally inconsistent and the information provided by the neighbors is used to restore consistency, that is, to only integrate the missing portion of a correct, but incomplete database; (iii) the  Max-Min Weak Model Semantics  that unifies the previous two different perspectives captured by the Max Weak Model Semantics and Min Weak Model Semantics. This last semantics allows to characterize each peer in the neighborhood as a resource used either to enrich (integrate) or to fix (repair) the knowledge, so as to define a kind of  integrate–repair  strategy for each peer. For each semantics, the paper also introduces an equivalent and alternative characterization, obtained by rewriting each mapping rule into prioritized rules so as to model a P2P system as a prioritized logic program. Finally, results about the computational complexity of P2P logic queries are investigated by considering  brave  and  cautious  reasoning.',\n",
       "  'Title: a vertex ordering characterization of simple triangle graphs\\nAbstract: Consider two horizontal lines in the plane. A pair of a point on the top line and an interval on the bottom line defines a triangle between two lines. The intersection graph of such triangles is called a simple-triangle graph. This paper shows a vertex ordering characterization of simple-triangle graphs as follows: a graph is a simple-triangle graph if and only if there is a linear ordering of the vertices that contains both an alternating orientation of the graph and a transitive orientation of the complement of the graph.',\n",
       "  'Title: mimicking human process text representation via latent semantic clustering for classification\\nAbstract: Considering that words with different characteristic in the text have different importance for classification, grouping them together separately can strengthen the semantic expression of each part. Thus we propose a new text representation scheme by clustering words according to their latent semantics and composing them together to get a set of cluster vectors, which are then concatenated as the final text representation. Evaluation on five classification benchmarks proves the effectiveness of our method. We further conduct visualization analysis showing statistical clustering results and verifying the validity of our motivation.',\n",
       "  'Title: the online laboratory conducting experiments in a real labor market\\nAbstract: Online labor markets have great potential as platforms for conducting experiments, as they provide immediate access to a large and diverse subject pool and allow researchers to conduct randomized controlled trials. We argue that online experiments can be just as valid---both internally and externally---as laboratory and field experiments, while requiring far less money and time to design and to conduct. In this paper, we first describe the benefits of conducting experiments in online labor markets; we then use one such market to replicate three classic experiments and confirm their results. We confirm that subjects (1) reverse decisions in response to how a decision-problem is framed, (2) have pro-social preferences (value payoffs to others positively), and (3) respond to priming by altering their choices. We also conduct a labor supply field experiment in which we confirm that workers have upward sloping labor supply curves. In addition to reporting these results, we discuss the unique threats to validity in an online setting and propose methods for coping with these threats. We also discuss the external validity of results from online domains and explain why online results can have external validity equal to or even better than that of traditional methods, depending on the research question. We conclude with our views on the potential role that online experiments can play within the social sciences, and then recommend software development priorities and best practices.',\n",
       "  'Title: maximum quadratic assignment problem reduction from maximum label cover and lp based approximation algorithm\\nAbstract: We show that for every positive $\\\\epsilon > 0$, unless NP $\\\\subset$ BPQP, it is impossible to approximate the maximum quadratic assignment problem within a factor better than $2^{\\\\log^{1-\\\\epsilon} n}$ by a reduction from the maximum label cover problem. Our result also implies that Approximate Graph Isomorphism is not robust and is in fact, $1 - \\\\epsilon$ vs $\\\\epsilon$ hard assuming the Unique Games Conjecture. #R##N#Then, we present an $O(\\\\sqrt{n})$-approximation algorithm for the problem based on rounding of the linear programming relaxation often used in the state of the art exact algorithms.',\n",
       "  'Title: picking apart story salads\\nAbstract: During natural disasters and conflicts, information about what happened is often confusing, messy, and distributed across many sources. We would like to be able to automatically identify relevant information and assemble it into coherent narratives of what happened. To make this task accessible to neural models, we introduce Story Salads, mixtures of multiple documents that can be generated at scale. By exploiting the Wikipedia hierarchy, we can generate salads that exhibit challenging inference problems. Story salads give rise to a novel, challenging clustering task, where the objective is to group sentences from the same narratives. We demonstrate that simple bag-of-words similarity clustering falls short on this task and that it is necessary to take into account global context and coherence.',\n",
       "  'Title: optimal scheduling of electric vehicles charging in low voltage distribution systems\\nAbstract: Uncoordinated charging of large-scale electric vehicles (EVs) will have a negative impact on the secure and economic operation of the power system, especially at the distribution level. Given that the charging load of EVs can be controlled to some extent, research on the optimal charging control of EVs has been extensively carried out. In this paper, two possible smart charging scenarios in China are studied: centralized optimal charging operated by an aggregator and decentralized optimal charging managed by individual users. Under the assumption that the aggregators and individual users only concern the economic benefits, new load peaks will arise under time of use (TOU) pricing which is extensively employed in China. To solve this problem, a simple incentive mechanism is proposed for centralized optimal charging while a rolling-update pricing scheme is devised for decentralized optimal charging. The original optimal charging models are modified to account for the developed schemes. Simulated tests corroborate the efficacy of optimal scheduling for charging EVs in various scenarios.',\n",
       "  ...])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from load_arxiv import get_raw_text_arxiv\n",
    "\n",
    "data, text = get_raw_text_arxiv(use_text=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
